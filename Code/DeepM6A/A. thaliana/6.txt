Using Theano backend.
WARNING (theano.gof.compilelock): Overriding existing lock by dead process '104015' (I am process '105005')
Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 10.0% of memory, cuDNN 5005)
loading data
6
(31408, 61, 4)
('train_label: count', (array([0, 1]), array([15704, 15704])))
('valid_label: count', (array([0, 1]), array([1962, 1962])))
('test_label: count', (array([0, 1]), array([1962, 1962])))
building model...............
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
cov1 (Convolution1D)             (None, 58, 80)        1360        convolution1d_input_1[0][0]      
____________________________________________________________________________________________________
leakyrelu_1 (LeakyReLU)          (None, 58, 80)        0           cov1[0][0]                       
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 58, 80)        0           leakyrelu_1[0][0]                
____________________________________________________________________________________________________
cov2 (Convolution1D)             (None, 57, 80)        12880       dropout_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_2 (LeakyReLU)          (None, 57, 80)        0           cov2[0][0]                       
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 57, 80)        0           leakyrelu_2[0][0]                
____________________________________________________________________________________________________
cov3 (Convolution1D)             (None, 54, 80)        25680       dropout_2[0][0]                  
____________________________________________________________________________________________________
leakyrelu_3 (LeakyReLU)          (None, 54, 80)        0           cov3[0][0]                       
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 54, 80)        0           leakyrelu_3[0][0]                
____________________________________________________________________________________________________
cov4 (Convolution1D)             (None, 51, 80)        25680       dropout_3[0][0]                  
____________________________________________________________________________________________________
leakyrelu_4 (LeakyReLU)          (None, 51, 80)        0           cov4[0][0]                       
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 51, 80)        0           leakyrelu_4[0][0]                
____________________________________________________________________________________________________
cov5 (Convolution1D)             (None, 48, 80)        25680       dropout_4[0][0]                  
____________________________________________________________________________________________________
leakyrelu_5 (LeakyReLU)          (None, 48, 80)        0           cov5[0][0]                       
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 48, 80)        0           leakyrelu_5[0][0]                
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 3840)          0           dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           384100      flatten_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_6 (LeakyReLU)          (None, 100)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 100)           0           leakyrelu_6[0][0]                
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             101         dropout_6[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    
====================================================================================================
Total params: 475481
____________________________________________________________________________________________________
compiling and fitting model...........
Train on 31408 samples, validate on 3924 samples
Epoch 1/500
Epoch 00000: val_loss improved from inf to 0.69315, saving model to ./bestmodel6.hdf5
4s - loss: 0.6932 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 2/500
Epoch 00001: val_loss improved from 0.69315 to 0.69313, saving model to ./bestmodel6.hdf5
3s - loss: 0.6932 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 3/500
Epoch 00002: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 4/500
Epoch 00003: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 5/500
Epoch 00004: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 6/500
Epoch 00005: val_loss did not improve
3s - loss: 0.6933 - acc: 0.4962 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 7/500
Epoch 00006: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 8/500
Epoch 00007: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4955 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 9/500
Epoch 00008: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 10/500
Epoch 00009: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5026 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 11/500
Epoch 00010: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5040 - val_loss: 0.6933 - val_acc: 0.5000
Epoch 12/500
Epoch 00011: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 13/500
Epoch 00012: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5622
Epoch 14/500
Epoch 00013: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 15/500
Epoch 00014: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 16/500
Epoch 00015: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 17/500
Epoch 00016: val_loss improved from 0.69313 to 0.69313, saving model to ./bestmodel6.hdf5
3s - loss: 0.6932 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 18/500
Epoch 00017: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 19/500
Epoch 00018: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 20/500
Epoch 00019: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 21/500
Epoch 00020: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5040 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 22/500
Epoch 00021: val_loss improved from 0.69313 to 0.69312, saving model to ./bestmodel6.hdf5
3s - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 23/500
Epoch 00022: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 24/500
Epoch 00023: val_loss improved from 0.69312 to 0.69312, saving model to ./bestmodel6.hdf5
4s - loss: 0.6932 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 25/500
Epoch 00024: val_loss improved from 0.69312 to 0.69311, saving model to ./bestmodel6.hdf5
4s - loss: 0.6932 - acc: 0.5032 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 26/500
Epoch 00025: val_loss improved from 0.69311 to 0.69309, saving model to ./bestmodel6.hdf5
3s - loss: 0.6932 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 27/500
Epoch 00026: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5030 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 28/500
Epoch 00027: val_loss improved from 0.69309 to 0.69307, saving model to ./bestmodel6.hdf5
3s - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 29/500
Epoch 00028: val_loss improved from 0.69307 to 0.69306, saving model to ./bestmodel6.hdf5
3s - loss: 0.6931 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 30/500
Epoch 00029: val_loss improved from 0.69306 to 0.69305, saving model to ./bestmodel6.hdf5
3s - loss: 0.6931 - acc: 0.5001 - val_loss: 0.6930 - val_acc: 0.5000
Epoch 31/500
Epoch 00030: val_loss improved from 0.69305 to 0.69302, saving model to ./bestmodel6.hdf5
3s - loss: 0.6931 - acc: 0.5067 - val_loss: 0.6930 - val_acc: 0.5000
Epoch 32/500
Epoch 00031: val_loss improved from 0.69302 to 0.69292, saving model to ./bestmodel6.hdf5
3s - loss: 0.6931 - acc: 0.5059 - val_loss: 0.6929 - val_acc: 0.5000
Epoch 33/500
Epoch 00032: val_loss improved from 0.69292 to 0.69278, saving model to ./bestmodel6.hdf5
3s - loss: 0.6930 - acc: 0.5085 - val_loss: 0.6928 - val_acc: 0.5000
Epoch 34/500
Epoch 00033: val_loss improved from 0.69278 to 0.69248, saving model to ./bestmodel6.hdf5
5s - loss: 0.6929 - acc: 0.5123 - val_loss: 0.6925 - val_acc: 0.6430
Epoch 35/500
Epoch 00034: val_loss improved from 0.69248 to 0.69159, saving model to ./bestmodel6.hdf5
4s - loss: 0.6925 - acc: 0.5322 - val_loss: 0.6916 - val_acc: 0.5334
Epoch 36/500
Epoch 00035: val_loss improved from 0.69159 to 0.68425, saving model to ./bestmodel6.hdf5
3s - loss: 0.6903 - acc: 0.5848 - val_loss: 0.6843 - val_acc: 0.6707
Epoch 37/500
Epoch 00036: val_loss improved from 0.68425 to 0.51074, saving model to ./bestmodel6.hdf5
3s - loss: 0.6414 - acc: 0.6746 - val_loss: 0.5107 - val_acc: 0.7564
Epoch 38/500
Epoch 00037: val_loss improved from 0.51074 to 0.46068, saving model to ./bestmodel6.hdf5
4s - loss: 0.5457 - acc: 0.7338 - val_loss: 0.4607 - val_acc: 0.7877
Epoch 39/500
Epoch 00038: val_loss did not improve
3s - loss: 0.5050 - acc: 0.7649 - val_loss: 0.4714 - val_acc: 0.7834
Epoch 40/500
Epoch 00039: val_loss improved from 0.46068 to 0.41168, saving model to ./bestmodel6.hdf5
3s - loss: 0.4883 - acc: 0.7778 - val_loss: 0.4117 - val_acc: 0.8208
Epoch 41/500
Epoch 00040: val_loss improved from 0.41168 to 0.39142, saving model to ./bestmodel6.hdf5
3s - loss: 0.4692 - acc: 0.7884 - val_loss: 0.3914 - val_acc: 0.8280
Epoch 42/500
Epoch 00041: val_loss improved from 0.39142 to 0.38274, saving model to ./bestmodel6.hdf5
3s - loss: 0.4571 - acc: 0.7958 - val_loss: 0.3827 - val_acc: 0.8310
Epoch 43/500
Epoch 00042: val_loss improved from 0.38274 to 0.36644, saving model to ./bestmodel6.hdf5
3s - loss: 0.4390 - acc: 0.8055 - val_loss: 0.3664 - val_acc: 0.8407
Epoch 44/500
Epoch 00043: val_loss improved from 0.36644 to 0.35950, saving model to ./bestmodel6.hdf5
4s - loss: 0.4296 - acc: 0.8136 - val_loss: 0.3595 - val_acc: 0.8451
Epoch 45/500
Epoch 00044: val_loss improved from 0.35950 to 0.35612, saving model to ./bestmodel6.hdf5
3s - loss: 0.4223 - acc: 0.8172 - val_loss: 0.3561 - val_acc: 0.8481
Epoch 46/500
Epoch 00045: val_loss improved from 0.35612 to 0.34927, saving model to ./bestmodel6.hdf5
4s - loss: 0.4152 - acc: 0.8201 - val_loss: 0.3493 - val_acc: 0.8443
Epoch 47/500
Epoch 00046: val_loss did not improve
3s - loss: 0.4124 - acc: 0.8229 - val_loss: 0.3507 - val_acc: 0.8502
Epoch 48/500
Epoch 00047: val_loss improved from 0.34927 to 0.33596, saving model to ./bestmodel6.hdf5
4s - loss: 0.4050 - acc: 0.8266 - val_loss: 0.3360 - val_acc: 0.8512
Epoch 49/500
Epoch 00048: val_loss did not improve
3s - loss: 0.4019 - acc: 0.8268 - val_loss: 0.3380 - val_acc: 0.8573
Epoch 50/500
Epoch 00049: val_loss improved from 0.33596 to 0.32695, saving model to ./bestmodel6.hdf5
4s - loss: 0.3966 - acc: 0.8304 - val_loss: 0.3269 - val_acc: 0.8601
Epoch 51/500
Epoch 00050: val_loss did not improve
3s - loss: 0.3927 - acc: 0.8312 - val_loss: 0.3473 - val_acc: 0.8568
Epoch 52/500
Epoch 00051: val_loss improved from 0.32695 to 0.31959, saving model to ./bestmodel6.hdf5
4s - loss: 0.3880 - acc: 0.8332 - val_loss: 0.3196 - val_acc: 0.8675
Epoch 53/500
Epoch 00052: val_loss improved from 0.31959 to 0.31638, saving model to ./bestmodel6.hdf5
3s - loss: 0.3834 - acc: 0.8376 - val_loss: 0.3164 - val_acc: 0.8698
Epoch 54/500
Epoch 00053: val_loss improved from 0.31638 to 0.31388, saving model to ./bestmodel6.hdf5
4s - loss: 0.3775 - acc: 0.8394 - val_loss: 0.3139 - val_acc: 0.8644
Epoch 55/500
Epoch 00054: val_loss improved from 0.31388 to 0.31309, saving model to ./bestmodel6.hdf5
4s - loss: 0.3744 - acc: 0.8416 - val_loss: 0.3131 - val_acc: 0.8731
Epoch 56/500
Epoch 00055: val_loss improved from 0.31309 to 0.30009, saving model to ./bestmodel6.hdf5
3s - loss: 0.3724 - acc: 0.8428 - val_loss: 0.3001 - val_acc: 0.8769
Epoch 57/500
Epoch 00056: val_loss did not improve
3s - loss: 0.3664 - acc: 0.8462 - val_loss: 0.3017 - val_acc: 0.8833
Epoch 58/500
Epoch 00057: val_loss improved from 0.30009 to 0.29290, saving model to ./bestmodel6.hdf5
3s - loss: 0.3624 - acc: 0.8478 - val_loss: 0.2929 - val_acc: 0.8802
Epoch 59/500
Epoch 00058: val_loss did not improve
3s - loss: 0.3599 - acc: 0.8495 - val_loss: 0.2943 - val_acc: 0.8843
Epoch 60/500
Epoch 00059: val_loss did not improve
3s - loss: 0.3571 - acc: 0.8512 - val_loss: 0.2969 - val_acc: 0.8889
Epoch 61/500
Epoch 00060: val_loss improved from 0.29290 to 0.28680, saving model to ./bestmodel6.hdf5
4s - loss: 0.3512 - acc: 0.8536 - val_loss: 0.2868 - val_acc: 0.8853
Epoch 62/500
Epoch 00061: val_loss improved from 0.28680 to 0.28583, saving model to ./bestmodel6.hdf5
3s - loss: 0.3478 - acc: 0.8549 - val_loss: 0.2858 - val_acc: 0.8846
Epoch 63/500
Epoch 00062: val_loss did not improve
3s - loss: 0.3441 - acc: 0.8566 - val_loss: 0.2925 - val_acc: 0.8879
Epoch 64/500
Epoch 00063: val_loss improved from 0.28583 to 0.28455, saving model to ./bestmodel6.hdf5
3s - loss: 0.3428 - acc: 0.8556 - val_loss: 0.2845 - val_acc: 0.8904
Epoch 65/500
Epoch 00064: val_loss improved from 0.28455 to 0.27924, saving model to ./bestmodel6.hdf5
3s - loss: 0.3390 - acc: 0.8613 - val_loss: 0.2792 - val_acc: 0.8894
Epoch 66/500
Epoch 00065: val_loss improved from 0.27924 to 0.27912, saving model to ./bestmodel6.hdf5
3s - loss: 0.3359 - acc: 0.8614 - val_loss: 0.2791 - val_acc: 0.8932
Epoch 67/500
Epoch 00066: val_loss improved from 0.27912 to 0.27529, saving model to ./bestmodel6.hdf5
3s - loss: 0.3309 - acc: 0.8633 - val_loss: 0.2753 - val_acc: 0.8942
Epoch 68/500
Epoch 00067: val_loss improved from 0.27529 to 0.27087, saving model to ./bestmodel6.hdf5
6s - loss: 0.3299 - acc: 0.8636 - val_loss: 0.2709 - val_acc: 0.8937
Epoch 69/500
Epoch 00068: val_loss did not improve
3s - loss: 0.3259 - acc: 0.8637 - val_loss: 0.2742 - val_acc: 0.8853
Epoch 70/500
Epoch 00069: val_loss improved from 0.27087 to 0.26681, saving model to ./bestmodel6.hdf5
3s - loss: 0.3227 - acc: 0.8674 - val_loss: 0.2668 - val_acc: 0.8935
Epoch 71/500
Epoch 00070: val_loss did not improve
3s - loss: 0.3197 - acc: 0.8677 - val_loss: 0.2742 - val_acc: 0.8945
Epoch 72/500
Epoch 00071: val_loss improved from 0.26681 to 0.26071, saving model to ./bestmodel6.hdf5
4s - loss: 0.3184 - acc: 0.8697 - val_loss: 0.2607 - val_acc: 0.8973
Epoch 73/500
Epoch 00072: val_loss did not improve
3s - loss: 0.3143 - acc: 0.8718 - val_loss: 0.2637 - val_acc: 0.8950
Epoch 74/500
Epoch 00073: val_loss did not improve
3s - loss: 0.3101 - acc: 0.8732 - val_loss: 0.2688 - val_acc: 0.8940
Epoch 75/500
Epoch 00074: val_loss improved from 0.26071 to 0.25485, saving model to ./bestmodel6.hdf5
3s - loss: 0.3086 - acc: 0.8753 - val_loss: 0.2548 - val_acc: 0.8981
Epoch 76/500
Epoch 00075: val_loss improved from 0.25485 to 0.25362, saving model to ./bestmodel6.hdf5
3s - loss: 0.3060 - acc: 0.8763 - val_loss: 0.2536 - val_acc: 0.9016
Epoch 77/500
Epoch 00076: val_loss improved from 0.25362 to 0.25253, saving model to ./bestmodel6.hdf5
4s - loss: 0.3007 - acc: 0.8786 - val_loss: 0.2525 - val_acc: 0.9021
Epoch 78/500
Epoch 00077: val_loss improved from 0.25253 to 0.25210, saving model to ./bestmodel6.hdf5
3s - loss: 0.3018 - acc: 0.8771 - val_loss: 0.2521 - val_acc: 0.9009
Epoch 79/500
Epoch 00078: val_loss improved from 0.25210 to 0.24772, saving model to ./bestmodel6.hdf5
3s - loss: 0.2983 - acc: 0.8775 - val_loss: 0.2477 - val_acc: 0.9027
Epoch 80/500
Epoch 00079: val_loss did not improve
3s - loss: 0.2953 - acc: 0.8822 - val_loss: 0.2530 - val_acc: 0.9006
Epoch 81/500
Epoch 00080: val_loss did not improve
3s - loss: 0.2928 - acc: 0.8816 - val_loss: 0.2481 - val_acc: 0.8993
Epoch 82/500
Epoch 00081: val_loss improved from 0.24772 to 0.24466, saving model to ./bestmodel6.hdf5
3s - loss: 0.2906 - acc: 0.8820 - val_loss: 0.2447 - val_acc: 0.9034
Epoch 83/500
Epoch 00082: val_loss did not improve
3s - loss: 0.2932 - acc: 0.8811 - val_loss: 0.2470 - val_acc: 0.9034
Epoch 84/500
Epoch 00083: val_loss improved from 0.24466 to 0.24401, saving model to ./bestmodel6.hdf5
3s - loss: 0.2916 - acc: 0.8827 - val_loss: 0.2440 - val_acc: 0.9032
Epoch 85/500
Epoch 00084: val_loss improved from 0.24401 to 0.24303, saving model to ./bestmodel6.hdf5
4s - loss: 0.2891 - acc: 0.8831 - val_loss: 0.2430 - val_acc: 0.9019
Epoch 86/500
Epoch 00085: val_loss did not improve
3s - loss: 0.2877 - acc: 0.8836 - val_loss: 0.2438 - val_acc: 0.9014
Epoch 87/500
Epoch 00086: val_loss improved from 0.24303 to 0.24226, saving model to ./bestmodel6.hdf5
3s - loss: 0.2841 - acc: 0.8860 - val_loss: 0.2423 - val_acc: 0.8998
Epoch 88/500
Epoch 00087: val_loss improved from 0.24226 to 0.24088, saving model to ./bestmodel6.hdf5
3s - loss: 0.2860 - acc: 0.8852 - val_loss: 0.2409 - val_acc: 0.9034
Epoch 89/500
Epoch 00088: val_loss did not improve
3s - loss: 0.2809 - acc: 0.8883 - val_loss: 0.2487 - val_acc: 0.8991
Epoch 90/500
Epoch 00089: val_loss did not improve
3s - loss: 0.2806 - acc: 0.8868 - val_loss: 0.2527 - val_acc: 0.9014
Epoch 91/500
Epoch 00090: val_loss improved from 0.24088 to 0.24061, saving model to ./bestmodel6.hdf5
3s - loss: 0.2806 - acc: 0.8867 - val_loss: 0.2406 - val_acc: 0.9034
Epoch 92/500
Epoch 00091: val_loss improved from 0.24061 to 0.24035, saving model to ./bestmodel6.hdf5
4s - loss: 0.2818 - acc: 0.8881 - val_loss: 0.2404 - val_acc: 0.9032
Epoch 93/500
Epoch 00092: val_loss did not improve
3s - loss: 0.2781 - acc: 0.8889 - val_loss: 0.2436 - val_acc: 0.9049
Epoch 94/500
Epoch 00093: val_loss improved from 0.24035 to 0.23940, saving model to ./bestmodel6.hdf5
3s - loss: 0.2770 - acc: 0.8898 - val_loss: 0.2394 - val_acc: 0.9049
Epoch 95/500
Epoch 00094: val_loss did not improve
3s - loss: 0.2771 - acc: 0.8893 - val_loss: 0.2424 - val_acc: 0.9014
Epoch 96/500
Epoch 00095: val_loss did not improve
3s - loss: 0.2787 - acc: 0.8894 - val_loss: 0.2398 - val_acc: 0.9032
Epoch 97/500
Epoch 00096: val_loss improved from 0.23940 to 0.23874, saving model to ./bestmodel6.hdf5
3s - loss: 0.2741 - acc: 0.8899 - val_loss: 0.2387 - val_acc: 0.9039
Epoch 98/500
Epoch 00097: val_loss improved from 0.23874 to 0.23667, saving model to ./bestmodel6.hdf5
3s - loss: 0.2723 - acc: 0.8910 - val_loss: 0.2367 - val_acc: 0.9047
Epoch 99/500
Epoch 00098: val_loss did not improve
3s - loss: 0.2708 - acc: 0.8920 - val_loss: 0.2402 - val_acc: 0.9014
Epoch 100/500
Epoch 00099: val_loss did not improve
3s - loss: 0.2701 - acc: 0.8938 - val_loss: 0.2382 - val_acc: 0.9016
Epoch 101/500
Epoch 00100: val_loss did not improve
3s - loss: 0.2698 - acc: 0.8932 - val_loss: 0.2368 - val_acc: 0.9024
Epoch 102/500
Epoch 00101: val_loss improved from 0.23667 to 0.23551, saving model to ./bestmodel6.hdf5
4s - loss: 0.2702 - acc: 0.8933 - val_loss: 0.2355 - val_acc: 0.9032
Epoch 103/500
Epoch 00102: val_loss did not improve
3s - loss: 0.2674 - acc: 0.8937 - val_loss: 0.2371 - val_acc: 0.9024
Epoch 104/500
Epoch 00103: val_loss did not improve
3s - loss: 0.2671 - acc: 0.8940 - val_loss: 0.2383 - val_acc: 0.9044
Epoch 105/500
Epoch 00104: val_loss did not improve
3s - loss: 0.2676 - acc: 0.8946 - val_loss: 0.2407 - val_acc: 0.9049
Epoch 106/500
Epoch 00105: val_loss did not improve
3s - loss: 0.2653 - acc: 0.8960 - val_loss: 0.2360 - val_acc: 0.9034
Epoch 107/500
Epoch 00106: val_loss improved from 0.23551 to 0.23542, saving model to ./bestmodel6.hdf5
4s - loss: 0.2643 - acc: 0.8952 - val_loss: 0.2354 - val_acc: 0.9032
Epoch 108/500
Epoch 00107: val_loss did not improve
3s - loss: 0.2638 - acc: 0.8958 - val_loss: 0.2393 - val_acc: 0.9047
Epoch 109/500
Epoch 00108: val_loss did not improve
3s - loss: 0.2634 - acc: 0.8966 - val_loss: 0.2362 - val_acc: 0.9057
Epoch 110/500
Epoch 00109: val_loss did not improve
3s - loss: 0.2641 - acc: 0.8956 - val_loss: 0.2363 - val_acc: 0.9055
Epoch 111/500
Epoch 00110: val_loss did not improve
3s - loss: 0.2595 - acc: 0.8981 - val_loss: 0.2360 - val_acc: 0.9060
Epoch 112/500
Epoch 00111: val_loss did not improve
3s - loss: 0.2624 - acc: 0.8969 - val_loss: 0.2362 - val_acc: 0.9029
Epoch 113/500
Epoch 00112: val_loss improved from 0.23542 to 0.23219, saving model to ./bestmodel6.hdf5
3s - loss: 0.2593 - acc: 0.8982 - val_loss: 0.2322 - val_acc: 0.9052
Epoch 114/500
Epoch 00113: val_loss did not improve
3s - loss: 0.2590 - acc: 0.8989 - val_loss: 0.2339 - val_acc: 0.9052
Epoch 115/500
Epoch 00114: val_loss did not improve
3s - loss: 0.2593 - acc: 0.8974 - val_loss: 0.2338 - val_acc: 0.9062
Epoch 116/500
Epoch 00115: val_loss did not improve
3s - loss: 0.2583 - acc: 0.8983 - val_loss: 0.2330 - val_acc: 0.9057
Epoch 117/500
Epoch 00116: val_loss did not improve
3s - loss: 0.2581 - acc: 0.8991 - val_loss: 0.2338 - val_acc: 0.9070
Epoch 118/500
Epoch 00117: val_loss improved from 0.23219 to 0.23150, saving model to ./bestmodel6.hdf5
3s - loss: 0.2568 - acc: 0.8981 - val_loss: 0.2315 - val_acc: 0.9037
Epoch 119/500
Epoch 00118: val_loss did not improve
3s - loss: 0.2559 - acc: 0.9008 - val_loss: 0.2328 - val_acc: 0.9062
Epoch 120/500
Epoch 00119: val_loss did not improve
3s - loss: 0.2543 - acc: 0.8990 - val_loss: 0.2355 - val_acc: 0.9077
Epoch 121/500
Epoch 00120: val_loss did not improve
3s - loss: 0.2552 - acc: 0.9003 - val_loss: 0.2345 - val_acc: 0.9060
Epoch 122/500
Epoch 00121: val_loss improved from 0.23150 to 0.22949, saving model to ./bestmodel6.hdf5
3s - loss: 0.2551 - acc: 0.9004 - val_loss: 0.2295 - val_acc: 0.9093
Epoch 123/500
Epoch 00122: val_loss did not improve
3s - loss: 0.2552 - acc: 0.9011 - val_loss: 0.2300 - val_acc: 0.9062
Epoch 124/500
Epoch 00123: val_loss did not improve
3s - loss: 0.2517 - acc: 0.9019 - val_loss: 0.2379 - val_acc: 0.9055
Epoch 125/500
Epoch 00124: val_loss did not improve
3s - loss: 0.2498 - acc: 0.9032 - val_loss: 0.2343 - val_acc: 0.9083
Epoch 126/500
Epoch 00125: val_loss did not improve
3s - loss: 0.2509 - acc: 0.9037 - val_loss: 0.2321 - val_acc: 0.9085
Epoch 127/500
Epoch 00126: val_loss did not improve
3s - loss: 0.2496 - acc: 0.9043 - val_loss: 0.2310 - val_acc: 0.9075
Epoch 128/500
Epoch 00127: val_loss did not improve
3s - loss: 0.2493 - acc: 0.9026 - val_loss: 0.2318 - val_acc: 0.9072
Epoch 129/500
Epoch 00128: val_loss did not improve
3s - loss: 0.2513 - acc: 0.9026 - val_loss: 0.2323 - val_acc: 0.9093
Epoch 130/500
Epoch 00129: val_loss did not improve
3s - loss: 0.2503 - acc: 0.9028 - val_loss: 0.2314 - val_acc: 0.9067
Epoch 131/500
Epoch 00130: val_loss improved from 0.22949 to 0.22885, saving model to ./bestmodel6.hdf5
3s - loss: 0.2491 - acc: 0.9039 - val_loss: 0.2288 - val_acc: 0.9100
Epoch 132/500
Epoch 00131: val_loss did not improve
3s - loss: 0.2445 - acc: 0.9048 - val_loss: 0.2313 - val_acc: 0.9090
Epoch 133/500
Epoch 00132: val_loss did not improve
3s - loss: 0.2448 - acc: 0.9052 - val_loss: 0.2303 - val_acc: 0.9093
Epoch 134/500
Epoch 00133: val_loss did not improve
3s - loss: 0.2446 - acc: 0.9048 - val_loss: 0.2304 - val_acc: 0.9095
Epoch 135/500
Epoch 00134: val_loss improved from 0.22885 to 0.22739, saving model to ./bestmodel6.hdf5
4s - loss: 0.2451 - acc: 0.9051 - val_loss: 0.2274 - val_acc: 0.9098
Epoch 136/500
Epoch 00135: val_loss improved from 0.22739 to 0.22633, saving model to ./bestmodel6.hdf5
3s - loss: 0.2449 - acc: 0.9050 - val_loss: 0.2263 - val_acc: 0.9067
Epoch 137/500
Epoch 00136: val_loss did not improve
3s - loss: 0.2442 - acc: 0.9060 - val_loss: 0.2298 - val_acc: 0.9060
Epoch 138/500
Epoch 00137: val_loss did not improve
3s - loss: 0.2457 - acc: 0.9048 - val_loss: 0.2345 - val_acc: 0.9029
Epoch 139/500
Epoch 00138: val_loss did not improve
3s - loss: 0.2434 - acc: 0.9057 - val_loss: 0.2266 - val_acc: 0.9100
Epoch 140/500
Epoch 00139: val_loss did not improve
3s - loss: 0.2426 - acc: 0.9074 - val_loss: 0.2316 - val_acc: 0.9077
Epoch 141/500
Epoch 00140: val_loss improved from 0.22633 to 0.22533, saving model to ./bestmodel6.hdf5
3s - loss: 0.2408 - acc: 0.9079 - val_loss: 0.2253 - val_acc: 0.9103
Epoch 142/500
Epoch 00141: val_loss did not improve
3s - loss: 0.2416 - acc: 0.9070 - val_loss: 0.2277 - val_acc: 0.9095
Epoch 143/500
Epoch 00142: val_loss did not improve
3s - loss: 0.2412 - acc: 0.9069 - val_loss: 0.2377 - val_acc: 0.9039
Epoch 144/500
Epoch 00143: val_loss did not improve
3s - loss: 0.2404 - acc: 0.9083 - val_loss: 0.2270 - val_acc: 0.9085
Epoch 145/500
Epoch 00144: val_loss did not improve
3s - loss: 0.2400 - acc: 0.9073 - val_loss: 0.2277 - val_acc: 0.9100
Epoch 146/500
Epoch 00145: val_loss did not improve
3s - loss: 0.2367 - acc: 0.9080 - val_loss: 0.2311 - val_acc: 0.9121
Epoch 147/500
Epoch 00146: val_loss did not improve
3s - loss: 0.2392 - acc: 0.9091 - val_loss: 0.2258 - val_acc: 0.9111
Epoch 148/500
Epoch 00147: val_loss did not improve
3s - loss: 0.2385 - acc: 0.9090 - val_loss: 0.2255 - val_acc: 0.9090
Epoch 149/500
Epoch 00148: val_loss improved from 0.22533 to 0.22490, saving model to ./bestmodel6.hdf5
4s - loss: 0.2387 - acc: 0.9092 - val_loss: 0.2249 - val_acc: 0.9123
Epoch 150/500
Epoch 00149: val_loss improved from 0.22490 to 0.22220, saving model to ./bestmodel6.hdf5
4s - loss: 0.2366 - acc: 0.9088 - val_loss: 0.2222 - val_acc: 0.9131
Epoch 151/500
Epoch 00150: val_loss did not improve
3s - loss: 0.2337 - acc: 0.9103 - val_loss: 0.2299 - val_acc: 0.9118
Epoch 152/500
Epoch 00151: val_loss did not improve
3s - loss: 0.2353 - acc: 0.9109 - val_loss: 0.2244 - val_acc: 0.9134
Epoch 153/500
Epoch 00152: val_loss did not improve
3s - loss: 0.2331 - acc: 0.9099 - val_loss: 0.2244 - val_acc: 0.9136
Epoch 154/500
Epoch 00153: val_loss did not improve
3s - loss: 0.2331 - acc: 0.9103 - val_loss: 0.2224 - val_acc: 0.9116
Epoch 155/500
Epoch 00154: val_loss did not improve
3s - loss: 0.2337 - acc: 0.9105 - val_loss: 0.2229 - val_acc: 0.9123
Epoch 156/500
Epoch 00155: val_loss did not improve
3s - loss: 0.2314 - acc: 0.9110 - val_loss: 0.2322 - val_acc: 0.9111
Epoch 157/500
Epoch 00156: val_loss improved from 0.22220 to 0.22214, saving model to ./bestmodel6.hdf5
4s - loss: 0.2338 - acc: 0.9108 - val_loss: 0.2221 - val_acc: 0.9126
Epoch 158/500
Epoch 00157: val_loss did not improve
3s - loss: 0.2310 - acc: 0.9113 - val_loss: 0.2280 - val_acc: 0.9111
Epoch 159/500
Epoch 00158: val_loss improved from 0.22214 to 0.22064, saving model to ./bestmodel6.hdf5
3s - loss: 0.2327 - acc: 0.9121 - val_loss: 0.2206 - val_acc: 0.9151
Epoch 160/500
Epoch 00159: val_loss did not improve
3s - loss: 0.2284 - acc: 0.9113 - val_loss: 0.2207 - val_acc: 0.9131
Epoch 161/500
Epoch 00160: val_loss did not improve
3s - loss: 0.2283 - acc: 0.9136 - val_loss: 0.2264 - val_acc: 0.9131
Epoch 162/500
Epoch 00161: val_loss did not improve
3s - loss: 0.2281 - acc: 0.9119 - val_loss: 0.2216 - val_acc: 0.9146
Epoch 163/500
Epoch 00162: val_loss did not improve
3s - loss: 0.2297 - acc: 0.9131 - val_loss: 0.2212 - val_acc: 0.9164
Epoch 164/500
Epoch 00163: val_loss improved from 0.22064 to 0.22011, saving model to ./bestmodel6.hdf5
3s - loss: 0.2282 - acc: 0.9155 - val_loss: 0.2201 - val_acc: 0.9159
Epoch 165/500
Epoch 00164: val_loss did not improve
3s - loss: 0.2305 - acc: 0.9109 - val_loss: 0.2226 - val_acc: 0.9167
Epoch 166/500
Epoch 00165: val_loss did not improve
3s - loss: 0.2283 - acc: 0.9122 - val_loss: 0.2211 - val_acc: 0.9167
Epoch 167/500
Epoch 00166: val_loss did not improve
3s - loss: 0.2250 - acc: 0.9146 - val_loss: 0.2202 - val_acc: 0.9167
Epoch 168/500
Epoch 00167: val_loss did not improve
3s - loss: 0.2247 - acc: 0.9139 - val_loss: 0.2206 - val_acc: 0.9151
Epoch 169/500
Epoch 00168: val_loss improved from 0.22011 to 0.21998, saving model to ./bestmodel6.hdf5
5s - loss: 0.2252 - acc: 0.9151 - val_loss: 0.2200 - val_acc: 0.9149
Epoch 170/500
Epoch 00169: val_loss improved from 0.21998 to 0.21794, saving model to ./bestmodel6.hdf5
4s - loss: 0.2248 - acc: 0.9136 - val_loss: 0.2179 - val_acc: 0.9167
Epoch 171/500
Epoch 00170: val_loss did not improve
3s - loss: 0.2268 - acc: 0.9128 - val_loss: 0.2198 - val_acc: 0.9139
Epoch 172/500
Epoch 00171: val_loss improved from 0.21794 to 0.21747, saving model to ./bestmodel6.hdf5
4s - loss: 0.2246 - acc: 0.9141 - val_loss: 0.2175 - val_acc: 0.9151
Epoch 173/500
Epoch 00172: val_loss did not improve
3s - loss: 0.2237 - acc: 0.9168 - val_loss: 0.2196 - val_acc: 0.9156
Epoch 174/500
Epoch 00173: val_loss did not improve
3s - loss: 0.2199 - acc: 0.9153 - val_loss: 0.2206 - val_acc: 0.9162
Epoch 175/500
Epoch 00174: val_loss did not improve
3s - loss: 0.2246 - acc: 0.9146 - val_loss: 0.2188 - val_acc: 0.9156
Epoch 176/500
Epoch 00175: val_loss did not improve
3s - loss: 0.2209 - acc: 0.9151 - val_loss: 0.2190 - val_acc: 0.9154
Epoch 177/500
Epoch 00176: val_loss did not improve
3s - loss: 0.2221 - acc: 0.9157 - val_loss: 0.2195 - val_acc: 0.9169
Epoch 178/500
Epoch 00177: val_loss did not improve
3s - loss: 0.2206 - acc: 0.9144 - val_loss: 0.2208 - val_acc: 0.9121
Epoch 179/500
Epoch 00178: val_loss did not improve
3s - loss: 0.2212 - acc: 0.9159 - val_loss: 0.2177 - val_acc: 0.9167
Epoch 180/500
Epoch 00179: val_loss improved from 0.21747 to 0.21578, saving model to ./bestmodel6.hdf5
3s - loss: 0.2216 - acc: 0.9145 - val_loss: 0.2158 - val_acc: 0.9169
Epoch 181/500
Epoch 00180: val_loss did not improve
3s - loss: 0.2191 - acc: 0.9178 - val_loss: 0.2175 - val_acc: 0.9185
Epoch 182/500
Epoch 00181: val_loss did not improve
3s - loss: 0.2191 - acc: 0.9161 - val_loss: 0.2271 - val_acc: 0.9151
Epoch 183/500
Epoch 00182: val_loss did not improve
3s - loss: 0.2201 - acc: 0.9172 - val_loss: 0.2163 - val_acc: 0.9167
Epoch 184/500
Epoch 00183: val_loss improved from 0.21578 to 0.21521, saving model to ./bestmodel6.hdf5
3s - loss: 0.2174 - acc: 0.9167 - val_loss: 0.2152 - val_acc: 0.9164
Epoch 185/500
Epoch 00184: val_loss did not improve
3s - loss: 0.2147 - acc: 0.9185 - val_loss: 0.2178 - val_acc: 0.9167
Epoch 186/500
Epoch 00185: val_loss did not improve
3s - loss: 0.2157 - acc: 0.9182 - val_loss: 0.2388 - val_acc: 0.9123
Epoch 187/500
Epoch 00186: val_loss did not improve
3s - loss: 0.2151 - acc: 0.9188 - val_loss: 0.2170 - val_acc: 0.9164
Epoch 188/500
Epoch 00187: val_loss did not improve
3s - loss: 0.2200 - acc: 0.9165 - val_loss: 0.2177 - val_acc: 0.9159
Epoch 189/500
Epoch 00188: val_loss did not improve
3s - loss: 0.2136 - acc: 0.9186 - val_loss: 0.2173 - val_acc: 0.9185
Epoch 190/500
Epoch 00189: val_loss did not improve
3s - loss: 0.2158 - acc: 0.9183 - val_loss: 0.2198 - val_acc: 0.9156
Epoch 191/500
Epoch 00190: val_loss did not improve
3s - loss: 0.2116 - acc: 0.9198 - val_loss: 0.2176 - val_acc: 0.9190
Epoch 192/500
Epoch 00191: val_loss improved from 0.21521 to 0.21494, saving model to ./bestmodel6.hdf5
3s - loss: 0.2107 - acc: 0.9210 - val_loss: 0.2149 - val_acc: 0.9177
Epoch 193/500
Epoch 00192: val_loss improved from 0.21494 to 0.21480, saving model to ./bestmodel6.hdf5
3s - loss: 0.2121 - acc: 0.9188 - val_loss: 0.2148 - val_acc: 0.9164
Epoch 194/500
Epoch 00193: val_loss did not improve
3s - loss: 0.2146 - acc: 0.9207 - val_loss: 0.2175 - val_acc: 0.9179
Epoch 195/500
Epoch 00194: val_loss did not improve
3s - loss: 0.2131 - acc: 0.9182 - val_loss: 0.2156 - val_acc: 0.9192
Epoch 196/500
Epoch 00195: val_loss did not improve
3s - loss: 0.2142 - acc: 0.9183 - val_loss: 0.2154 - val_acc: 0.9195
Epoch 197/500
Epoch 00196: val_loss did not improve
3s - loss: 0.2097 - acc: 0.9196 - val_loss: 0.2173 - val_acc: 0.9169
Epoch 198/500
Epoch 00197: val_loss improved from 0.21480 to 0.21435, saving model to ./bestmodel6.hdf5
3s - loss: 0.2094 - acc: 0.9202 - val_loss: 0.2144 - val_acc: 0.9195
Epoch 199/500
Epoch 00198: val_loss did not improve
3s - loss: 0.2061 - acc: 0.9224 - val_loss: 0.2145 - val_acc: 0.9207
Epoch 200/500
Epoch 00199: val_loss did not improve
3s - loss: 0.2102 - acc: 0.9197 - val_loss: 0.2147 - val_acc: 0.9172
Epoch 201/500
Epoch 00200: val_loss improved from 0.21435 to 0.21268, saving model to ./bestmodel6.hdf5
4s - loss: 0.2101 - acc: 0.9203 - val_loss: 0.2127 - val_acc: 0.9174
Epoch 202/500
Epoch 00201: val_loss did not improve
3s - loss: 0.2094 - acc: 0.9199 - val_loss: 0.2134 - val_acc: 0.9200
Epoch 203/500
Epoch 00202: val_loss did not improve
3s - loss: 0.2094 - acc: 0.9203 - val_loss: 0.2141 - val_acc: 0.9187
Epoch 204/500
Epoch 00203: val_loss did not improve
3s - loss: 0.2079 - acc: 0.9215 - val_loss: 0.2130 - val_acc: 0.9195
Epoch 205/500
Epoch 00204: val_loss improved from 0.21268 to 0.21196, saving model to ./bestmodel6.hdf5
3s - loss: 0.2080 - acc: 0.9207 - val_loss: 0.2120 - val_acc: 0.9200
Epoch 206/500
Epoch 00205: val_loss did not improve
3s - loss: 0.2084 - acc: 0.9208 - val_loss: 0.2159 - val_acc: 0.9179
Epoch 207/500
Epoch 00206: val_loss did not improve
3s - loss: 0.2080 - acc: 0.9198 - val_loss: 0.2192 - val_acc: 0.9154
Epoch 208/500
Epoch 00207: val_loss did not improve
3s - loss: 0.2054 - acc: 0.9212 - val_loss: 0.2162 - val_acc: 0.9197
Epoch 209/500
Epoch 00208: val_loss did not improve
3s - loss: 0.2029 - acc: 0.9234 - val_loss: 0.2135 - val_acc: 0.9197
Epoch 210/500
Epoch 00209: val_loss did not improve
3s - loss: 0.2063 - acc: 0.9219 - val_loss: 0.2127 - val_acc: 0.9207
Epoch 211/500
Epoch 00210: val_loss did not improve
3s - loss: 0.2037 - acc: 0.9224 - val_loss: 0.2132 - val_acc: 0.9200
Epoch 212/500
Epoch 00211: val_loss did not improve
3s - loss: 0.2045 - acc: 0.9221 - val_loss: 0.2127 - val_acc: 0.9195
Epoch 213/500
Epoch 00212: val_loss did not improve
3s - loss: 0.2040 - acc: 0.9215 - val_loss: 0.2121 - val_acc: 0.9205
Epoch 214/500
Epoch 00213: val_loss did not improve
3s - loss: 0.2034 - acc: 0.9221 - val_loss: 0.2204 - val_acc: 0.9162
Epoch 215/500
Epoch 00214: val_loss did not improve
3s - loss: 0.2031 - acc: 0.9228 - val_loss: 0.2130 - val_acc: 0.9192
Epoch 216/500
Epoch 00215: val_loss did not improve
3s - loss: 0.2008 - acc: 0.9238 - val_loss: 0.2125 - val_acc: 0.9195
Epoch 217/500
Epoch 00216: val_loss did not improve
3s - loss: 0.2031 - acc: 0.9223 - val_loss: 0.2141 - val_acc: 0.9185
Epoch 218/500
Epoch 00217: val_loss did not improve
3s - loss: 0.2007 - acc: 0.9251 - val_loss: 0.2157 - val_acc: 0.9185
Epoch 219/500
Epoch 00218: val_loss did not improve
3s - loss: 0.2018 - acc: 0.9244 - val_loss: 0.2128 - val_acc: 0.9197
Epoch 220/500
Epoch 00219: val_loss did not improve
3s - loss: 0.2012 - acc: 0.9227 - val_loss: 0.2157 - val_acc: 0.9179
Epoch 221/500
Epoch 00220: val_loss did not improve
3s - loss: 0.1992 - acc: 0.9244 - val_loss: 0.2291 - val_acc: 0.9139
Epoch 222/500
Epoch 00221: val_loss did not improve
3s - loss: 0.2014 - acc: 0.9231 - val_loss: 0.2140 - val_acc: 0.9179
Epoch 223/500
Epoch 00222: val_loss did not improve
3s - loss: 0.1997 - acc: 0.9241 - val_loss: 0.2222 - val_acc: 0.9174
Epoch 224/500
Epoch 00223: val_loss did not improve
3s - loss: 0.1990 - acc: 0.9254 - val_loss: 0.2134 - val_acc: 0.9190
Epoch 225/500
Epoch 00224: val_loss did not improve
3s - loss: 0.1972 - acc: 0.9259 - val_loss: 0.2137 - val_acc: 0.9207
Epoch 226/500
Epoch 00225: val_loss did not improve
3s - loss: 0.1997 - acc: 0.9232 - val_loss: 0.2156 - val_acc: 0.9200
Epoch 227/500
Epoch 00226: val_loss did not improve
3s - loss: 0.1962 - acc: 0.9246 - val_loss: 0.2175 - val_acc: 0.9177
Epoch 228/500
Epoch 00227: val_loss did not improve
3s - loss: 0.1974 - acc: 0.9241 - val_loss: 0.2157 - val_acc: 0.9202
Epoch 229/500
Epoch 00228: val_loss did not improve
3s - loss: 0.1965 - acc: 0.9241 - val_loss: 0.2129 - val_acc: 0.9223
Epoch 230/500
Epoch 00229: val_loss did not improve
3s - loss: 0.1957 - acc: 0.9259 - val_loss: 0.2128 - val_acc: 0.9169
Epoch 231/500
Epoch 00230: val_loss did not improve
3s - loss: 0.1965 - acc: 0.9235 - val_loss: 0.2152 - val_acc: 0.9200
Epoch 232/500
Epoch 00231: val_loss did not improve
3s - loss: 0.1963 - acc: 0.9247 - val_loss: 0.2145 - val_acc: 0.9192
Epoch 233/500
Epoch 00232: val_loss improved from 0.21196 to 0.21146, saving model to ./bestmodel6.hdf5
4s - loss: 0.1968 - acc: 0.9258 - val_loss: 0.2115 - val_acc: 0.9192
Epoch 234/500
Epoch 00233: val_loss did not improve
3s - loss: 0.1941 - acc: 0.9265 - val_loss: 0.2128 - val_acc: 0.9202
Epoch 235/500
Epoch 00234: val_loss did not improve
3s - loss: 0.1951 - acc: 0.9250 - val_loss: 0.2139 - val_acc: 0.9207
Epoch 236/500
Epoch 00235: val_loss did not improve
3s - loss: 0.1930 - acc: 0.9254 - val_loss: 0.2165 - val_acc: 0.9167
Epoch 237/500
Epoch 00236: val_loss did not improve
3s - loss: 0.1963 - acc: 0.9256 - val_loss: 0.2158 - val_acc: 0.9213
Epoch 238/500
Epoch 00237: val_loss did not improve
3s - loss: 0.1953 - acc: 0.9253 - val_loss: 0.2145 - val_acc: 0.9207
Epoch 239/500
Epoch 00238: val_loss did not improve
3s - loss: 0.1935 - acc: 0.9261 - val_loss: 0.2156 - val_acc: 0.9205
Epoch 240/500
Epoch 00239: val_loss did not improve
3s - loss: 0.1891 - acc: 0.9282 - val_loss: 0.2171 - val_acc: 0.9207
Epoch 241/500
Epoch 00240: val_loss did not improve
3s - loss: 0.1908 - acc: 0.9279 - val_loss: 0.2156 - val_acc: 0.9215
Epoch 242/500
Epoch 00241: val_loss did not improve
3s - loss: 0.1958 - acc: 0.9261 - val_loss: 0.2158 - val_acc: 0.9179
Epoch 243/500
Epoch 00242: val_loss did not improve
3s - loss: 0.1908 - acc: 0.9281 - val_loss: 0.2157 - val_acc: 0.9210
Epoch 244/500
Epoch 00243: val_loss did not improve
3s - loss: 0.1922 - acc: 0.9279 - val_loss: 0.2147 - val_acc: 0.9179
Epoch 245/500
Epoch 00244: val_loss did not improve
3s - loss: 0.1913 - acc: 0.9269 - val_loss: 0.2143 - val_acc: 0.9192
Epoch 246/500
Epoch 00245: val_loss did not improve
3s - loss: 0.1913 - acc: 0.9282 - val_loss: 0.2164 - val_acc: 0.9197
Epoch 247/500
Epoch 00246: val_loss did not improve
3s - loss: 0.1893 - acc: 0.9272 - val_loss: 0.2258 - val_acc: 0.9156
Epoch 248/500
Epoch 00247: val_loss did not improve
3s - loss: 0.1893 - acc: 0.9293 - val_loss: 0.2150 - val_acc: 0.9205
Epoch 249/500
Epoch 00248: val_loss did not improve
3s - loss: 0.1909 - acc: 0.9284 - val_loss: 0.2152 - val_acc: 0.9202
Epoch 250/500
Epoch 00249: val_loss did not improve
3s - loss: 0.1887 - acc: 0.9293 - val_loss: 0.2151 - val_acc: 0.9195
Epoch 251/500
Epoch 00250: val_loss did not improve
3s - loss: 0.1904 - acc: 0.9278 - val_loss: 0.2153 - val_acc: 0.9179
Epoch 252/500
Epoch 00251: val_loss did not improve
3s - loss: 0.1905 - acc: 0.9270 - val_loss: 0.2138 - val_acc: 0.9179
Epoch 253/500
Epoch 00252: val_loss did not improve
3s - loss: 0.1893 - acc: 0.9285 - val_loss: 0.2182 - val_acc: 0.9195
Epoch 254/500
Epoch 00253: val_loss did not improve
3s - loss: 0.1891 - acc: 0.9278 - val_loss: 0.2129 - val_acc: 0.9177
Epoch 255/500
Epoch 00254: val_loss did not improve
3s - loss: 0.1862 - acc: 0.9292 - val_loss: 0.2145 - val_acc: 0.9200
Epoch 256/500
Epoch 00255: val_loss did not improve
3s - loss: 0.1878 - acc: 0.9289 - val_loss: 0.2152 - val_acc: 0.9207
Epoch 257/500
Epoch 00256: val_loss did not improve
3s - loss: 0.1854 - acc: 0.9287 - val_loss: 0.2182 - val_acc: 0.9169
Epoch 258/500
Epoch 00257: val_loss did not improve
3s - loss: 0.1856 - acc: 0.9301 - val_loss: 0.2151 - val_acc: 0.9205
Epoch 259/500
Epoch 00258: val_loss did not improve
3s - loss: 0.1873 - acc: 0.9283 - val_loss: 0.2159 - val_acc: 0.9179
Epoch 260/500
Epoch 00259: val_loss did not improve
3s - loss: 0.1865 - acc: 0.9286 - val_loss: 0.2189 - val_acc: 0.9182
Epoch 261/500
Epoch 00260: val_loss did not improve
3s - loss: 0.1844 - acc: 0.9298 - val_loss: 0.2150 - val_acc: 0.9197
Epoch 262/500
Epoch 00261: val_loss did not improve
3s - loss: 0.1851 - acc: 0.9303 - val_loss: 0.2147 - val_acc: 0.9195
Epoch 263/500
Epoch 00262: val_loss did not improve
3s - loss: 0.1833 - acc: 0.9318 - val_loss: 0.2148 - val_acc: 0.9192
Epoch 264/500
Epoch 00263: val_loss did not improve
3s - loss: 0.1822 - acc: 0.9307 - val_loss: 0.2130 - val_acc: 0.9195
Epoch 265/500
Epoch 00264: val_loss did not improve
3s - loss: 0.1824 - acc: 0.9310 - val_loss: 0.2250 - val_acc: 0.9169
Epoch 266/500
Epoch 00265: val_loss did not improve
3s - loss: 0.1822 - acc: 0.9305 - val_loss: 0.2132 - val_acc: 0.9200
Epoch 267/500
Epoch 00266: val_loss did not improve
3s - loss: 0.1811 - acc: 0.9312 - val_loss: 0.2154 - val_acc: 0.9195
Epoch 268/500
Epoch 00267: val_loss did not improve
3s - loss: 0.1808 - acc: 0.9303 - val_loss: 0.2188 - val_acc: 0.9177
Epoch 269/500
Epoch 00268: val_loss did not improve
3s - loss: 0.1818 - acc: 0.9296 - val_loss: 0.2182 - val_acc: 0.9202
Epoch 270/500
Epoch 00269: val_loss did not improve
3s - loss: 0.1816 - acc: 0.9309 - val_loss: 0.2214 - val_acc: 0.9177
Epoch 271/500
Epoch 00270: val_loss did not improve
3s - loss: 0.1821 - acc: 0.9310 - val_loss: 0.2148 - val_acc: 0.9215
Epoch 272/500
Epoch 00271: val_loss did not improve
3s - loss: 0.1838 - acc: 0.9308 - val_loss: 0.2161 - val_acc: 0.9202
Epoch 273/500
Epoch 00272: val_loss did not improve
3s - loss: 0.1792 - acc: 0.9313 - val_loss: 0.2162 - val_acc: 0.9213
Epoch 274/500
Epoch 00273: val_loss did not improve
3s - loss: 0.1791 - acc: 0.9318 - val_loss: 0.2190 - val_acc: 0.9200
Epoch 275/500
Epoch 00274: val_loss did not improve
3s - loss: 0.1803 - acc: 0.9318 - val_loss: 0.2192 - val_acc: 0.9190
Epoch 276/500
Epoch 00275: val_loss did not improve
3s - loss: 0.1792 - acc: 0.9309 - val_loss: 0.2169 - val_acc: 0.9200
Epoch 277/500
Epoch 00276: val_loss did not improve
3s - loss: 0.1767 - acc: 0.9324 - val_loss: 0.2185 - val_acc: 0.9210
Epoch 278/500
Epoch 00277: val_loss did not improve
3s - loss: 0.1822 - acc: 0.9305 - val_loss: 0.2170 - val_acc: 0.9192
Epoch 279/500
Epoch 00278: val_loss did not improve
3s - loss: 0.1779 - acc: 0.9323 - val_loss: 0.2269 - val_acc: 0.9149
Epoch 280/500
Epoch 00279: val_loss did not improve
3s - loss: 0.1762 - acc: 0.9331 - val_loss: 0.2196 - val_acc: 0.9210
Epoch 281/500
Epoch 00280: val_loss did not improve
3s - loss: 0.1781 - acc: 0.9320 - val_loss: 0.2203 - val_acc: 0.9177
Epoch 282/500
Epoch 00281: val_loss did not improve
3s - loss: 0.1772 - acc: 0.9313 - val_loss: 0.2215 - val_acc: 0.9185
Epoch 283/500
Epoch 00282: val_loss did not improve
3s - loss: 0.1750 - acc: 0.9320 - val_loss: 0.2226 - val_acc: 0.9177
Epoch 284/500
Epoch 00283: val_loss did not improve
Epoch 00283: early stopping
3s - loss: 0.1777 - acc: 0.9335 - val_loss: 0.2179 - val_acc: 0.9192
training done!
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
cov1 (Convolution1D)             (None, 58, 80)        1360        convolution1d_input_2[0][0]      
____________________________________________________________________________________________________
leakyrelu_1 (LeakyReLU)          (None, 58, 80)        0           cov1[0][0]                       
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 58, 80)        0           leakyrelu_1[0][0]                
____________________________________________________________________________________________________
cov2 (Convolution1D)             (None, 57, 80)        12880       dropout_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_2 (LeakyReLU)          (None, 57, 80)        0           cov2[0][0]                       
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 57, 80)        0           leakyrelu_2[0][0]                
____________________________________________________________________________________________________
cov3 (Convolution1D)             (None, 54, 80)        25680       dropout_2[0][0]                  
____________________________________________________________________________________________________
leakyrelu_3 (LeakyReLU)          (None, 54, 80)        0           cov3[0][0]                       
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 54, 80)        0           leakyrelu_3[0][0]                
____________________________________________________________________________________________________
cov4 (Convolution1D)             (None, 51, 80)        25680       dropout_3[0][0]                  
____________________________________________________________________________________________________
leakyrelu_4 (LeakyReLU)          (None, 51, 80)        0           cov4[0][0]                       
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 51, 80)        0           leakyrelu_4[0][0]                
____________________________________________________________________________________________________
cov5 (Convolution1D)             (None, 48, 80)        25680       dropout_4[0][0]                  
____________________________________________________________________________________________________
leakyrelu_5 (LeakyReLU)          (None, 48, 80)        0           cov5[0][0]                       
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 48, 80)        0           leakyrelu_5[0][0]                
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 3840)          0           dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           384100      flatten_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_6 (LeakyReLU)          (None, 100)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 100)           0           leakyrelu_6[0][0]                
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             101         dropout_6[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    
====================================================================================================
Total params: 475481
____________________________________________________________________________________________________
**************vadiation results on validation dataset************
  32/3924 [..............................] - ETA: 0s  64/3924 [..............................] - ETA: 15s 160/3924 [>.............................] - ETA: 6s  256/3924 [>.............................] - ETA: 4s 352/3924 [=>............................] - ETA: 2s 448/3924 [==>...........................] - ETA: 2s 544/3924 [===>..........................] - ETA: 1s 640/3924 [===>..........................] - ETA: 1s 736/3924 [====>.........................] - ETA: 1s 832/3924 [=====>........................] - ETA: 1s 928/3924 [======>.......................] - ETA: 1s1024/3924 [======>.......................] - ETA: 1s1120/3924 [=======>......................] - ETA: 0s1216/3924 [========>.....................] - ETA: 0s1312/3924 [=========>....................] - ETA: 0s1408/3924 [=========>....................] - ETA: 0s1504/3924 [==========>...................] - ETA: 0s1600/3924 [===========>..................] - ETA: 0s1696/3924 [===========>..................] - ETA: 0s1792/3924 [============>.................] - ETA: 0s1888/3924 [=============>................] - ETA: 0s1984/3924 [==============>...............] - ETA: 0s2080/3924 [==============>...............] - ETA: 0s2176/3924 [===============>..............] - ETA: 0s2272/3924 [================>.............] - ETA: 0s2368/3924 [=================>............] - ETA: 0s2464/3924 [=================>............] - ETA: 0s2560/3924 [==================>...........] - ETA: 0s2656/3924 [===================>..........] - ETA: 0s2752/3924 [====================>.........] - ETA: 0s2848/3924 [====================>.........] - ETA: 0s2944/3924 [=====================>........] - ETA: 0s3040/3924 [======================>.......] - ETA: 0s3168/3924 [=======================>......] - ETA: 0s3296/3924 [========================>.....] - ETA: 0s3424/3924 [=========================>....] - ETA: 0s3552/3924 [==========================>...] - ETA: 0s3680/3924 [===========================>..] - ETA: 0s3808/3924 [============================>.] - ETA: 0s3924/3924 [==============================] - 0s     
[0.21145508299362403, 0.91921508652476114]
  32/3924 [..............................] - ETA: 0s 160/3924 [>.............................] - ETA: 0s 288/3924 [=>............................] - ETA: 0s 416/3924 [==>...........................] - ETA: 0s 544/3924 [===>..........................] - ETA: 0s 672/3924 [====>.........................] - ETA: 0s 800/3924 [=====>........................] - ETA: 0s 928/3924 [======>.......................] - ETA: 0s1056/3924 [=======>......................] - ETA: 0s1184/3924 [========>.....................] - ETA: 0s1312/3924 [=========>....................] - ETA: 0s1440/3924 [==========>...................] - ETA: 0s1568/3924 [==========>...................] - ETA: 0s1696/3924 [===========>..................] - ETA: 0s1824/3924 [============>.................] - ETA: 0s1952/3924 [=============>................] - ETA: 0s2080/3924 [==============>...............] - ETA: 0s2208/3924 [===============>..............] - ETA: 0s2336/3924 [================>.............] - ETA: 0s2464/3924 [=================>............] - ETA: 0s2592/3924 [==================>...........] - ETA: 0s2720/3924 [===================>..........] - ETA: 0s2848/3924 [====================>.........] - ETA: 0s2976/3924 [=====================>........] - ETA: 0s3104/3924 [======================>.......] - ETA: 0s3232/3924 [=======================>......] - ETA: 0s3360/3924 [========================>.....] - ETA: 0s3488/3924 [=========================>....] - ETA: 0s3616/3924 [==========================>...] - ETA: 0s3776/3924 [===========================>..] - ETA: 0s3924/3924 [==============================] - 0s     
  32/3924 [..............................] - ETA: 0s 192/3924 [>.............................] - ETA: 0s 352/3924 [=>............................] - ETA: 0s 512/3924 [==>...........................] - ETA: 0s 672/3924 [====>.........................] - ETA: 0s 832/3924 [=====>........................] - ETA: 0s 992/3924 [======>.......................] - ETA: 0s1152/3924 [=======>......................] - ETA: 0s1312/3924 [=========>....................] - ETA: 0s1472/3924 [==========>...................] - ETA: 0s1632/3924 [===========>..................] - ETA: 0s1792/3924 [============>.................] - ETA: 0s1952/3924 [=============>................] - ETA: 0s2112/3924 [===============>..............] - ETA: 0s2272/3924 [================>.............] - ETA: 0s2432/3924 [=================>............] - ETA: 0s2592/3924 [==================>...........] - ETA: 0s2752/3924 [====================>.........] - ETA: 0s2912/3924 [=====================>........] - ETA: 0s3072/3924 [======================>.......] - ETA: 0s3232/3924 [=======================>......] - ETA: 0s3392/3924 [========================>.....] - ETA: 0s3552/3924 [==========================>...] - ETA: 0s3712/3924 [===========================>..] - ETA: 0s3872/3924 [============================>.] - ETA: 0s************************
auc: 0.971626292005
mcc: 0.839145598979
negative ---> precision:0.90259422418, recall:0.939857288481, f1score:0.920848938826, support:1962
positive ---> precision:0.937267410952, recall:0.898572884811, f1score:0.917512360135, support:1962
************************
**************prediction results on test dataset************
  32/3932 [..............................] - ETA: 0s 160/3932 [>.............................] - ETA: 0s 288/3932 [=>............................] - ETA: 0s 416/3932 [==>...........................] - ETA: 0s 544/3932 [===>..........................] - ETA: 0s 672/3932 [====>.........................] - ETA: 0s 800/3932 [=====>........................] - ETA: 0s 928/3932 [======>.......................] - ETA: 0s1056/3932 [=======>......................] - ETA: 0s1184/3932 [========>.....................] - ETA: 0s1312/3932 [=========>....................] - ETA: 0s1440/3932 [=========>....................] - ETA: 0s1568/3932 [==========>...................] - ETA: 0s1696/3932 [===========>..................] - ETA: 0s1824/3932 [============>.................] - ETA: 0s1952/3932 [=============>................] - ETA: 0s2080/3932 [==============>...............] - ETA: 0s2208/3932 [===============>..............] - ETA: 0s2336/3932 [================>.............] - ETA: 0s2464/3932 [=================>............] - ETA: 0s2592/3932 [==================>...........] - ETA: 0s2720/3932 [===================>..........] - ETA: 0s2848/3932 [====================>.........] - ETA: 0s2976/3932 [=====================>........] - ETA: 0s3104/3932 [======================>.......] - ETA: 0s3232/3932 [=======================>......] - ETA: 0s3360/3932 [========================>.....] - ETA: 0s3488/3932 [=========================>....] - ETA: 0s3616/3932 [==========================>...] - ETA: 0s3744/3932 [===========================>..] - ETA: 0s3872/3932 [============================>.] - ETA: 0s[0.23926472298550824, 0.91480162773103224]
  32/3932 [..............................] - ETA: 0s 192/3932 [>.............................] - ETA: 0s 352/3932 [=>............................] - ETA: 0s 512/3932 [==>...........................] - ETA: 0s 672/3932 [====>.........................] - ETA: 0s 832/3932 [=====>........................] - ETA: 0s 992/3932 [======>.......................] - ETA: 0s1152/3932 [=======>......................] - ETA: 0s1312/3932 [=========>....................] - ETA: 0s1472/3932 [==========>...................] - ETA: 0s1632/3932 [===========>..................] - ETA: 0s1792/3932 [============>.................] - ETA: 0s1952/3932 [=============>................] - ETA: 0s2112/3932 [===============>..............] - ETA: 0s2272/3932 [================>.............] - ETA: 0s2432/3932 [=================>............] - ETA: 0s2592/3932 [==================>...........] - ETA: 0s2752/3932 [===================>..........] - ETA: 0s2912/3932 [=====================>........] - ETA: 0s3072/3932 [======================>.......] - ETA: 0s3232/3932 [=======================>......] - ETA: 0s3392/3932 [========================>.....] - ETA: 0s3552/3932 [==========================>...] - ETA: 0s3712/3932 [===========================>..] - ETA: 0s3872/3932 [============================>.] - ETA: 0s  32/3932 [..............................] - ETA: 0s 192/3932 [>.............................] - ETA: 0s 352/3932 [=>............................] - ETA: 0s 512/3932 [==>...........................] - ETA: 0s 672/3932 [====>.........................] - ETA: 0s 832/3932 [=====>........................] - ETA: 0s 992/3932 [======>.......................] - ETA: 0s1152/3932 [=======>......................] - ETA: 0s1312/3932 [=========>....................] - ETA: 0s1472/3932 [==========>...................] - ETA: 0s1632/3932 [===========>..................] - ETA: 0s1792/3932 [============>.................] - ETA: 0s1952/3932 [=============>................] - ETA: 0s2112/3932 [===============>..............] - ETA: 0s2272/3932 [================>.............] - ETA: 0s2432/3932 [=================>............] - ETA: 0s2592/3932 [==================>...........] - ETA: 0s2752/3932 [===================>..........] - ETA: 0s2912/3932 [=====================>........] - ETA: 0s3072/3932 [======================>.......] - ETA: 0s3232/3932 [=======================>......] - ETA: 0s3392/3932 [========================>.....] - ETA: 0s3552/3932 [==========================>...] - ETA: 0s3712/3932 [===========================>..] - ETA: 0s3872/3932 [============================>.] - ETA: 0s************************
auc: 0.964786673552
mcc: 0.830700180161
negative ---> precision:0.894533139816, recall:0.940488301119, f1score:0.916935283908, support:1966
positive ---> precision:0.93726541555, recall:0.889114954222, f1score:0.912555468546, support:1966
************************
