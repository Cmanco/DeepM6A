Using Theano backend.
INFO (theano.gof.compilelock): Waiting for existing lock by process '104015' (I am process '105759')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '105005' (I am process '105759')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '105855' (I am process '105759')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '105759')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '192674' (I am process '105759')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '105875' (I am process '105759')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '105906' (I am process '105759')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
INFO (theano.gof.compilelock): Waiting for existing lock by process '192763' (I am process '105759')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 10.0% of memory, cuDNN 5005)
loading data
2
(31411, 61, 4)
('train_label: count', (array([0, 1]), array([15705, 15706])))
('valid_label: count', (array([0, 1]), array([1963, 1963])))
('test_label: count', (array([0, 1]), array([1963, 1963])))
building model...............
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
cov1 (Convolution1D)             (None, 58, 80)        1360        convolution1d_input_1[0][0]      
____________________________________________________________________________________________________
leakyrelu_1 (LeakyReLU)          (None, 58, 80)        0           cov1[0][0]                       
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 58, 80)        0           leakyrelu_1[0][0]                
____________________________________________________________________________________________________
cov2 (Convolution1D)             (None, 57, 80)        12880       dropout_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_2 (LeakyReLU)          (None, 57, 80)        0           cov2[0][0]                       
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 57, 80)        0           leakyrelu_2[0][0]                
____________________________________________________________________________________________________
cov3 (Convolution1D)             (None, 54, 80)        25680       dropout_2[0][0]                  
____________________________________________________________________________________________________
leakyrelu_3 (LeakyReLU)          (None, 54, 80)        0           cov3[0][0]                       
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 54, 80)        0           leakyrelu_3[0][0]                
____________________________________________________________________________________________________
cov4 (Convolution1D)             (None, 51, 80)        25680       dropout_3[0][0]                  
____________________________________________________________________________________________________
leakyrelu_4 (LeakyReLU)          (None, 51, 80)        0           cov4[0][0]                       
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 51, 80)        0           leakyrelu_4[0][0]                
____________________________________________________________________________________________________
cov5 (Convolution1D)             (None, 48, 80)        25680       dropout_4[0][0]                  
____________________________________________________________________________________________________
leakyrelu_5 (LeakyReLU)          (None, 48, 80)        0           cov5[0][0]                       
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 48, 80)        0           leakyrelu_5[0][0]                
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 3840)          0           dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           384100      flatten_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_6 (LeakyReLU)          (None, 100)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 100)           0           leakyrelu_6[0][0]                
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             101         dropout_6[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    
====================================================================================================
Total params: 475481
____________________________________________________________________________________________________
compiling and fitting model...........
Train on 31411 samples, validate on 3926 samples
Epoch 1/500
Epoch 00000: val_loss improved from inf to 0.69313, saving model to ./bestmodel2.hdf5
4s - loss: 0.6932 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 2/500
Epoch 00001: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 3/500
Epoch 00002: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 4/500
Epoch 00003: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 5/500
Epoch 00004: val_loss did not improve
3s - loss: 0.6933 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 6/500
Epoch 00005: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 7/500
Epoch 00006: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 8/500
Epoch 00007: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 9/500
Epoch 00008: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4970 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 10/500
Epoch 00009: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5031 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 11/500
Epoch 00010: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 12/500
Epoch 00011: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 13/500
Epoch 00012: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 14/500
Epoch 00013: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 15/500
Epoch 00014: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 16/500
Epoch 00015: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 17/500
Epoch 00016: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5048 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 18/500
Epoch 00017: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 19/500
Epoch 00018: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 20/500
Epoch 00019: val_loss improved from 0.69313 to 0.69313, saving model to ./bestmodel2.hdf5
4s - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 21/500
Epoch 00020: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5038 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 22/500
Epoch 00021: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 23/500
Epoch 00022: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 24/500
Epoch 00023: val_loss improved from 0.69313 to 0.69312, saving model to ./bestmodel2.hdf5
3s - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 25/500
Epoch 00024: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 26/500
Epoch 00025: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 27/500
Epoch 00026: val_loss improved from 0.69312 to 0.69312, saving model to ./bestmodel2.hdf5
3s - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 28/500
Epoch 00027: val_loss improved from 0.69312 to 0.69312, saving model to ./bestmodel2.hdf5
4s - loss: 0.6931 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 29/500
Epoch 00028: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 30/500
Epoch 00029: val_loss improved from 0.69312 to 0.69307, saving model to ./bestmodel2.hdf5
3s - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 31/500
Epoch 00030: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5066 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 32/500
Epoch 00031: val_loss improved from 0.69307 to 0.69299, saving model to ./bestmodel2.hdf5
5s - loss: 0.6931 - acc: 0.5053 - val_loss: 0.6930 - val_acc: 0.5000
Epoch 33/500
Epoch 00032: val_loss improved from 0.69299 to 0.69291, saving model to ./bestmodel2.hdf5
4s - loss: 0.6931 - acc: 0.5047 - val_loss: 0.6929 - val_acc: 0.5000
Epoch 34/500
Epoch 00033: val_loss improved from 0.69291 to 0.69269, saving model to ./bestmodel2.hdf5
4s - loss: 0.6929 - acc: 0.5162 - val_loss: 0.6927 - val_acc: 0.5092
Epoch 35/500
Epoch 00034: val_loss improved from 0.69269 to 0.69207, saving model to ./bestmodel2.hdf5
3s - loss: 0.6925 - acc: 0.5281 - val_loss: 0.6921 - val_acc: 0.5048
Epoch 36/500
Epoch 00035: val_loss improved from 0.69207 to 0.68860, saving model to ./bestmodel2.hdf5
4s - loss: 0.6911 - acc: 0.5584 - val_loss: 0.6886 - val_acc: 0.6977
Epoch 37/500
Epoch 00036: val_loss improved from 0.68860 to 0.58764, saving model to ./bestmodel2.hdf5
3s - loss: 0.6675 - acc: 0.6498 - val_loss: 0.5876 - val_acc: 0.7142
Epoch 38/500
Epoch 00037: val_loss improved from 0.58764 to 0.51659, saving model to ./bestmodel2.hdf5
4s - loss: 0.5527 - acc: 0.7286 - val_loss: 0.5166 - val_acc: 0.7580
Epoch 39/500
Epoch 00038: val_loss improved from 0.51659 to 0.47640, saving model to ./bestmodel2.hdf5
3s - loss: 0.5118 - acc: 0.7608 - val_loss: 0.4764 - val_acc: 0.7812
Epoch 40/500
Epoch 00039: val_loss improved from 0.47640 to 0.45183, saving model to ./bestmodel2.hdf5
3s - loss: 0.4828 - acc: 0.7805 - val_loss: 0.4518 - val_acc: 0.7988
Epoch 41/500
Epoch 00040: val_loss did not improve
3s - loss: 0.4679 - acc: 0.7911 - val_loss: 0.4580 - val_acc: 0.7932
Epoch 42/500
Epoch 00041: val_loss improved from 0.45183 to 0.42090, saving model to ./bestmodel2.hdf5
3s - loss: 0.4551 - acc: 0.7964 - val_loss: 0.4209 - val_acc: 0.8194
Epoch 43/500
Epoch 00042: val_loss improved from 0.42090 to 0.40380, saving model to ./bestmodel2.hdf5
3s - loss: 0.4401 - acc: 0.8051 - val_loss: 0.4038 - val_acc: 0.8293
Epoch 44/500
Epoch 00043: val_loss improved from 0.40380 to 0.38939, saving model to ./bestmodel2.hdf5
3s - loss: 0.4274 - acc: 0.8122 - val_loss: 0.3894 - val_acc: 0.8352
Epoch 45/500
Epoch 00044: val_loss improved from 0.38939 to 0.38854, saving model to ./bestmodel2.hdf5
5s - loss: 0.4203 - acc: 0.8161 - val_loss: 0.3885 - val_acc: 0.8347
Epoch 46/500
Epoch 00045: val_loss did not improve
3s - loss: 0.4151 - acc: 0.8189 - val_loss: 0.3916 - val_acc: 0.8327
Epoch 47/500
Epoch 00046: val_loss improved from 0.38854 to 0.37274, saving model to ./bestmodel2.hdf5
5s - loss: 0.4069 - acc: 0.8231 - val_loss: 0.3727 - val_acc: 0.8431
Epoch 48/500
Epoch 00047: val_loss did not improve
3s - loss: 0.4036 - acc: 0.8248 - val_loss: 0.3763 - val_acc: 0.8426
Epoch 49/500
Epoch 00048: val_loss improved from 0.37274 to 0.36332, saving model to ./bestmodel2.hdf5
4s - loss: 0.3987 - acc: 0.8289 - val_loss: 0.3633 - val_acc: 0.8467
Epoch 50/500
Epoch 00049: val_loss improved from 0.36332 to 0.35781, saving model to ./bestmodel2.hdf5
3s - loss: 0.3928 - acc: 0.8308 - val_loss: 0.3578 - val_acc: 0.8538
Epoch 51/500
Epoch 00050: val_loss improved from 0.35781 to 0.35502, saving model to ./bestmodel2.hdf5
3s - loss: 0.3921 - acc: 0.8312 - val_loss: 0.3550 - val_acc: 0.8528
Epoch 52/500
Epoch 00051: val_loss improved from 0.35502 to 0.34952, saving model to ./bestmodel2.hdf5
3s - loss: 0.3871 - acc: 0.8356 - val_loss: 0.3495 - val_acc: 0.8533
Epoch 53/500
Epoch 00052: val_loss improved from 0.34952 to 0.34784, saving model to ./bestmodel2.hdf5
3s - loss: 0.3834 - acc: 0.8388 - val_loss: 0.3478 - val_acc: 0.8579
Epoch 54/500
Epoch 00053: val_loss improved from 0.34784 to 0.34609, saving model to ./bestmodel2.hdf5
4s - loss: 0.3788 - acc: 0.8389 - val_loss: 0.3461 - val_acc: 0.8602
Epoch 55/500
Epoch 00054: val_loss improved from 0.34609 to 0.34291, saving model to ./bestmodel2.hdf5
4s - loss: 0.3676 - acc: 0.8440 - val_loss: 0.3429 - val_acc: 0.8609
Epoch 56/500
Epoch 00055: val_loss improved from 0.34291 to 0.33327, saving model to ./bestmodel2.hdf5
3s - loss: 0.3687 - acc: 0.8451 - val_loss: 0.3333 - val_acc: 0.8591
Epoch 57/500
Epoch 00056: val_loss improved from 0.33327 to 0.32307, saving model to ./bestmodel2.hdf5
3s - loss: 0.3622 - acc: 0.8482 - val_loss: 0.3231 - val_acc: 0.8645
Epoch 58/500
Epoch 00057: val_loss did not improve
3s - loss: 0.3605 - acc: 0.8502 - val_loss: 0.3438 - val_acc: 0.8589
Epoch 59/500
Epoch 00058: val_loss improved from 0.32307 to 0.32193, saving model to ./bestmodel2.hdf5
3s - loss: 0.3571 - acc: 0.8515 - val_loss: 0.3219 - val_acc: 0.8681
Epoch 60/500
Epoch 00059: val_loss did not improve
3s - loss: 0.3515 - acc: 0.8522 - val_loss: 0.3484 - val_acc: 0.8594
Epoch 61/500
Epoch 00060: val_loss improved from 0.32193 to 0.31422, saving model to ./bestmodel2.hdf5
4s - loss: 0.3495 - acc: 0.8553 - val_loss: 0.3142 - val_acc: 0.8675
Epoch 62/500
Epoch 00061: val_loss improved from 0.31422 to 0.31271, saving model to ./bestmodel2.hdf5
5s - loss: 0.3467 - acc: 0.8573 - val_loss: 0.3127 - val_acc: 0.8681
Epoch 63/500
Epoch 00062: val_loss improved from 0.31271 to 0.31182, saving model to ./bestmodel2.hdf5
3s - loss: 0.3440 - acc: 0.8568 - val_loss: 0.3118 - val_acc: 0.8734
Epoch 64/500
Epoch 00063: val_loss improved from 0.31182 to 0.30912, saving model to ./bestmodel2.hdf5
4s - loss: 0.3427 - acc: 0.8579 - val_loss: 0.3091 - val_acc: 0.8762
Epoch 65/500
Epoch 00064: val_loss improved from 0.30912 to 0.30246, saving model to ./bestmodel2.hdf5
4s - loss: 0.3364 - acc: 0.8600 - val_loss: 0.3025 - val_acc: 0.8770
Epoch 66/500
Epoch 00065: val_loss did not improve
3s - loss: 0.3354 - acc: 0.8620 - val_loss: 0.3027 - val_acc: 0.8752
Epoch 67/500
Epoch 00066: val_loss improved from 0.30246 to 0.29773, saving model to ./bestmodel2.hdf5
4s - loss: 0.3325 - acc: 0.8635 - val_loss: 0.2977 - val_acc: 0.8790
Epoch 68/500
Epoch 00067: val_loss improved from 0.29773 to 0.29647, saving model to ./bestmodel2.hdf5
5s - loss: 0.3338 - acc: 0.8622 - val_loss: 0.2965 - val_acc: 0.8795
Epoch 69/500
Epoch 00068: val_loss did not improve
3s - loss: 0.3293 - acc: 0.8638 - val_loss: 0.2986 - val_acc: 0.8780
Epoch 70/500
Epoch 00069: val_loss improved from 0.29647 to 0.29486, saving model to ./bestmodel2.hdf5
3s - loss: 0.3275 - acc: 0.8655 - val_loss: 0.2949 - val_acc: 0.8816
Epoch 71/500
Epoch 00070: val_loss improved from 0.29486 to 0.29354, saving model to ./bestmodel2.hdf5
4s - loss: 0.3225 - acc: 0.8693 - val_loss: 0.2935 - val_acc: 0.8788
Epoch 72/500
Epoch 00071: val_loss improved from 0.29354 to 0.28988, saving model to ./bestmodel2.hdf5
4s - loss: 0.3201 - acc: 0.8685 - val_loss: 0.2899 - val_acc: 0.8831
Epoch 73/500
Epoch 00072: val_loss improved from 0.28988 to 0.28311, saving model to ./bestmodel2.hdf5
3s - loss: 0.3197 - acc: 0.8700 - val_loss: 0.2831 - val_acc: 0.8846
Epoch 74/500
Epoch 00073: val_loss improved from 0.28311 to 0.28284, saving model to ./bestmodel2.hdf5
3s - loss: 0.3170 - acc: 0.8714 - val_loss: 0.2828 - val_acc: 0.8859
Epoch 75/500
Epoch 00074: val_loss did not improve
3s - loss: 0.3102 - acc: 0.8736 - val_loss: 0.2946 - val_acc: 0.8795
Epoch 76/500
Epoch 00075: val_loss did not improve
3s - loss: 0.3079 - acc: 0.8743 - val_loss: 0.2854 - val_acc: 0.8803
Epoch 77/500
Epoch 00076: val_loss improved from 0.28284 to 0.27656, saving model to ./bestmodel2.hdf5
3s - loss: 0.3075 - acc: 0.8736 - val_loss: 0.2766 - val_acc: 0.8864
Epoch 78/500
Epoch 00077: val_loss improved from 0.27656 to 0.26867, saving model to ./bestmodel2.hdf5
3s - loss: 0.3043 - acc: 0.8771 - val_loss: 0.2687 - val_acc: 0.8869
Epoch 79/500
Epoch 00078: val_loss improved from 0.26867 to 0.26598, saving model to ./bestmodel2.hdf5
3s - loss: 0.2997 - acc: 0.8803 - val_loss: 0.2660 - val_acc: 0.8879
Epoch 80/500
Epoch 00079: val_loss improved from 0.26598 to 0.26490, saving model to ./bestmodel2.hdf5
5s - loss: 0.2985 - acc: 0.8792 - val_loss: 0.2649 - val_acc: 0.8874
Epoch 81/500
Epoch 00080: val_loss did not improve
3s - loss: 0.2947 - acc: 0.8790 - val_loss: 0.2654 - val_acc: 0.8905
Epoch 82/500
Epoch 00081: val_loss improved from 0.26490 to 0.26100, saving model to ./bestmodel2.hdf5
3s - loss: 0.2950 - acc: 0.8804 - val_loss: 0.2610 - val_acc: 0.8910
Epoch 83/500
Epoch 00082: val_loss improved from 0.26100 to 0.26074, saving model to ./bestmodel2.hdf5
3s - loss: 0.2934 - acc: 0.8834 - val_loss: 0.2607 - val_acc: 0.8910
Epoch 84/500
Epoch 00083: val_loss did not improve
3s - loss: 0.2907 - acc: 0.8830 - val_loss: 0.2628 - val_acc: 0.8923
Epoch 85/500
Epoch 00084: val_loss improved from 0.26074 to 0.25809, saving model to ./bestmodel2.hdf5
3s - loss: 0.2895 - acc: 0.8844 - val_loss: 0.2581 - val_acc: 0.8912
Epoch 86/500
Epoch 00085: val_loss improved from 0.25809 to 0.25624, saving model to ./bestmodel2.hdf5
3s - loss: 0.2879 - acc: 0.8839 - val_loss: 0.2562 - val_acc: 0.8923
Epoch 87/500
Epoch 00086: val_loss did not improve
3s - loss: 0.2853 - acc: 0.8851 - val_loss: 0.2564 - val_acc: 0.8915
Epoch 88/500
Epoch 00087: val_loss did not improve
3s - loss: 0.2840 - acc: 0.8858 - val_loss: 0.2564 - val_acc: 0.8923
Epoch 89/500
Epoch 00088: val_loss improved from 0.25624 to 0.25236, saving model to ./bestmodel2.hdf5
3s - loss: 0.2851 - acc: 0.8852 - val_loss: 0.2524 - val_acc: 0.8945
Epoch 90/500
Epoch 00089: val_loss improved from 0.25236 to 0.25112, saving model to ./bestmodel2.hdf5
3s - loss: 0.2844 - acc: 0.8859 - val_loss: 0.2511 - val_acc: 0.8953
Epoch 91/500
Epoch 00090: val_loss improved from 0.25112 to 0.25050, saving model to ./bestmodel2.hdf5
4s - loss: 0.2819 - acc: 0.8866 - val_loss: 0.2505 - val_acc: 0.8956
Epoch 92/500
Epoch 00091: val_loss did not improve
3s - loss: 0.2820 - acc: 0.8868 - val_loss: 0.2534 - val_acc: 0.8966
Epoch 93/500
Epoch 00092: val_loss did not improve
3s - loss: 0.2783 - acc: 0.8881 - val_loss: 0.2638 - val_acc: 0.8933
Epoch 94/500
Epoch 00093: val_loss improved from 0.25050 to 0.25009, saving model to ./bestmodel2.hdf5
3s - loss: 0.2790 - acc: 0.8898 - val_loss: 0.2501 - val_acc: 0.8953
Epoch 95/500
Epoch 00094: val_loss improved from 0.25009 to 0.25005, saving model to ./bestmodel2.hdf5
4s - loss: 0.2786 - acc: 0.8886 - val_loss: 0.2501 - val_acc: 0.8996
Epoch 96/500
Epoch 00095: val_loss did not improve
3s - loss: 0.2754 - acc: 0.8913 - val_loss: 0.2514 - val_acc: 0.8981
Epoch 97/500
Epoch 00096: val_loss did not improve
3s - loss: 0.2751 - acc: 0.8896 - val_loss: 0.2603 - val_acc: 0.8956
Epoch 98/500
Epoch 00097: val_loss improved from 0.25005 to 0.24814, saving model to ./bestmodel2.hdf5
3s - loss: 0.2747 - acc: 0.8916 - val_loss: 0.2481 - val_acc: 0.8984
Epoch 99/500
Epoch 00098: val_loss improved from 0.24814 to 0.24635, saving model to ./bestmodel2.hdf5
4s - loss: 0.2755 - acc: 0.8903 - val_loss: 0.2464 - val_acc: 0.8979
Epoch 100/500
Epoch 00099: val_loss did not improve
3s - loss: 0.2730 - acc: 0.8909 - val_loss: 0.2494 - val_acc: 0.9002
Epoch 101/500
Epoch 00100: val_loss improved from 0.24635 to 0.24447, saving model to ./bestmodel2.hdf5
3s - loss: 0.2714 - acc: 0.8926 - val_loss: 0.2445 - val_acc: 0.9002
Epoch 102/500
Epoch 00101: val_loss improved from 0.24447 to 0.24405, saving model to ./bestmodel2.hdf5
4s - loss: 0.2709 - acc: 0.8928 - val_loss: 0.2441 - val_acc: 0.8996
Epoch 103/500
Epoch 00102: val_loss improved from 0.24405 to 0.24368, saving model to ./bestmodel2.hdf5
4s - loss: 0.2699 - acc: 0.8913 - val_loss: 0.2437 - val_acc: 0.9004
Epoch 104/500
Epoch 00103: val_loss improved from 0.24368 to 0.24365, saving model to ./bestmodel2.hdf5
3s - loss: 0.2674 - acc: 0.8929 - val_loss: 0.2436 - val_acc: 0.8986
Epoch 105/500
Epoch 00104: val_loss did not improve
3s - loss: 0.2668 - acc: 0.8943 - val_loss: 0.2483 - val_acc: 0.8994
Epoch 106/500
Epoch 00105: val_loss improved from 0.24365 to 0.24257, saving model to ./bestmodel2.hdf5
3s - loss: 0.2675 - acc: 0.8936 - val_loss: 0.2426 - val_acc: 0.9037
Epoch 107/500
Epoch 00106: val_loss did not improve
3s - loss: 0.2664 - acc: 0.8940 - val_loss: 0.2445 - val_acc: 0.9017
Epoch 108/500
Epoch 00107: val_loss improved from 0.24257 to 0.24247, saving model to ./bestmodel2.hdf5
4s - loss: 0.2648 - acc: 0.8954 - val_loss: 0.2425 - val_acc: 0.9019
Epoch 109/500
Epoch 00108: val_loss improved from 0.24247 to 0.24090, saving model to ./bestmodel2.hdf5
4s - loss: 0.2650 - acc: 0.8936 - val_loss: 0.2409 - val_acc: 0.8999
Epoch 110/500
Epoch 00109: val_loss did not improve
3s - loss: 0.2644 - acc: 0.8966 - val_loss: 0.2581 - val_acc: 0.8966
Epoch 111/500
Epoch 00110: val_loss improved from 0.24090 to 0.24065, saving model to ./bestmodel2.hdf5
5s - loss: 0.2645 - acc: 0.8960 - val_loss: 0.2406 - val_acc: 0.9035
Epoch 112/500
Epoch 00111: val_loss improved from 0.24065 to 0.24001, saving model to ./bestmodel2.hdf5
3s - loss: 0.2624 - acc: 0.8961 - val_loss: 0.2400 - val_acc: 0.9019
Epoch 113/500
Epoch 00112: val_loss improved from 0.24001 to 0.23768, saving model to ./bestmodel2.hdf5
3s - loss: 0.2614 - acc: 0.8976 - val_loss: 0.2377 - val_acc: 0.9030
Epoch 114/500
Epoch 00113: val_loss did not improve
3s - loss: 0.2601 - acc: 0.8977 - val_loss: 0.2401 - val_acc: 0.9035
Epoch 115/500
Epoch 00114: val_loss improved from 0.23768 to 0.23735, saving model to ./bestmodel2.hdf5
3s - loss: 0.2604 - acc: 0.8971 - val_loss: 0.2374 - val_acc: 0.9035
Epoch 116/500
Epoch 00115: val_loss did not improve
3s - loss: 0.2590 - acc: 0.8988 - val_loss: 0.2404 - val_acc: 0.9042
Epoch 117/500
Epoch 00116: val_loss did not improve
3s - loss: 0.2579 - acc: 0.8987 - val_loss: 0.2460 - val_acc: 0.9027
Epoch 118/500
Epoch 00117: val_loss improved from 0.23735 to 0.23684, saving model to ./bestmodel2.hdf5
3s - loss: 0.2599 - acc: 0.8983 - val_loss: 0.2368 - val_acc: 0.9024
Epoch 119/500
Epoch 00118: val_loss did not improve
3s - loss: 0.2567 - acc: 0.8985 - val_loss: 0.2434 - val_acc: 0.9042
Epoch 120/500
Epoch 00119: val_loss did not improve
3s - loss: 0.2559 - acc: 0.9014 - val_loss: 0.2395 - val_acc: 0.9040
Epoch 121/500
Epoch 00120: val_loss improved from 0.23684 to 0.23606, saving model to ./bestmodel2.hdf5
4s - loss: 0.2567 - acc: 0.8987 - val_loss: 0.2361 - val_acc: 0.9058
Epoch 122/500
Epoch 00121: val_loss did not improve
3s - loss: 0.2557 - acc: 0.9004 - val_loss: 0.2401 - val_acc: 0.9045
Epoch 123/500
Epoch 00122: val_loss did not improve
3s - loss: 0.2539 - acc: 0.9011 - val_loss: 0.2387 - val_acc: 0.9052
Epoch 124/500
Epoch 00123: val_loss did not improve
3s - loss: 0.2532 - acc: 0.9008 - val_loss: 0.2377 - val_acc: 0.9063
Epoch 125/500
Epoch 00124: val_loss did not improve
3s - loss: 0.2547 - acc: 0.9004 - val_loss: 0.2398 - val_acc: 0.9052
Epoch 126/500
Epoch 00125: val_loss did not improve
3s - loss: 0.2559 - acc: 0.9005 - val_loss: 0.2390 - val_acc: 0.9068
Epoch 127/500
Epoch 00126: val_loss improved from 0.23606 to 0.23526, saving model to ./bestmodel2.hdf5
5s - loss: 0.2525 - acc: 0.9018 - val_loss: 0.2353 - val_acc: 0.9045
Epoch 128/500
Epoch 00127: val_loss did not improve
3s - loss: 0.2518 - acc: 0.9017 - val_loss: 0.2366 - val_acc: 0.9047
Epoch 129/500
Epoch 00128: val_loss improved from 0.23526 to 0.23344, saving model to ./bestmodel2.hdf5
3s - loss: 0.2503 - acc: 0.9018 - val_loss: 0.2334 - val_acc: 0.9070
Epoch 130/500
Epoch 00129: val_loss improved from 0.23344 to 0.23319, saving model to ./bestmodel2.hdf5
5s - loss: 0.2490 - acc: 0.9023 - val_loss: 0.2332 - val_acc: 0.9078
Epoch 131/500
Epoch 00130: val_loss did not improve
3s - loss: 0.2488 - acc: 0.9040 - val_loss: 0.2402 - val_acc: 0.9060
Epoch 132/500
Epoch 00131: val_loss improved from 0.23319 to 0.23311, saving model to ./bestmodel2.hdf5
3s - loss: 0.2512 - acc: 0.9004 - val_loss: 0.2331 - val_acc: 0.9096
Epoch 133/500
Epoch 00132: val_loss improved from 0.23311 to 0.23151, saving model to ./bestmodel2.hdf5
3s - loss: 0.2486 - acc: 0.9025 - val_loss: 0.2315 - val_acc: 0.9091
Epoch 134/500
Epoch 00133: val_loss did not improve
3s - loss: 0.2467 - acc: 0.9021 - val_loss: 0.2323 - val_acc: 0.9124
Epoch 135/500
Epoch 00134: val_loss did not improve
3s - loss: 0.2494 - acc: 0.9029 - val_loss: 0.2370 - val_acc: 0.9055
Epoch 136/500
Epoch 00135: val_loss did not improve
3s - loss: 0.2439 - acc: 0.9059 - val_loss: 0.2376 - val_acc: 0.9083
Epoch 137/500
Epoch 00136: val_loss did not improve
3s - loss: 0.2472 - acc: 0.9033 - val_loss: 0.2328 - val_acc: 0.9086
Epoch 138/500
Epoch 00137: val_loss did not improve
3s - loss: 0.2454 - acc: 0.9037 - val_loss: 0.2317 - val_acc: 0.9096
Epoch 139/500
Epoch 00138: val_loss did not improve
3s - loss: 0.2427 - acc: 0.9054 - val_loss: 0.2355 - val_acc: 0.9088
Epoch 140/500
Epoch 00139: val_loss did not improve
3s - loss: 0.2447 - acc: 0.9057 - val_loss: 0.2331 - val_acc: 0.9070
Epoch 141/500
Epoch 00140: val_loss did not improve
3s - loss: 0.2444 - acc: 0.9037 - val_loss: 0.2397 - val_acc: 0.9078
Epoch 142/500
Epoch 00141: val_loss improved from 0.23151 to 0.22893, saving model to ./bestmodel2.hdf5
5s - loss: 0.2434 - acc: 0.9064 - val_loss: 0.2289 - val_acc: 0.9103
Epoch 143/500
Epoch 00142: val_loss did not improve
3s - loss: 0.2416 - acc: 0.9050 - val_loss: 0.2320 - val_acc: 0.9109
Epoch 144/500
Epoch 00143: val_loss did not improve
3s - loss: 0.2432 - acc: 0.9047 - val_loss: 0.2318 - val_acc: 0.9126
Epoch 145/500
Epoch 00144: val_loss did not improve
3s - loss: 0.2386 - acc: 0.9085 - val_loss: 0.2329 - val_acc: 0.9109
Epoch 146/500
Epoch 00145: val_loss improved from 0.22893 to 0.22682, saving model to ./bestmodel2.hdf5
5s - loss: 0.2395 - acc: 0.9078 - val_loss: 0.2268 - val_acc: 0.9119
Epoch 147/500
Epoch 00146: val_loss did not improve
3s - loss: 0.2405 - acc: 0.9057 - val_loss: 0.2423 - val_acc: 0.9050
Epoch 148/500
Epoch 00147: val_loss did not improve
3s - loss: 0.2392 - acc: 0.9078 - val_loss: 0.2339 - val_acc: 0.9070
Epoch 149/500
Epoch 00148: val_loss did not improve
3s - loss: 0.2389 - acc: 0.9066 - val_loss: 0.2292 - val_acc: 0.9124
Epoch 150/500
Epoch 00149: val_loss did not improve
3s - loss: 0.2392 - acc: 0.9071 - val_loss: 0.2352 - val_acc: 0.9096
Epoch 151/500
Epoch 00150: val_loss did not improve
3s - loss: 0.2393 - acc: 0.9065 - val_loss: 0.2285 - val_acc: 0.9121
Epoch 152/500
Epoch 00151: val_loss did not improve
3s - loss: 0.2368 - acc: 0.9087 - val_loss: 0.2279 - val_acc: 0.9139
Epoch 153/500
Epoch 00152: val_loss did not improve
3s - loss: 0.2390 - acc: 0.9071 - val_loss: 0.2273 - val_acc: 0.9119
Epoch 154/500
Epoch 00153: val_loss did not improve
3s - loss: 0.2361 - acc: 0.9094 - val_loss: 0.2269 - val_acc: 0.9137
Epoch 155/500
Epoch 00154: val_loss did not improve
3s - loss: 0.2355 - acc: 0.9101 - val_loss: 0.2324 - val_acc: 0.9119
Epoch 156/500
Epoch 00155: val_loss improved from 0.22682 to 0.22483, saving model to ./bestmodel2.hdf5
3s - loss: 0.2340 - acc: 0.9099 - val_loss: 0.2248 - val_acc: 0.9134
Epoch 157/500
Epoch 00156: val_loss did not improve
3s - loss: 0.2350 - acc: 0.9093 - val_loss: 0.2283 - val_acc: 0.9111
Epoch 158/500
Epoch 00157: val_loss did not improve
3s - loss: 0.2344 - acc: 0.9098 - val_loss: 0.2298 - val_acc: 0.9142
Epoch 159/500
Epoch 00158: val_loss did not improve
3s - loss: 0.2335 - acc: 0.9093 - val_loss: 0.2259 - val_acc: 0.9124
Epoch 160/500
Epoch 00159: val_loss did not improve
3s - loss: 0.2301 - acc: 0.9101 - val_loss: 0.2344 - val_acc: 0.9096
Epoch 161/500
Epoch 00160: val_loss did not improve
3s - loss: 0.2320 - acc: 0.9101 - val_loss: 0.2290 - val_acc: 0.9134
Epoch 162/500
Epoch 00161: val_loss did not improve
3s - loss: 0.2323 - acc: 0.9096 - val_loss: 0.2309 - val_acc: 0.9088
Epoch 163/500
Epoch 00162: val_loss did not improve
3s - loss: 0.2310 - acc: 0.9100 - val_loss: 0.2259 - val_acc: 0.9157
Epoch 164/500
Epoch 00163: val_loss did not improve
3s - loss: 0.2311 - acc: 0.9091 - val_loss: 0.2256 - val_acc: 0.9144
Epoch 165/500
Epoch 00164: val_loss did not improve
3s - loss: 0.2285 - acc: 0.9109 - val_loss: 0.2391 - val_acc: 0.9093
Epoch 166/500
Epoch 00165: val_loss improved from 0.22483 to 0.22186, saving model to ./bestmodel2.hdf5
4s - loss: 0.2304 - acc: 0.9118 - val_loss: 0.2219 - val_acc: 0.9157
Epoch 167/500
Epoch 00166: val_loss did not improve
3s - loss: 0.2266 - acc: 0.9132 - val_loss: 0.2274 - val_acc: 0.9162
Epoch 168/500
Epoch 00167: val_loss did not improve
3s - loss: 0.2286 - acc: 0.9124 - val_loss: 0.2260 - val_acc: 0.9139
Epoch 169/500
Epoch 00168: val_loss did not improve
3s - loss: 0.2302 - acc: 0.9097 - val_loss: 0.2235 - val_acc: 0.9167
Epoch 170/500
Epoch 00169: val_loss did not improve
3s - loss: 0.2282 - acc: 0.9122 - val_loss: 0.2339 - val_acc: 0.9137
Epoch 171/500
Epoch 00170: val_loss did not improve
3s - loss: 0.2291 - acc: 0.9103 - val_loss: 0.2301 - val_acc: 0.9096
Epoch 172/500
Epoch 00171: val_loss did not improve
3s - loss: 0.2254 - acc: 0.9114 - val_loss: 0.2258 - val_acc: 0.9101
Epoch 173/500
Epoch 00172: val_loss did not improve
3s - loss: 0.2240 - acc: 0.9140 - val_loss: 0.2279 - val_acc: 0.9126
Epoch 174/500
Epoch 00173: val_loss did not improve
3s - loss: 0.2244 - acc: 0.9141 - val_loss: 0.2266 - val_acc: 0.9131
Epoch 175/500
Epoch 00174: val_loss did not improve
3s - loss: 0.2248 - acc: 0.9136 - val_loss: 0.2220 - val_acc: 0.9154
Epoch 176/500
Epoch 00175: val_loss did not improve
3s - loss: 0.2258 - acc: 0.9145 - val_loss: 0.2231 - val_acc: 0.9152
Epoch 177/500
Epoch 00176: val_loss did not improve
3s - loss: 0.2248 - acc: 0.9138 - val_loss: 0.2229 - val_acc: 0.9131
Epoch 178/500
Epoch 00177: val_loss did not improve
3s - loss: 0.2244 - acc: 0.9135 - val_loss: 0.2297 - val_acc: 0.9111
Epoch 179/500
Epoch 00178: val_loss did not improve
3s - loss: 0.2230 - acc: 0.9140 - val_loss: 0.2229 - val_acc: 0.9131
Epoch 180/500
Epoch 00179: val_loss improved from 0.22186 to 0.22134, saving model to ./bestmodel2.hdf5
3s - loss: 0.2222 - acc: 0.9138 - val_loss: 0.2213 - val_acc: 0.9152
Epoch 181/500
Epoch 00180: val_loss did not improve
3s - loss: 0.2225 - acc: 0.9154 - val_loss: 0.2219 - val_acc: 0.9149
Epoch 182/500
Epoch 00181: val_loss did not improve
3s - loss: 0.2218 - acc: 0.9145 - val_loss: 0.2231 - val_acc: 0.9149
Epoch 183/500
Epoch 00182: val_loss did not improve
3s - loss: 0.2210 - acc: 0.9130 - val_loss: 0.2229 - val_acc: 0.9157
Epoch 184/500
Epoch 00183: val_loss did not improve
3s - loss: 0.2187 - acc: 0.9169 - val_loss: 0.2252 - val_acc: 0.9144
Epoch 185/500
Epoch 00184: val_loss did not improve
3s - loss: 0.2208 - acc: 0.9145 - val_loss: 0.2238 - val_acc: 0.9154
Epoch 186/500
Epoch 00185: val_loss improved from 0.22134 to 0.22053, saving model to ./bestmodel2.hdf5
4s - loss: 0.2202 - acc: 0.9159 - val_loss: 0.2205 - val_acc: 0.9159
Epoch 187/500
Epoch 00186: val_loss did not improve
3s - loss: 0.2205 - acc: 0.9156 - val_loss: 0.2237 - val_acc: 0.9157
Epoch 188/500
Epoch 00187: val_loss did not improve
3s - loss: 0.2171 - acc: 0.9160 - val_loss: 0.2268 - val_acc: 0.9131
Epoch 189/500
Epoch 00188: val_loss did not improve
3s - loss: 0.2168 - acc: 0.9167 - val_loss: 0.2284 - val_acc: 0.9137
Epoch 190/500
Epoch 00189: val_loss did not improve
3s - loss: 0.2151 - acc: 0.9185 - val_loss: 0.2219 - val_acc: 0.9126
Epoch 191/500
Epoch 00190: val_loss did not improve
3s - loss: 0.2192 - acc: 0.9172 - val_loss: 0.2250 - val_acc: 0.9134
Epoch 192/500
Epoch 00191: val_loss did not improve
3s - loss: 0.2167 - acc: 0.9160 - val_loss: 0.2241 - val_acc: 0.9159
Epoch 193/500
Epoch 00192: val_loss did not improve
3s - loss: 0.2155 - acc: 0.9183 - val_loss: 0.2221 - val_acc: 0.9157
Epoch 194/500
Epoch 00193: val_loss improved from 0.22053 to 0.22048, saving model to ./bestmodel2.hdf5
3s - loss: 0.2160 - acc: 0.9187 - val_loss: 0.2205 - val_acc: 0.9185
Epoch 195/500
Epoch 00194: val_loss did not improve
3s - loss: 0.2168 - acc: 0.9163 - val_loss: 0.2247 - val_acc: 0.9149
Epoch 196/500
Epoch 00195: val_loss did not improve
3s - loss: 0.2142 - acc: 0.9181 - val_loss: 0.2232 - val_acc: 0.9154
Epoch 197/500
Epoch 00196: val_loss did not improve
3s - loss: 0.2147 - acc: 0.9164 - val_loss: 0.2214 - val_acc: 0.9137
Epoch 198/500
Epoch 00197: val_loss did not improve
3s - loss: 0.2141 - acc: 0.9169 - val_loss: 0.2234 - val_acc: 0.9157
Epoch 199/500
Epoch 00198: val_loss did not improve
3s - loss: 0.2116 - acc: 0.9183 - val_loss: 0.2206 - val_acc: 0.9149
Epoch 200/500
Epoch 00199: val_loss did not improve
3s - loss: 0.2148 - acc: 0.9161 - val_loss: 0.2214 - val_acc: 0.9162
Epoch 201/500
Epoch 00200: val_loss did not improve
3s - loss: 0.2104 - acc: 0.9193 - val_loss: 0.2260 - val_acc: 0.9162
Epoch 202/500
Epoch 00201: val_loss did not improve
3s - loss: 0.2090 - acc: 0.9200 - val_loss: 0.2210 - val_acc: 0.9139
Epoch 203/500
Epoch 00202: val_loss improved from 0.22048 to 0.21898, saving model to ./bestmodel2.hdf5
5s - loss: 0.2124 - acc: 0.9187 - val_loss: 0.2190 - val_acc: 0.9144
Epoch 204/500
Epoch 00203: val_loss did not improve
3s - loss: 0.2092 - acc: 0.9201 - val_loss: 0.2211 - val_acc: 0.9170
Epoch 205/500
Epoch 00204: val_loss did not improve
3s - loss: 0.2067 - acc: 0.9198 - val_loss: 0.2233 - val_acc: 0.9172
Epoch 206/500
Epoch 00205: val_loss improved from 0.21898 to 0.21838, saving model to ./bestmodel2.hdf5
4s - loss: 0.2077 - acc: 0.9194 - val_loss: 0.2184 - val_acc: 0.9182
Epoch 207/500
Epoch 00206: val_loss did not improve
3s - loss: 0.2116 - acc: 0.9193 - val_loss: 0.2245 - val_acc: 0.9111
Epoch 208/500
Epoch 00207: val_loss did not improve
3s - loss: 0.2083 - acc: 0.9205 - val_loss: 0.2246 - val_acc: 0.9170
Epoch 209/500
Epoch 00208: val_loss did not improve
3s - loss: 0.2074 - acc: 0.9215 - val_loss: 0.2240 - val_acc: 0.9185
Epoch 210/500
Epoch 00209: val_loss did not improve
3s - loss: 0.2063 - acc: 0.9217 - val_loss: 0.2250 - val_acc: 0.9154
Epoch 211/500
Epoch 00210: val_loss did not improve
3s - loss: 0.2080 - acc: 0.9210 - val_loss: 0.2223 - val_acc: 0.9152
Epoch 212/500
Epoch 00211: val_loss did not improve
3s - loss: 0.2067 - acc: 0.9198 - val_loss: 0.2212 - val_acc: 0.9139
Epoch 213/500
Epoch 00212: val_loss did not improve
3s - loss: 0.2065 - acc: 0.9210 - val_loss: 0.2205 - val_acc: 0.9162
Epoch 214/500
Epoch 00213: val_loss did not improve
3s - loss: 0.2073 - acc: 0.9218 - val_loss: 0.2223 - val_acc: 0.9142
Epoch 215/500
Epoch 00214: val_loss did not improve
3s - loss: 0.2057 - acc: 0.9198 - val_loss: 0.2367 - val_acc: 0.9106
Epoch 216/500
Epoch 00215: val_loss did not improve
3s - loss: 0.2048 - acc: 0.9225 - val_loss: 0.2242 - val_acc: 0.9147
Epoch 217/500
Epoch 00216: val_loss did not improve
3s - loss: 0.2058 - acc: 0.9209 - val_loss: 0.2252 - val_acc: 0.9119
Epoch 218/500
Epoch 00217: val_loss did not improve
3s - loss: 0.2022 - acc: 0.9203 - val_loss: 0.2285 - val_acc: 0.9154
Epoch 219/500
Epoch 00218: val_loss did not improve
3s - loss: 0.2038 - acc: 0.9241 - val_loss: 0.2236 - val_acc: 0.9144
Epoch 220/500
Epoch 00219: val_loss did not improve
3s - loss: 0.2030 - acc: 0.9226 - val_loss: 0.2222 - val_acc: 0.9142
Epoch 221/500
Epoch 00220: val_loss did not improve
3s - loss: 0.2031 - acc: 0.9214 - val_loss: 0.2193 - val_acc: 0.9170
Epoch 222/500
Epoch 00221: val_loss did not improve
3s - loss: 0.2048 - acc: 0.9220 - val_loss: 0.2253 - val_acc: 0.9167
Epoch 223/500
Epoch 00222: val_loss did not improve
3s - loss: 0.2041 - acc: 0.9218 - val_loss: 0.2214 - val_acc: 0.9152
Epoch 224/500
Epoch 00223: val_loss did not improve
3s - loss: 0.2017 - acc: 0.9226 - val_loss: 0.2221 - val_acc: 0.9159
Epoch 225/500
Epoch 00224: val_loss did not improve
3s - loss: 0.2022 - acc: 0.9233 - val_loss: 0.2190 - val_acc: 0.9170
Epoch 226/500
Epoch 00225: val_loss did not improve
3s - loss: 0.2003 - acc: 0.9237 - val_loss: 0.2219 - val_acc: 0.9152
Epoch 227/500
Epoch 00226: val_loss did not improve
3s - loss: 0.1983 - acc: 0.9240 - val_loss: 0.2236 - val_acc: 0.9154
Epoch 228/500
Epoch 00227: val_loss did not improve
3s - loss: 0.2017 - acc: 0.9215 - val_loss: 0.2220 - val_acc: 0.9167
Epoch 229/500
Epoch 00228: val_loss did not improve
3s - loss: 0.2009 - acc: 0.9229 - val_loss: 0.2228 - val_acc: 0.9172
Epoch 230/500
Epoch 00229: val_loss did not improve
3s - loss: 0.2006 - acc: 0.9237 - val_loss: 0.2280 - val_acc: 0.9167
Epoch 231/500
Epoch 00230: val_loss did not improve
3s - loss: 0.1977 - acc: 0.9245 - val_loss: 0.2227 - val_acc: 0.9149
Epoch 232/500
Epoch 00231: val_loss did not improve
3s - loss: 0.1968 - acc: 0.9248 - val_loss: 0.2251 - val_acc: 0.9162
Epoch 233/500
Epoch 00232: val_loss did not improve
3s - loss: 0.2003 - acc: 0.9226 - val_loss: 0.2222 - val_acc: 0.9177
Epoch 234/500
Epoch 00233: val_loss did not improve
3s - loss: 0.1961 - acc: 0.9252 - val_loss: 0.2225 - val_acc: 0.9167
Epoch 235/500
Epoch 00234: val_loss did not improve
3s - loss: 0.1997 - acc: 0.9237 - val_loss: 0.2233 - val_acc: 0.9167
Epoch 236/500
Epoch 00235: val_loss did not improve
3s - loss: 0.1988 - acc: 0.9245 - val_loss: 0.2202 - val_acc: 0.9167
Epoch 237/500
Epoch 00236: val_loss did not improve
3s - loss: 0.1977 - acc: 0.9236 - val_loss: 0.2217 - val_acc: 0.9159
Epoch 238/500
Epoch 00237: val_loss did not improve
3s - loss: 0.1952 - acc: 0.9260 - val_loss: 0.2196 - val_acc: 0.9152
Epoch 239/500
Epoch 00238: val_loss did not improve
3s - loss: 0.1948 - acc: 0.9255 - val_loss: 0.2227 - val_acc: 0.9180
Epoch 240/500
Epoch 00239: val_loss did not improve
3s - loss: 0.1931 - acc: 0.9266 - val_loss: 0.2207 - val_acc: 0.9193
Epoch 241/500
Epoch 00240: val_loss did not improve
3s - loss: 0.1961 - acc: 0.9248 - val_loss: 0.2303 - val_acc: 0.9159
Epoch 242/500
Epoch 00241: val_loss improved from 0.21838 to 0.21815, saving model to ./bestmodel2.hdf5
4s - loss: 0.1985 - acc: 0.9244 - val_loss: 0.2181 - val_acc: 0.9165
Epoch 243/500
Epoch 00242: val_loss did not improve
3s - loss: 0.1932 - acc: 0.9275 - val_loss: 0.2251 - val_acc: 0.9172
Epoch 244/500
Epoch 00243: val_loss did not improve
3s - loss: 0.1953 - acc: 0.9244 - val_loss: 0.2191 - val_acc: 0.9170
Epoch 245/500
Epoch 00244: val_loss did not improve
3s - loss: 0.1940 - acc: 0.9248 - val_loss: 0.2239 - val_acc: 0.9149
Epoch 246/500
Epoch 00245: val_loss did not improve
3s - loss: 0.1943 - acc: 0.9270 - val_loss: 0.2188 - val_acc: 0.9175
Epoch 247/500
Epoch 00246: val_loss did not improve
3s - loss: 0.1902 - acc: 0.9270 - val_loss: 0.2327 - val_acc: 0.9170
Epoch 248/500
Epoch 00247: val_loss did not improve
3s - loss: 0.1937 - acc: 0.9267 - val_loss: 0.2224 - val_acc: 0.9154
Epoch 249/500
Epoch 00248: val_loss did not improve
3s - loss: 0.1924 - acc: 0.9272 - val_loss: 0.2252 - val_acc: 0.9175
Epoch 250/500
Epoch 00249: val_loss did not improve
3s - loss: 0.1933 - acc: 0.9249 - val_loss: 0.2194 - val_acc: 0.9159
Epoch 251/500
Epoch 00250: val_loss did not improve
3s - loss: 0.1915 - acc: 0.9265 - val_loss: 0.2232 - val_acc: 0.9172
Epoch 252/500
Epoch 00251: val_loss did not improve
3s - loss: 0.1907 - acc: 0.9271 - val_loss: 0.2240 - val_acc: 0.9167
Epoch 253/500
Epoch 00252: val_loss did not improve
3s - loss: 0.1915 - acc: 0.9265 - val_loss: 0.2232 - val_acc: 0.9159
Epoch 254/500
Epoch 00253: val_loss did not improve
3s - loss: 0.1910 - acc: 0.9271 - val_loss: 0.2245 - val_acc: 0.9172
Epoch 255/500
Epoch 00254: val_loss did not improve
3s - loss: 0.1910 - acc: 0.9263 - val_loss: 0.2211 - val_acc: 0.9165
Epoch 256/500
Epoch 00255: val_loss did not improve
3s - loss: 0.1909 - acc: 0.9275 - val_loss: 0.2203 - val_acc: 0.9170
Epoch 257/500
Epoch 00256: val_loss did not improve
3s - loss: 0.1889 - acc: 0.9275 - val_loss: 0.2264 - val_acc: 0.9144
Epoch 258/500
Epoch 00257: val_loss did not improve
3s - loss: 0.1888 - acc: 0.9276 - val_loss: 0.2261 - val_acc: 0.9180
Epoch 259/500
Epoch 00258: val_loss did not improve
3s - loss: 0.1888 - acc: 0.9272 - val_loss: 0.2259 - val_acc: 0.9154
Epoch 260/500
Epoch 00259: val_loss did not improve
3s - loss: 0.1867 - acc: 0.9278 - val_loss: 0.2240 - val_acc: 0.9182
Epoch 261/500
Epoch 00260: val_loss did not improve
3s - loss: 0.1887 - acc: 0.9271 - val_loss: 0.2267 - val_acc: 0.9154
Epoch 262/500
Epoch 00261: val_loss did not improve
3s - loss: 0.1853 - acc: 0.9290 - val_loss: 0.2270 - val_acc: 0.9152
Epoch 263/500
Epoch 00262: val_loss did not improve
3s - loss: 0.1850 - acc: 0.9284 - val_loss: 0.2248 - val_acc: 0.9187
Epoch 264/500
Epoch 00263: val_loss did not improve
3s - loss: 0.1846 - acc: 0.9290 - val_loss: 0.2307 - val_acc: 0.9170
Epoch 265/500
Epoch 00264: val_loss did not improve
3s - loss: 0.1841 - acc: 0.9299 - val_loss: 0.2235 - val_acc: 0.9177
Epoch 266/500
Epoch 00265: val_loss did not improve
3s - loss: 0.1859 - acc: 0.9291 - val_loss: 0.2258 - val_acc: 0.9167
Epoch 267/500
Epoch 00266: val_loss did not improve
3s - loss: 0.1851 - acc: 0.9285 - val_loss: 0.2215 - val_acc: 0.9159
Epoch 268/500
Epoch 00267: val_loss did not improve
3s - loss: 0.1834 - acc: 0.9304 - val_loss: 0.2256 - val_acc: 0.9157
Epoch 269/500
Epoch 00268: val_loss did not improve
3s - loss: 0.1818 - acc: 0.9291 - val_loss: 0.2251 - val_acc: 0.9187
Epoch 270/500
Epoch 00269: val_loss did not improve
3s - loss: 0.1840 - acc: 0.9297 - val_loss: 0.2247 - val_acc: 0.9170
Epoch 271/500
Epoch 00270: val_loss did not improve
3s - loss: 0.1825 - acc: 0.9295 - val_loss: 0.2220 - val_acc: 0.9165
Epoch 272/500
Epoch 00271: val_loss did not improve
3s - loss: 0.1849 - acc: 0.9298 - val_loss: 0.2267 - val_acc: 0.9165
Epoch 273/500
Epoch 00272: val_loss did not improve
3s - loss: 0.1828 - acc: 0.9291 - val_loss: 0.2235 - val_acc: 0.9198
Epoch 274/500
Epoch 00273: val_loss did not improve
3s - loss: 0.1838 - acc: 0.9300 - val_loss: 0.2243 - val_acc: 0.9185
Epoch 275/500
Epoch 00274: val_loss did not improve
3s - loss: 0.1827 - acc: 0.9304 - val_loss: 0.2241 - val_acc: 0.9193
Epoch 276/500
Epoch 00275: val_loss did not improve
3s - loss: 0.1840 - acc: 0.9299 - val_loss: 0.2232 - val_acc: 0.9203
Epoch 277/500
Epoch 00276: val_loss did not improve
3s - loss: 0.1812 - acc: 0.9314 - val_loss: 0.2294 - val_acc: 0.9157
Epoch 278/500
Epoch 00277: val_loss did not improve
3s - loss: 0.1794 - acc: 0.9312 - val_loss: 0.2288 - val_acc: 0.9157
Epoch 279/500
Epoch 00278: val_loss did not improve
3s - loss: 0.1825 - acc: 0.9304 - val_loss: 0.2251 - val_acc: 0.9165
Epoch 280/500
Epoch 00279: val_loss did not improve
3s - loss: 0.1802 - acc: 0.9322 - val_loss: 0.2219 - val_acc: 0.9165
Epoch 281/500
Epoch 00280: val_loss did not improve
3s - loss: 0.1817 - acc: 0.9302 - val_loss: 0.2260 - val_acc: 0.9157
Epoch 282/500
Epoch 00281: val_loss did not improve
3s - loss: 0.1811 - acc: 0.9301 - val_loss: 0.2253 - val_acc: 0.9180
Epoch 283/500
Epoch 00282: val_loss did not improve
3s - loss: 0.1805 - acc: 0.9323 - val_loss: 0.2273 - val_acc: 0.9165
Epoch 284/500
Epoch 00283: val_loss did not improve
3s - loss: 0.1789 - acc: 0.9313 - val_loss: 0.2266 - val_acc: 0.9175
Epoch 285/500
Epoch 00284: val_loss did not improve
3s - loss: 0.1781 - acc: 0.9326 - val_loss: 0.2234 - val_acc: 0.9195
Epoch 286/500
Epoch 00285: val_loss did not improve
3s - loss: 0.1837 - acc: 0.9297 - val_loss: 0.2251 - val_acc: 0.9147
Epoch 287/500
Epoch 00286: val_loss did not improve
3s - loss: 0.1798 - acc: 0.9311 - val_loss: 0.2328 - val_acc: 0.9170
Epoch 288/500
Epoch 00287: val_loss did not improve
3s - loss: 0.1813 - acc: 0.9299 - val_loss: 0.2276 - val_acc: 0.9139
Epoch 289/500
Epoch 00288: val_loss did not improve
3s - loss: 0.1767 - acc: 0.9334 - val_loss: 0.2289 - val_acc: 0.9175
Epoch 290/500
Epoch 00289: val_loss did not improve
3s - loss: 0.1763 - acc: 0.9311 - val_loss: 0.2224 - val_acc: 0.9167
Epoch 291/500
Epoch 00290: val_loss did not improve
3s - loss: 0.1789 - acc: 0.9302 - val_loss: 0.2219 - val_acc: 0.9172
Epoch 292/500
Epoch 00291: val_loss did not improve
3s - loss: 0.1761 - acc: 0.9335 - val_loss: 0.2253 - val_acc: 0.9182
Epoch 293/500
Epoch 00292: val_loss did not improve
Epoch 00292: early stopping
3s - loss: 0.1773 - acc: 0.9300 - val_loss: 0.2268 - val_acc: 0.9165
training done!
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
cov1 (Convolution1D)             (None, 58, 80)        1360        convolution1d_input_2[0][0]      
____________________________________________________________________________________________________
leakyrelu_1 (LeakyReLU)          (None, 58, 80)        0           cov1[0][0]                       
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 58, 80)        0           leakyrelu_1[0][0]                
____________________________________________________________________________________________________
cov2 (Convolution1D)             (None, 57, 80)        12880       dropout_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_2 (LeakyReLU)          (None, 57, 80)        0           cov2[0][0]                       
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 57, 80)        0           leakyrelu_2[0][0]                
____________________________________________________________________________________________________
cov3 (Convolution1D)             (None, 54, 80)        25680       dropout_2[0][0]                  
____________________________________________________________________________________________________
leakyrelu_3 (LeakyReLU)          (None, 54, 80)        0           cov3[0][0]                       
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 54, 80)        0           leakyrelu_3[0][0]                
____________________________________________________________________________________________________
cov4 (Convolution1D)             (None, 51, 80)        25680       dropout_3[0][0]                  
____________________________________________________________________________________________________
leakyrelu_4 (LeakyReLU)          (None, 51, 80)        0           cov4[0][0]                       
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 51, 80)        0           leakyrelu_4[0][0]                
____________________________________________________________________________________________________
cov5 (Convolution1D)             (None, 48, 80)        25680       dropout_4[0][0]                  
____________________________________________________________________________________________________
leakyrelu_5 (LeakyReLU)          (None, 48, 80)        0           cov5[0][0]                       
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 48, 80)        0           leakyrelu_5[0][0]                
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 3840)          0           dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           384100      flatten_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_6 (LeakyReLU)          (None, 100)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 100)           0           leakyrelu_6[0][0]                
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             101         dropout_6[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    
====================================================================================================
Total params: 475481
____________________________________________________________________________________________________
**************vadiation results on validation dataset************
  32/3926 [..............................] - ETA: 0s  64/3926 [..............................] - ETA: 12s 160/3926 [>.............................] - ETA: 5s  256/3926 [>.............................] - ETA: 3s 352/3926 [=>............................] - ETA: 2s 448/3926 [==>...........................] - ETA: 1s 544/3926 [===>..........................] - ETA: 1s 640/3926 [===>..........................] - ETA: 1s 736/3926 [====>.........................] - ETA: 1s 832/3926 [=====>........................] - ETA: 1s 928/3926 [======>.......................] - ETA: 0s1024/3926 [======>.......................] - ETA: 0s1120/3926 [=======>......................] - ETA: 0s1216/3926 [========>.....................] - ETA: 0s1312/3926 [=========>....................] - ETA: 0s1440/3926 [==========>...................] - ETA: 0s1568/3926 [==========>...................] - ETA: 0s1696/3926 [===========>..................] - ETA: 0s1824/3926 [============>.................] - ETA: 0s1952/3926 [=============>................] - ETA: 0s2080/3926 [==============>...............] - ETA: 0s2208/3926 [===============>..............] - ETA: 0s2336/3926 [================>.............] - ETA: 0s2464/3926 [=================>............] - ETA: 0s2592/3926 [==================>...........] - ETA: 0s2720/3926 [===================>..........] - ETA: 0s2848/3926 [====================>.........] - ETA: 0s2976/3926 [=====================>........] - ETA: 0s3104/3926 [======================>.......] - ETA: 0s3232/3926 [=======================>......] - ETA: 0s3360/3926 [========================>.....] - ETA: 0s3488/3926 [=========================>....] - ETA: 0s3616/3926 [==========================>...] - ETA: 0s3744/3926 [===========================>..] - ETA: 0s3872/3926 [============================>.] - ETA: 0s[0.21814509217403885, 0.91645440642953946]
  32/3926 [..............................] - ETA: 0s 160/3926 [>.............................] - ETA: 0s 288/3926 [=>............................] - ETA: 0s 416/3926 [==>...........................] - ETA: 0s 544/3926 [===>..........................] - ETA: 0s 672/3926 [====>.........................] - ETA: 0s 800/3926 [=====>........................] - ETA: 0s 928/3926 [======>.......................] - ETA: 0s1056/3926 [=======>......................] - ETA: 0s1184/3926 [========>.....................] - ETA: 0s1312/3926 [=========>....................] - ETA: 0s1440/3926 [==========>...................] - ETA: 0s1568/3926 [==========>...................] - ETA: 0s1696/3926 [===========>..................] - ETA: 0s1824/3926 [============>.................] - ETA: 0s1952/3926 [=============>................] - ETA: 0s2080/3926 [==============>...............] - ETA: 0s2208/3926 [===============>..............] - ETA: 0s2336/3926 [================>.............] - ETA: 0s2464/3926 [=================>............] - ETA: 0s2592/3926 [==================>...........] - ETA: 0s2752/3926 [====================>.........] - ETA: 0s2912/3926 [=====================>........] - ETA: 0s3072/3926 [======================>.......] - ETA: 0s3232/3926 [=======================>......] - ETA: 0s3360/3926 [========================>.....] - ETA: 0s3520/3926 [=========================>....] - ETA: 0s3680/3926 [===========================>..] - ETA: 0s3808/3926 [============================>.] - ETA: 0s  32/3926 [..............................] - ETA: 0s 192/3926 [>.............................] - ETA: 0s 352/3926 [=>............................] - ETA: 0s 512/3926 [==>...........................] - ETA: 0s 672/3926 [====>.........................] - ETA: 0s 832/3926 [=====>........................] - ETA: 0s 992/3926 [======>.......................] - ETA: 0s1152/3926 [=======>......................] - ETA: 0s1312/3926 [=========>....................] - ETA: 0s1472/3926 [==========>...................] - ETA: 0s1632/3926 [===========>..................] - ETA: 0s1792/3926 [============>.................] - ETA: 0s1952/3926 [=============>................] - ETA: 0s2112/3926 [===============>..............] - ETA: 0s2272/3926 [================>.............] - ETA: 0s2432/3926 [=================>............] - ETA: 0s2592/3926 [==================>...........] - ETA: 0s2752/3926 [====================>.........] - ETA: 0s2912/3926 [=====================>........] - ETA: 0s3072/3926 [======================>.......] - ETA: 0s3232/3926 [=======================>......] - ETA: 0s3392/3926 [========================>.....] - ETA: 0s3552/3926 [==========================>...] - ETA: 0s3712/3926 [===========================>..] - ETA: 0s3872/3926 [============================>.] - ETA: 0s************************
auc: 0.970498802476
mcc: 0.833324565972
negative ---> precision:0.903703703704, recall:0.932246561386, f1score:0.917753259779, support:1963
positive ---> precision:0.930036822725, recall:0.900662251656, f1score:0.915113871636, support:1963
************************
**************prediction results on test dataset************
  32/3927 [..............................] - ETA: 0s 192/3927 [>.............................] - ETA: 0s 352/3927 [=>............................] - ETA: 0s 512/3927 [==>...........................] - ETA: 0s 672/3927 [====>.........................] - ETA: 0s 832/3927 [=====>........................] - ETA: 0s 992/3927 [======>.......................] - ETA: 0s1120/3927 [=======>......................] - ETA: 0s1248/3927 [========>.....................] - ETA: 0s1408/3927 [=========>....................] - ETA: 0s1536/3927 [==========>...................] - ETA: 0s1696/3927 [===========>..................] - ETA: 0s1856/3927 [=============>................] - ETA: 0s2016/3927 [==============>...............] - ETA: 0s2176/3927 [===============>..............] - ETA: 0s2336/3927 [================>.............] - ETA: 0s2496/3927 [==================>...........] - ETA: 0s2656/3927 [===================>..........] - ETA: 0s2816/3927 [====================>.........] - ETA: 0s2976/3927 [=====================>........] - ETA: 0s3136/3927 [======================>.......] - ETA: 0s3264/3927 [=======================>......] - ETA: 0s3424/3927 [=========================>....] - ETA: 0s3584/3927 [==========================>...] - ETA: 0s3744/3927 [===========================>..] - ETA: 0s3904/3927 [============================>.] - ETA: 0s[0.22665047633426316, 0.91723962324340136]
  32/3927 [..............................] - ETA: 0s 192/3927 [>.............................] - ETA: 0s 352/3927 [=>............................] - ETA: 0s 512/3927 [==>...........................] - ETA: 0s 672/3927 [====>.........................] - ETA: 0s 832/3927 [=====>........................] - ETA: 0s 992/3927 [======>.......................] - ETA: 0s1152/3927 [=======>......................] - ETA: 0s1312/3927 [=========>....................] - ETA: 0s1472/3927 [==========>...................] - ETA: 0s1632/3927 [===========>..................] - ETA: 0s1792/3927 [============>.................] - ETA: 0s1952/3927 [=============>................] - ETA: 0s2112/3927 [===============>..............] - ETA: 0s2272/3927 [================>.............] - ETA: 0s2432/3927 [=================>............] - ETA: 0s2592/3927 [==================>...........] - ETA: 0s2752/3927 [====================>.........] - ETA: 0s2912/3927 [=====================>........] - ETA: 0s3072/3927 [======================>.......] - ETA: 0s3232/3927 [=======================>......] - ETA: 0s3392/3927 [========================>.....] - ETA: 0s3552/3927 [==========================>...] - ETA: 0s3712/3927 [===========================>..] - ETA: 0s3872/3927 [============================>.] - ETA: 0s  32/3927 [..............................] - ETA: 0s 192/3927 [>.............................] - ETA: 0s 352/3927 [=>............................] - ETA: 0s 512/3927 [==>...........................] - ETA: 0s 672/3927 [====>.........................] - ETA: 0s 832/3927 [=====>........................] - ETA: 0s 992/3927 [======>.......................] - ETA: 0s1152/3927 [=======>......................] - ETA: 0s1312/3927 [=========>....................] - ETA: 0s1472/3927 [==========>...................] - ETA: 0s1632/3927 [===========>..................] - ETA: 0s1792/3927 [============>.................] - ETA: 0s1952/3927 [=============>................] - ETA: 0s2112/3927 [===============>..............] - ETA: 0s2272/3927 [================>.............] - ETA: 0s2432/3927 [=================>............] - ETA: 0s2592/3927 [==================>...........] - ETA: 0s2752/3927 [====================>.........] - ETA: 0s2912/3927 [=====================>........] - ETA: 0s3072/3927 [======================>.......] - ETA: 0s3232/3927 [=======================>......] - ETA: 0s3392/3927 [========================>.....] - ETA: 0s3552/3927 [==========================>...] - ETA: 0s3712/3927 [===========================>..] - ETA: 0s3872/3927 [============================>.] - ETA: 0s************************
auc: 0.967759715635
mcc: 0.835087063441
negative ---> precision:0.901912702305, recall:0.936354378819, f1score:0.918810891831, support:1964
positive ---> precision:0.933792372881, recall:0.898115129903, f1score:0.915606336017, support:1963
************************
