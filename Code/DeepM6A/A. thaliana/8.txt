Using Theano backend.
INFO (theano.gof.compilelock): Waiting for existing lock by unknown process (I am process '192674')
INFO (theano.gof.compilelock): To manually release the lock, delete /home/xsede/users/xs-tanfei/.theano/compiledir_Linux-2.6-el6.x86_64-x86_64-with-redhat-6.9-Santiago-x86_64-2.7.9-64/lock_dir
Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 10.0% of memory, cuDNN 5005)
loading data
8
(31414, 61, 4)
('train_label: count', (array([0, 1]), array([15707, 15707])))
('valid_label: count', (array([0, 1]), array([1961, 1961])))
('test_label: count', (array([0, 1]), array([1961, 1961])))
building model...............
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
cov1 (Convolution1D)             (None, 58, 80)        1360        convolution1d_input_1[0][0]      
____________________________________________________________________________________________________
leakyrelu_1 (LeakyReLU)          (None, 58, 80)        0           cov1[0][0]                       
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 58, 80)        0           leakyrelu_1[0][0]                
____________________________________________________________________________________________________
cov2 (Convolution1D)             (None, 57, 80)        12880       dropout_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_2 (LeakyReLU)          (None, 57, 80)        0           cov2[0][0]                       
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 57, 80)        0           leakyrelu_2[0][0]                
____________________________________________________________________________________________________
cov3 (Convolution1D)             (None, 54, 80)        25680       dropout_2[0][0]                  
____________________________________________________________________________________________________
leakyrelu_3 (LeakyReLU)          (None, 54, 80)        0           cov3[0][0]                       
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 54, 80)        0           leakyrelu_3[0][0]                
____________________________________________________________________________________________________
cov4 (Convolution1D)             (None, 51, 80)        25680       dropout_3[0][0]                  
____________________________________________________________________________________________________
leakyrelu_4 (LeakyReLU)          (None, 51, 80)        0           cov4[0][0]                       
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 51, 80)        0           leakyrelu_4[0][0]                
____________________________________________________________________________________________________
cov5 (Convolution1D)             (None, 48, 80)        25680       dropout_4[0][0]                  
____________________________________________________________________________________________________
leakyrelu_5 (LeakyReLU)          (None, 48, 80)        0           cov5[0][0]                       
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 48, 80)        0           leakyrelu_5[0][0]                
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 3840)          0           dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           384100      flatten_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_6 (LeakyReLU)          (None, 100)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 100)           0           leakyrelu_6[0][0]                
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             101         dropout_6[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    
====================================================================================================
Total params: 475481
____________________________________________________________________________________________________
compiling and fitting model...........
Train on 31414 samples, validate on 3922 samples
Epoch 1/500
Epoch 00000: val_loss improved from inf to 0.69314, saving model to ./bestmodel8.hdf5
3s - loss: 0.6932 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 2/500
Epoch 00001: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 3/500
Epoch 00002: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 4/500
Epoch 00003: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 5/500
Epoch 00004: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 6/500
Epoch 00005: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4959 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 7/500
Epoch 00006: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 8/500
Epoch 00007: val_loss did not improve
3s - loss: 0.6933 - acc: 0.4939 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 9/500
Epoch 00008: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 10/500
Epoch 00009: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 11/500
Epoch 00010: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 12/500
Epoch 00011: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5020 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 13/500
Epoch 00012: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4957 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 14/500
Epoch 00013: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 15/500
Epoch 00014: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 16/500
Epoch 00015: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 17/500
Epoch 00016: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 18/500
Epoch 00017: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 19/500
Epoch 00018: val_loss did not improve
3s - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 20/500
Epoch 00019: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 21/500
Epoch 00020: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 22/500
Epoch 00021: val_loss improved from 0.69314 to 0.69313, saving model to ./bestmodel8.hdf5
3s - loss: 0.6931 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 23/500
Epoch 00022: val_loss improved from 0.69313 to 0.69312, saving model to ./bestmodel8.hdf5
4s - loss: 0.6932 - acc: 0.4997 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 24/500
Epoch 00023: val_loss improved from 0.69312 to 0.69311, saving model to ./bestmodel8.hdf5
4s - loss: 0.6932 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 25/500
Epoch 00024: val_loss improved from 0.69311 to 0.69310, saving model to ./bestmodel8.hdf5
4s - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 26/500
Epoch 00025: val_loss improved from 0.69310 to 0.69308, saving model to ./bestmodel8.hdf5
4s - loss: 0.6931 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 27/500
Epoch 00026: val_loss did not improve
3s - loss: 0.6932 - acc: 0.5029 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 28/500
Epoch 00027: val_loss improved from 0.69308 to 0.69305, saving model to ./bestmodel8.hdf5
4s - loss: 0.6931 - acc: 0.5033 - val_loss: 0.6930 - val_acc: 0.5000
Epoch 29/500
Epoch 00028: val_loss did not improve
3s - loss: 0.6931 - acc: 0.5037 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 30/500
Epoch 00029: val_loss improved from 0.69305 to 0.69289, saving model to ./bestmodel8.hdf5
4s - loss: 0.6931 - acc: 0.5084 - val_loss: 0.6929 - val_acc: 0.5000
Epoch 31/500
Epoch 00030: val_loss improved from 0.69289 to 0.69266, saving model to ./bestmodel8.hdf5
4s - loss: 0.6928 - acc: 0.5115 - val_loss: 0.6927 - val_acc: 0.5594
Epoch 32/500
Epoch 00031: val_loss improved from 0.69266 to 0.69206, saving model to ./bestmodel8.hdf5
4s - loss: 0.6926 - acc: 0.5178 - val_loss: 0.6921 - val_acc: 0.5003
Epoch 33/500
Epoch 00032: val_loss improved from 0.69206 to 0.68749, saving model to ./bestmodel8.hdf5
4s - loss: 0.6910 - acc: 0.5480 - val_loss: 0.6875 - val_acc: 0.6994
Epoch 34/500
Epoch 00033: val_loss improved from 0.68749 to 0.56258, saving model to ./bestmodel8.hdf5
4s - loss: 0.6494 - acc: 0.6713 - val_loss: 0.5626 - val_acc: 0.7183
Epoch 35/500
Epoch 00034: val_loss improved from 0.56258 to 0.50113, saving model to ./bestmodel8.hdf5
4s - loss: 0.5381 - acc: 0.7384 - val_loss: 0.5011 - val_acc: 0.7647
Epoch 36/500
Epoch 00035: val_loss improved from 0.50113 to 0.46865, saving model to ./bestmodel8.hdf5
3s - loss: 0.5027 - acc: 0.7663 - val_loss: 0.4687 - val_acc: 0.7899
Epoch 37/500
Epoch 00036: val_loss improved from 0.46865 to 0.44745, saving model to ./bestmodel8.hdf5
4s - loss: 0.4810 - acc: 0.7814 - val_loss: 0.4475 - val_acc: 0.7958
Epoch 38/500
Epoch 00037: val_loss improved from 0.44745 to 0.43023, saving model to ./bestmodel8.hdf5
5s - loss: 0.4658 - acc: 0.7902 - val_loss: 0.4302 - val_acc: 0.8065
Epoch 39/500
Epoch 00038: val_loss improved from 0.43023 to 0.40849, saving model to ./bestmodel8.hdf5
3s - loss: 0.4508 - acc: 0.8005 - val_loss: 0.4085 - val_acc: 0.8197
Epoch 40/500
Epoch 00039: val_loss improved from 0.40849 to 0.39086, saving model to ./bestmodel8.hdf5
4s - loss: 0.4351 - acc: 0.8093 - val_loss: 0.3909 - val_acc: 0.8304
Epoch 41/500
Epoch 00040: val_loss improved from 0.39086 to 0.38713, saving model to ./bestmodel8.hdf5
4s - loss: 0.4276 - acc: 0.8122 - val_loss: 0.3871 - val_acc: 0.8299
Epoch 42/500
Epoch 00041: val_loss improved from 0.38713 to 0.38162, saving model to ./bestmodel8.hdf5
4s - loss: 0.4192 - acc: 0.8181 - val_loss: 0.3816 - val_acc: 0.8355
Epoch 43/500
Epoch 00042: val_loss improved from 0.38162 to 0.37379, saving model to ./bestmodel8.hdf5
3s - loss: 0.4112 - acc: 0.8217 - val_loss: 0.3738 - val_acc: 0.8417
Epoch 44/500
Epoch 00043: val_loss improved from 0.37379 to 0.36469, saving model to ./bestmodel8.hdf5
3s - loss: 0.4061 - acc: 0.8264 - val_loss: 0.3647 - val_acc: 0.8460
Epoch 45/500
Epoch 00044: val_loss improved from 0.36469 to 0.36427, saving model to ./bestmodel8.hdf5
3s - loss: 0.4026 - acc: 0.8281 - val_loss: 0.3643 - val_acc: 0.8452
Epoch 46/500
Epoch 00045: val_loss improved from 0.36427 to 0.36165, saving model to ./bestmodel8.hdf5
3s - loss: 0.3979 - acc: 0.8304 - val_loss: 0.3616 - val_acc: 0.8450
Epoch 47/500
Epoch 00046: val_loss did not improve
3s - loss: 0.3910 - acc: 0.8329 - val_loss: 0.3666 - val_acc: 0.8396
Epoch 48/500
Epoch 00047: val_loss improved from 0.36165 to 0.35681, saving model to ./bestmodel8.hdf5
3s - loss: 0.3868 - acc: 0.8341 - val_loss: 0.3568 - val_acc: 0.8463
Epoch 49/500
Epoch 00048: val_loss improved from 0.35681 to 0.34586, saving model to ./bestmodel8.hdf5
4s - loss: 0.3833 - acc: 0.8353 - val_loss: 0.3459 - val_acc: 0.8544
Epoch 50/500
Epoch 00049: val_loss improved from 0.34586 to 0.34511, saving model to ./bestmodel8.hdf5
4s - loss: 0.3798 - acc: 0.8381 - val_loss: 0.3451 - val_acc: 0.8559
Epoch 51/500
Epoch 00050: val_loss improved from 0.34511 to 0.33775, saving model to ./bestmodel8.hdf5
3s - loss: 0.3768 - acc: 0.8421 - val_loss: 0.3377 - val_acc: 0.8580
Epoch 52/500
Epoch 00051: val_loss did not improve
3s - loss: 0.3675 - acc: 0.8473 - val_loss: 0.3401 - val_acc: 0.8539
Epoch 53/500
Epoch 00052: val_loss improved from 0.33775 to 0.33094, saving model to ./bestmodel8.hdf5
3s - loss: 0.3622 - acc: 0.8473 - val_loss: 0.3309 - val_acc: 0.8649
Epoch 54/500
Epoch 00053: val_loss did not improve
3s - loss: 0.3616 - acc: 0.8470 - val_loss: 0.3360 - val_acc: 0.8570
Epoch 55/500
Epoch 00054: val_loss improved from 0.33094 to 0.32244, saving model to ./bestmodel8.hdf5
3s - loss: 0.3550 - acc: 0.8515 - val_loss: 0.3224 - val_acc: 0.8692
Epoch 56/500
Epoch 00055: val_loss did not improve
3s - loss: 0.3508 - acc: 0.8543 - val_loss: 0.3303 - val_acc: 0.8628
Epoch 57/500
Epoch 00056: val_loss improved from 0.32244 to 0.32200, saving model to ./bestmodel8.hdf5
3s - loss: 0.3496 - acc: 0.8541 - val_loss: 0.3220 - val_acc: 0.8677
Epoch 58/500
Epoch 00057: val_loss improved from 0.32200 to 0.32090, saving model to ./bestmodel8.hdf5
4s - loss: 0.3470 - acc: 0.8554 - val_loss: 0.3209 - val_acc: 0.8687
Epoch 59/500
Epoch 00058: val_loss improved from 0.32090 to 0.32077, saving model to ./bestmodel8.hdf5
3s - loss: 0.3439 - acc: 0.8567 - val_loss: 0.3208 - val_acc: 0.8684
Epoch 60/500
Epoch 00059: val_loss improved from 0.32077 to 0.31780, saving model to ./bestmodel8.hdf5
4s - loss: 0.3424 - acc: 0.8559 - val_loss: 0.3178 - val_acc: 0.8702
Epoch 61/500
Epoch 00060: val_loss improved from 0.31780 to 0.31458, saving model to ./bestmodel8.hdf5
4s - loss: 0.3367 - acc: 0.8596 - val_loss: 0.3146 - val_acc: 0.8700
Epoch 62/500
Epoch 00061: val_loss did not improve
3s - loss: 0.3359 - acc: 0.8610 - val_loss: 0.3155 - val_acc: 0.8697
Epoch 63/500
Epoch 00062: val_loss improved from 0.31458 to 0.31201, saving model to ./bestmodel8.hdf5
3s - loss: 0.3321 - acc: 0.8635 - val_loss: 0.3120 - val_acc: 0.8738
Epoch 64/500
Epoch 00063: val_loss did not improve
3s - loss: 0.3301 - acc: 0.8640 - val_loss: 0.3129 - val_acc: 0.8720
Epoch 65/500
Epoch 00064: val_loss improved from 0.31201 to 0.31139, saving model to ./bestmodel8.hdf5
3s - loss: 0.3272 - acc: 0.8654 - val_loss: 0.3114 - val_acc: 0.8738
Epoch 66/500
Epoch 00065: val_loss improved from 0.31139 to 0.31009, saving model to ./bestmodel8.hdf5
3s - loss: 0.3257 - acc: 0.8650 - val_loss: 0.3101 - val_acc: 0.8730
Epoch 67/500
Epoch 00066: val_loss improved from 0.31009 to 0.30697, saving model to ./bestmodel8.hdf5
4s - loss: 0.3225 - acc: 0.8669 - val_loss: 0.3070 - val_acc: 0.8743
Epoch 68/500
Epoch 00067: val_loss did not improve
3s - loss: 0.3174 - acc: 0.8697 - val_loss: 0.3111 - val_acc: 0.8761
Epoch 69/500
Epoch 00068: val_loss did not improve
3s - loss: 0.3172 - acc: 0.8679 - val_loss: 0.3071 - val_acc: 0.8771
Epoch 70/500
Epoch 00069: val_loss improved from 0.30697 to 0.30443, saving model to ./bestmodel8.hdf5
4s - loss: 0.3136 - acc: 0.8724 - val_loss: 0.3044 - val_acc: 0.8763
Epoch 71/500
Epoch 00070: val_loss improved from 0.30443 to 0.30252, saving model to ./bestmodel8.hdf5
3s - loss: 0.3146 - acc: 0.8709 - val_loss: 0.3025 - val_acc: 0.8774
Epoch 72/500
Epoch 00071: val_loss improved from 0.30252 to 0.30110, saving model to ./bestmodel8.hdf5
4s - loss: 0.3096 - acc: 0.8750 - val_loss: 0.3011 - val_acc: 0.8779
Epoch 73/500
Epoch 00072: val_loss did not improve
3s - loss: 0.3081 - acc: 0.8726 - val_loss: 0.3085 - val_acc: 0.8763
Epoch 74/500
Epoch 00073: val_loss improved from 0.30110 to 0.29879, saving model to ./bestmodel8.hdf5
5s - loss: 0.3044 - acc: 0.8749 - val_loss: 0.2988 - val_acc: 0.8830
Epoch 75/500
Epoch 00074: val_loss improved from 0.29879 to 0.29534, saving model to ./bestmodel8.hdf5
4s - loss: 0.3010 - acc: 0.8767 - val_loss: 0.2953 - val_acc: 0.8814
Epoch 76/500
Epoch 00075: val_loss improved from 0.29534 to 0.29050, saving model to ./bestmodel8.hdf5
3s - loss: 0.2976 - acc: 0.8793 - val_loss: 0.2905 - val_acc: 0.8837
Epoch 77/500
Epoch 00076: val_loss did not improve
3s - loss: 0.2953 - acc: 0.8805 - val_loss: 0.2916 - val_acc: 0.8845
Epoch 78/500
Epoch 00077: val_loss improved from 0.29050 to 0.28769, saving model to ./bestmodel8.hdf5
4s - loss: 0.2954 - acc: 0.8808 - val_loss: 0.2877 - val_acc: 0.8858
Epoch 79/500
Epoch 00078: val_loss did not improve
3s - loss: 0.2915 - acc: 0.8826 - val_loss: 0.2981 - val_acc: 0.8822
Epoch 80/500
Epoch 00079: val_loss improved from 0.28769 to 0.28731, saving model to ./bestmodel8.hdf5
3s - loss: 0.2914 - acc: 0.8835 - val_loss: 0.2873 - val_acc: 0.8850
Epoch 81/500
Epoch 00080: val_loss improved from 0.28731 to 0.28321, saving model to ./bestmodel8.hdf5
3s - loss: 0.2881 - acc: 0.8834 - val_loss: 0.2832 - val_acc: 0.8858
Epoch 82/500
Epoch 00081: val_loss did not improve
3s - loss: 0.2855 - acc: 0.8860 - val_loss: 0.2833 - val_acc: 0.8876
Epoch 83/500
Epoch 00082: val_loss improved from 0.28321 to 0.28148, saving model to ./bestmodel8.hdf5
3s - loss: 0.2839 - acc: 0.8853 - val_loss: 0.2815 - val_acc: 0.8883
Epoch 84/500
Epoch 00083: val_loss did not improve
3s - loss: 0.2818 - acc: 0.8869 - val_loss: 0.2872 - val_acc: 0.8860
Epoch 85/500
Epoch 00084: val_loss improved from 0.28148 to 0.27864, saving model to ./bestmodel8.hdf5
3s - loss: 0.2800 - acc: 0.8878 - val_loss: 0.2786 - val_acc: 0.8868
Epoch 86/500
Epoch 00085: val_loss did not improve
3s - loss: 0.2763 - acc: 0.8895 - val_loss: 0.2790 - val_acc: 0.8891
Epoch 87/500
Epoch 00086: val_loss improved from 0.27864 to 0.27772, saving model to ./bestmodel8.hdf5
3s - loss: 0.2790 - acc: 0.8870 - val_loss: 0.2777 - val_acc: 0.8876
Epoch 88/500
Epoch 00087: val_loss did not improve
3s - loss: 0.2756 - acc: 0.8900 - val_loss: 0.2806 - val_acc: 0.8883
Epoch 89/500
Epoch 00088: val_loss improved from 0.27772 to 0.27699, saving model to ./bestmodel8.hdf5
3s - loss: 0.2719 - acc: 0.8912 - val_loss: 0.2770 - val_acc: 0.8906
Epoch 90/500
Epoch 00089: val_loss did not improve
3s - loss: 0.2734 - acc: 0.8910 - val_loss: 0.2799 - val_acc: 0.8878
Epoch 91/500
Epoch 00090: val_loss did not improve
3s - loss: 0.2724 - acc: 0.8910 - val_loss: 0.2803 - val_acc: 0.8878
Epoch 92/500
Epoch 00091: val_loss improved from 0.27699 to 0.27380, saving model to ./bestmodel8.hdf5
4s - loss: 0.2704 - acc: 0.8923 - val_loss: 0.2738 - val_acc: 0.8904
Epoch 93/500
Epoch 00092: val_loss did not improve
3s - loss: 0.2687 - acc: 0.8937 - val_loss: 0.2782 - val_acc: 0.8906
Epoch 94/500
Epoch 00093: val_loss improved from 0.27380 to 0.27352, saving model to ./bestmodel8.hdf5
3s - loss: 0.2683 - acc: 0.8937 - val_loss: 0.2735 - val_acc: 0.8904
Epoch 95/500
Epoch 00094: val_loss did not improve
3s - loss: 0.2680 - acc: 0.8933 - val_loss: 0.2771 - val_acc: 0.8881
Epoch 96/500
Epoch 00095: val_loss improved from 0.27352 to 0.27267, saving model to ./bestmodel8.hdf5
3s - loss: 0.2665 - acc: 0.8943 - val_loss: 0.2727 - val_acc: 0.8901
Epoch 97/500
Epoch 00096: val_loss did not improve
3s - loss: 0.2662 - acc: 0.8948 - val_loss: 0.2765 - val_acc: 0.8883
Epoch 98/500
Epoch 00097: val_loss did not improve
3s - loss: 0.2639 - acc: 0.8955 - val_loss: 0.2881 - val_acc: 0.8893
Epoch 99/500
Epoch 00098: val_loss did not improve
3s - loss: 0.2618 - acc: 0.8968 - val_loss: 0.2807 - val_acc: 0.8893
Epoch 100/500
Epoch 00099: val_loss did not improve
3s - loss: 0.2598 - acc: 0.8968 - val_loss: 0.2774 - val_acc: 0.8886
Epoch 101/500
Epoch 00100: val_loss improved from 0.27267 to 0.27228, saving model to ./bestmodel8.hdf5
3s - loss: 0.2615 - acc: 0.8957 - val_loss: 0.2723 - val_acc: 0.8893
Epoch 102/500
Epoch 00101: val_loss did not improve
3s - loss: 0.2605 - acc: 0.8973 - val_loss: 0.2835 - val_acc: 0.8883
Epoch 103/500
Epoch 00102: val_loss did not improve
3s - loss: 0.2605 - acc: 0.8964 - val_loss: 0.2730 - val_acc: 0.8921
Epoch 104/500
Epoch 00103: val_loss did not improve
3s - loss: 0.2602 - acc: 0.8972 - val_loss: 0.2736 - val_acc: 0.8919
Epoch 105/500
Epoch 00104: val_loss improved from 0.27228 to 0.27152, saving model to ./bestmodel8.hdf5
4s - loss: 0.2586 - acc: 0.8978 - val_loss: 0.2715 - val_acc: 0.8924
Epoch 106/500
Epoch 00105: val_loss improved from 0.27152 to 0.26846, saving model to ./bestmodel8.hdf5
3s - loss: 0.2590 - acc: 0.8985 - val_loss: 0.2685 - val_acc: 0.8944
Epoch 107/500
Epoch 00106: val_loss did not improve
3s - loss: 0.2545 - acc: 0.8997 - val_loss: 0.2709 - val_acc: 0.8916
Epoch 108/500
Epoch 00107: val_loss improved from 0.26846 to 0.26798, saving model to ./bestmodel8.hdf5
4s - loss: 0.2564 - acc: 0.9002 - val_loss: 0.2680 - val_acc: 0.8942
Epoch 109/500
Epoch 00108: val_loss improved from 0.26798 to 0.26657, saving model to ./bestmodel8.hdf5
4s - loss: 0.2549 - acc: 0.9003 - val_loss: 0.2666 - val_acc: 0.8950
Epoch 110/500
Epoch 00109: val_loss did not improve
3s - loss: 0.2541 - acc: 0.9004 - val_loss: 0.2722 - val_acc: 0.8896
Epoch 111/500
Epoch 00110: val_loss did not improve
3s - loss: 0.2520 - acc: 0.9009 - val_loss: 0.2832 - val_acc: 0.8888
Epoch 112/500
Epoch 00111: val_loss did not improve
3s - loss: 0.2531 - acc: 0.9006 - val_loss: 0.2684 - val_acc: 0.8927
Epoch 113/500
Epoch 00112: val_loss did not improve
3s - loss: 0.2513 - acc: 0.8998 - val_loss: 0.2673 - val_acc: 0.8924
Epoch 114/500
Epoch 00113: val_loss did not improve
3s - loss: 0.2522 - acc: 0.9019 - val_loss: 0.2673 - val_acc: 0.8947
Epoch 115/500
Epoch 00114: val_loss improved from 0.26657 to 0.26538, saving model to ./bestmodel8.hdf5
3s - loss: 0.2477 - acc: 0.9032 - val_loss: 0.2654 - val_acc: 0.8967
Epoch 116/500
Epoch 00115: val_loss improved from 0.26538 to 0.26501, saving model to ./bestmodel8.hdf5
4s - loss: 0.2493 - acc: 0.9014 - val_loss: 0.2650 - val_acc: 0.8960
Epoch 117/500
Epoch 00116: val_loss did not improve
3s - loss: 0.2498 - acc: 0.9039 - val_loss: 0.2676 - val_acc: 0.8937
Epoch 118/500
Epoch 00117: val_loss did not improve
3s - loss: 0.2465 - acc: 0.9039 - val_loss: 0.2681 - val_acc: 0.8950
Epoch 119/500
Epoch 00118: val_loss did not improve
3s - loss: 0.2479 - acc: 0.9047 - val_loss: 0.2672 - val_acc: 0.8955
Epoch 120/500
Epoch 00119: val_loss did not improve
3s - loss: 0.2461 - acc: 0.9051 - val_loss: 0.2685 - val_acc: 0.8929
Epoch 121/500
Epoch 00120: val_loss improved from 0.26501 to 0.26380, saving model to ./bestmodel8.hdf5
4s - loss: 0.2454 - acc: 0.9042 - val_loss: 0.2638 - val_acc: 0.8970
Epoch 122/500
Epoch 00121: val_loss did not improve
3s - loss: 0.2449 - acc: 0.9043 - val_loss: 0.2669 - val_acc: 0.8950
Epoch 123/500
Epoch 00122: val_loss did not improve
3s - loss: 0.2427 - acc: 0.9057 - val_loss: 0.2719 - val_acc: 0.8932
Epoch 124/500
Epoch 00123: val_loss did not improve
3s - loss: 0.2422 - acc: 0.9053 - val_loss: 0.2664 - val_acc: 0.8950
Epoch 125/500
Epoch 00124: val_loss did not improve
3s - loss: 0.2447 - acc: 0.9045 - val_loss: 0.2644 - val_acc: 0.8962
Epoch 126/500
Epoch 00125: val_loss did not improve
3s - loss: 0.2405 - acc: 0.9065 - val_loss: 0.2649 - val_acc: 0.8957
Epoch 127/500
Epoch 00126: val_loss improved from 0.26380 to 0.26185, saving model to ./bestmodel8.hdf5
3s - loss: 0.2412 - acc: 0.9068 - val_loss: 0.2619 - val_acc: 0.8967
Epoch 128/500
Epoch 00127: val_loss did not improve
3s - loss: 0.2397 - acc: 0.9068 - val_loss: 0.2703 - val_acc: 0.8914
Epoch 129/500
Epoch 00128: val_loss did not improve
3s - loss: 0.2430 - acc: 0.9054 - val_loss: 0.2644 - val_acc: 0.8957
Epoch 130/500
Epoch 00129: val_loss did not improve
3s - loss: 0.2374 - acc: 0.9069 - val_loss: 0.2670 - val_acc: 0.8927
Epoch 131/500
Epoch 00130: val_loss did not improve
3s - loss: 0.2387 - acc: 0.9085 - val_loss: 0.2644 - val_acc: 0.8960
Epoch 132/500
Epoch 00131: val_loss did not improve
3s - loss: 0.2389 - acc: 0.9064 - val_loss: 0.2631 - val_acc: 0.8939
Epoch 133/500
Epoch 00132: val_loss did not improve
3s - loss: 0.2347 - acc: 0.9091 - val_loss: 0.2649 - val_acc: 0.8967
Epoch 134/500
Epoch 00133: val_loss did not improve
3s - loss: 0.2375 - acc: 0.9083 - val_loss: 0.2690 - val_acc: 0.8972
Epoch 135/500
Epoch 00134: val_loss did not improve
3s - loss: 0.2335 - acc: 0.9102 - val_loss: 0.2631 - val_acc: 0.8983
Epoch 136/500
Epoch 00135: val_loss did not improve
3s - loss: 0.2356 - acc: 0.9100 - val_loss: 0.2624 - val_acc: 0.8955
Epoch 137/500
Epoch 00136: val_loss did not improve
3s - loss: 0.2342 - acc: 0.9116 - val_loss: 0.2645 - val_acc: 0.8962
Epoch 138/500
Epoch 00137: val_loss improved from 0.26185 to 0.26138, saving model to ./bestmodel8.hdf5
4s - loss: 0.2340 - acc: 0.9096 - val_loss: 0.2614 - val_acc: 0.8970
Epoch 139/500
Epoch 00138: val_loss did not improve
3s - loss: 0.2338 - acc: 0.9102 - val_loss: 0.2619 - val_acc: 0.8967
Epoch 140/500
Epoch 00139: val_loss did not improve
3s - loss: 0.2325 - acc: 0.9080 - val_loss: 0.2626 - val_acc: 0.8962
Epoch 141/500
Epoch 00140: val_loss did not improve
3s - loss: 0.2334 - acc: 0.9093 - val_loss: 0.2652 - val_acc: 0.8975
Epoch 142/500
Epoch 00141: val_loss improved from 0.26138 to 0.26131, saving model to ./bestmodel8.hdf5
3s - loss: 0.2320 - acc: 0.9117 - val_loss: 0.2613 - val_acc: 0.8978
Epoch 143/500
Epoch 00142: val_loss did not improve
3s - loss: 0.2321 - acc: 0.9118 - val_loss: 0.2679 - val_acc: 0.8962
Epoch 144/500
Epoch 00143: val_loss did not improve
3s - loss: 0.2325 - acc: 0.9121 - val_loss: 0.2619 - val_acc: 0.8970
Epoch 145/500
Epoch 00144: val_loss did not improve
3s - loss: 0.2303 - acc: 0.9097 - val_loss: 0.2619 - val_acc: 0.8947
Epoch 146/500
Epoch 00145: val_loss did not improve
3s - loss: 0.2315 - acc: 0.9120 - val_loss: 0.2623 - val_acc: 0.8962
Epoch 147/500
Epoch 00146: val_loss did not improve
3s - loss: 0.2289 - acc: 0.9125 - val_loss: 0.2622 - val_acc: 0.8952
Epoch 148/500
Epoch 00147: val_loss did not improve
3s - loss: 0.2283 - acc: 0.9113 - val_loss: 0.2656 - val_acc: 0.8970
Epoch 149/500
Epoch 00148: val_loss improved from 0.26131 to 0.26121, saving model to ./bestmodel8.hdf5
3s - loss: 0.2270 - acc: 0.9137 - val_loss: 0.2612 - val_acc: 0.8960
Epoch 150/500
Epoch 00149: val_loss did not improve
3s - loss: 0.2273 - acc: 0.9123 - val_loss: 0.2638 - val_acc: 0.8957
Epoch 151/500
Epoch 00150: val_loss improved from 0.26121 to 0.25559, saving model to ./bestmodel8.hdf5
4s - loss: 0.2249 - acc: 0.9144 - val_loss: 0.2556 - val_acc: 0.8972
Epoch 152/500
Epoch 00151: val_loss did not improve
3s - loss: 0.2278 - acc: 0.9135 - val_loss: 0.2672 - val_acc: 0.8957
Epoch 153/500
Epoch 00152: val_loss did not improve
3s - loss: 0.2259 - acc: 0.9134 - val_loss: 0.2572 - val_acc: 0.8998
Epoch 154/500
Epoch 00153: val_loss did not improve
3s - loss: 0.2244 - acc: 0.9151 - val_loss: 0.2598 - val_acc: 0.8972
Epoch 155/500
Epoch 00154: val_loss did not improve
3s - loss: 0.2240 - acc: 0.9141 - val_loss: 0.2604 - val_acc: 0.8990
Epoch 156/500
Epoch 00155: val_loss did not improve
3s - loss: 0.2250 - acc: 0.9140 - val_loss: 0.2633 - val_acc: 0.8975
Epoch 157/500
Epoch 00156: val_loss did not improve
3s - loss: 0.2234 - acc: 0.9129 - val_loss: 0.2640 - val_acc: 0.8965
Epoch 158/500
Epoch 00157: val_loss did not improve
3s - loss: 0.2224 - acc: 0.9159 - val_loss: 0.2611 - val_acc: 0.8957
Epoch 159/500
Epoch 00158: val_loss did not improve
3s - loss: 0.2214 - acc: 0.9159 - val_loss: 0.2645 - val_acc: 0.8998
Epoch 160/500
Epoch 00159: val_loss did not improve
3s - loss: 0.2202 - acc: 0.9167 - val_loss: 0.2600 - val_acc: 0.9006
Epoch 161/500
Epoch 00160: val_loss did not improve
3s - loss: 0.2227 - acc: 0.9152 - val_loss: 0.2591 - val_acc: 0.8952
Epoch 162/500
Epoch 00161: val_loss did not improve
3s - loss: 0.2221 - acc: 0.9152 - val_loss: 0.2573 - val_acc: 0.8957
Epoch 163/500
Epoch 00162: val_loss did not improve
3s - loss: 0.2200 - acc: 0.9166 - val_loss: 0.2580 - val_acc: 0.8952
Epoch 164/500
Epoch 00163: val_loss did not improve
3s - loss: 0.2200 - acc: 0.9149 - val_loss: 0.2593 - val_acc: 0.8975
Epoch 165/500
Epoch 00164: val_loss did not improve
3s - loss: 0.2174 - acc: 0.9176 - val_loss: 0.2608 - val_acc: 0.8970
Epoch 166/500
Epoch 00165: val_loss did not improve
3s - loss: 0.2186 - acc: 0.9164 - val_loss: 0.2636 - val_acc: 0.8998
Epoch 167/500
Epoch 00166: val_loss did not improve
3s - loss: 0.2174 - acc: 0.9172 - val_loss: 0.2661 - val_acc: 0.8972
Epoch 168/500
Epoch 00167: val_loss did not improve
3s - loss: 0.2162 - acc: 0.9172 - val_loss: 0.2600 - val_acc: 0.8970
Epoch 169/500
Epoch 00168: val_loss did not improve
3s - loss: 0.2172 - acc: 0.9170 - val_loss: 0.2591 - val_acc: 0.8957
Epoch 170/500
Epoch 00169: val_loss improved from 0.25559 to 0.25525, saving model to ./bestmodel8.hdf5
4s - loss: 0.2147 - acc: 0.9183 - val_loss: 0.2552 - val_acc: 0.9006
Epoch 171/500
Epoch 00170: val_loss did not improve
3s - loss: 0.2166 - acc: 0.9191 - val_loss: 0.2569 - val_acc: 0.9008
Epoch 172/500
Epoch 00171: val_loss improved from 0.25525 to 0.25426, saving model to ./bestmodel8.hdf5
4s - loss: 0.2146 - acc: 0.9180 - val_loss: 0.2543 - val_acc: 0.9016
Epoch 173/500
Epoch 00172: val_loss did not improve
3s - loss: 0.2156 - acc: 0.9165 - val_loss: 0.2612 - val_acc: 0.9008
Epoch 174/500
Epoch 00173: val_loss did not improve
3s - loss: 0.2161 - acc: 0.9199 - val_loss: 0.2574 - val_acc: 0.8988
Epoch 175/500
Epoch 00174: val_loss did not improve
3s - loss: 0.2149 - acc: 0.9182 - val_loss: 0.2650 - val_acc: 0.8983
Epoch 176/500
Epoch 00175: val_loss did not improve
3s - loss: 0.2119 - acc: 0.9196 - val_loss: 0.2576 - val_acc: 0.9029
Epoch 177/500
Epoch 00176: val_loss did not improve
3s - loss: 0.2115 - acc: 0.9209 - val_loss: 0.2581 - val_acc: 0.8962
Epoch 178/500
Epoch 00177: val_loss did not improve
3s - loss: 0.2125 - acc: 0.9197 - val_loss: 0.2578 - val_acc: 0.9013
Epoch 179/500
Epoch 00178: val_loss did not improve
3s - loss: 0.2120 - acc: 0.9194 - val_loss: 0.2585 - val_acc: 0.9003
Epoch 180/500
Epoch 00179: val_loss improved from 0.25426 to 0.25413, saving model to ./bestmodel8.hdf5
5s - loss: 0.2125 - acc: 0.9189 - val_loss: 0.2541 - val_acc: 0.9011
Epoch 181/500
Epoch 00180: val_loss did not improve
3s - loss: 0.2093 - acc: 0.9194 - val_loss: 0.2565 - val_acc: 0.8993
Epoch 182/500
Epoch 00181: val_loss did not improve
3s - loss: 0.2102 - acc: 0.9203 - val_loss: 0.2552 - val_acc: 0.9013
Epoch 183/500
Epoch 00182: val_loss did not improve
3s - loss: 0.2094 - acc: 0.9214 - val_loss: 0.2565 - val_acc: 0.9016
Epoch 184/500
Epoch 00183: val_loss improved from 0.25413 to 0.25398, saving model to ./bestmodel8.hdf5
4s - loss: 0.2111 - acc: 0.9195 - val_loss: 0.2540 - val_acc: 0.9036
Epoch 185/500
Epoch 00184: val_loss did not improve
3s - loss: 0.2090 - acc: 0.9207 - val_loss: 0.2618 - val_acc: 0.9011
Epoch 186/500
Epoch 00185: val_loss did not improve
3s - loss: 0.2094 - acc: 0.9199 - val_loss: 0.2559 - val_acc: 0.9023
Epoch 187/500
Epoch 00186: val_loss did not improve
3s - loss: 0.2092 - acc: 0.9209 - val_loss: 0.2541 - val_acc: 0.9011
Epoch 188/500
Epoch 00187: val_loss did not improve
3s - loss: 0.2063 - acc: 0.9214 - val_loss: 0.2592 - val_acc: 0.9011
Epoch 189/500
Epoch 00188: val_loss did not improve
3s - loss: 0.2071 - acc: 0.9220 - val_loss: 0.2585 - val_acc: 0.8990
Epoch 190/500
Epoch 00189: val_loss did not improve
3s - loss: 0.2061 - acc: 0.9217 - val_loss: 0.2542 - val_acc: 0.9008
Epoch 191/500
Epoch 00190: val_loss improved from 0.25398 to 0.25285, saving model to ./bestmodel8.hdf5
3s - loss: 0.2076 - acc: 0.9206 - val_loss: 0.2528 - val_acc: 0.9013
Epoch 192/500
Epoch 00191: val_loss improved from 0.25285 to 0.25027, saving model to ./bestmodel8.hdf5
4s - loss: 0.2058 - acc: 0.9200 - val_loss: 0.2503 - val_acc: 0.8998
Epoch 193/500
Epoch 00192: val_loss improved from 0.25027 to 0.24940, saving model to ./bestmodel8.hdf5
3s - loss: 0.2036 - acc: 0.9226 - val_loss: 0.2494 - val_acc: 0.9044
Epoch 194/500
Epoch 00193: val_loss did not improve
3s - loss: 0.2057 - acc: 0.9230 - val_loss: 0.2552 - val_acc: 0.9034
Epoch 195/500
Epoch 00194: val_loss did not improve
3s - loss: 0.2025 - acc: 0.9240 - val_loss: 0.2515 - val_acc: 0.9049
Epoch 196/500
Epoch 00195: val_loss did not improve
3s - loss: 0.2026 - acc: 0.9222 - val_loss: 0.2570 - val_acc: 0.9021
Epoch 197/500
Epoch 00196: val_loss did not improve
3s - loss: 0.2009 - acc: 0.9231 - val_loss: 0.2627 - val_acc: 0.8983
Epoch 198/500
Epoch 00197: val_loss did not improve
3s - loss: 0.2008 - acc: 0.9235 - val_loss: 0.2538 - val_acc: 0.9059
Epoch 199/500
Epoch 00198: val_loss did not improve
3s - loss: 0.2024 - acc: 0.9237 - val_loss: 0.2610 - val_acc: 0.9016
Epoch 200/500
Epoch 00199: val_loss did not improve
3s - loss: 0.2008 - acc: 0.9236 - val_loss: 0.2564 - val_acc: 0.9034
Epoch 201/500
Epoch 00200: val_loss did not improve
3s - loss: 0.2020 - acc: 0.9239 - val_loss: 0.2564 - val_acc: 0.9003
Epoch 202/500
Epoch 00201: val_loss did not improve
3s - loss: 0.1993 - acc: 0.9234 - val_loss: 0.2576 - val_acc: 0.9041
Epoch 203/500
Epoch 00202: val_loss did not improve
3s - loss: 0.1974 - acc: 0.9266 - val_loss: 0.2626 - val_acc: 0.8995
Epoch 204/500
Epoch 00203: val_loss did not improve
3s - loss: 0.2002 - acc: 0.9248 - val_loss: 0.2585 - val_acc: 0.9018
Epoch 205/500
Epoch 00204: val_loss did not improve
3s - loss: 0.2013 - acc: 0.9234 - val_loss: 0.2517 - val_acc: 0.9016
Epoch 206/500
Epoch 00205: val_loss did not improve
3s - loss: 0.1989 - acc: 0.9232 - val_loss: 0.2608 - val_acc: 0.9026
Epoch 207/500
Epoch 00206: val_loss did not improve
3s - loss: 0.1990 - acc: 0.9238 - val_loss: 0.2522 - val_acc: 0.9039
Epoch 208/500
Epoch 00207: val_loss did not improve
3s - loss: 0.1990 - acc: 0.9243 - val_loss: 0.2632 - val_acc: 0.9023
Epoch 209/500
Epoch 00208: val_loss did not improve
3s - loss: 0.1995 - acc: 0.9235 - val_loss: 0.2550 - val_acc: 0.9026
Epoch 210/500
Epoch 00209: val_loss did not improve
3s - loss: 0.1982 - acc: 0.9258 - val_loss: 0.2534 - val_acc: 0.9013
Epoch 211/500
Epoch 00210: val_loss did not improve
3s - loss: 0.1953 - acc: 0.9248 - val_loss: 0.2550 - val_acc: 0.9044
Epoch 212/500
Epoch 00211: val_loss did not improve
3s - loss: 0.1932 - acc: 0.9260 - val_loss: 0.2546 - val_acc: 0.9054
Epoch 213/500
Epoch 00212: val_loss did not improve
3s - loss: 0.1939 - acc: 0.9273 - val_loss: 0.2523 - val_acc: 0.9049
Epoch 214/500
Epoch 00213: val_loss did not improve
3s - loss: 0.1962 - acc: 0.9253 - val_loss: 0.2566 - val_acc: 0.9013
Epoch 215/500
Epoch 00214: val_loss did not improve
3s - loss: 0.1941 - acc: 0.9250 - val_loss: 0.2538 - val_acc: 0.9013
Epoch 216/500
Epoch 00215: val_loss did not improve
3s - loss: 0.1933 - acc: 0.9259 - val_loss: 0.2547 - val_acc: 0.9031
Epoch 217/500
Epoch 00216: val_loss did not improve
3s - loss: 0.1916 - acc: 0.9266 - val_loss: 0.2534 - val_acc: 0.9041
Epoch 218/500
Epoch 00217: val_loss did not improve
3s - loss: 0.1929 - acc: 0.9281 - val_loss: 0.2560 - val_acc: 0.9003
Epoch 219/500
Epoch 00218: val_loss did not improve
3s - loss: 0.1933 - acc: 0.9269 - val_loss: 0.2544 - val_acc: 0.9036
Epoch 220/500
Epoch 00219: val_loss did not improve
3s - loss: 0.1920 - acc: 0.9268 - val_loss: 0.2506 - val_acc: 0.9008
Epoch 221/500
Epoch 00220: val_loss did not improve
3s - loss: 0.1896 - acc: 0.9291 - val_loss: 0.2572 - val_acc: 0.9018
Epoch 222/500
Epoch 00221: val_loss did not improve
3s - loss: 0.1919 - acc: 0.9275 - val_loss: 0.2639 - val_acc: 0.9041
Epoch 223/500
Epoch 00222: val_loss did not improve
3s - loss: 0.1931 - acc: 0.9272 - val_loss: 0.2562 - val_acc: 0.8990
Epoch 224/500
Epoch 00223: val_loss did not improve
3s - loss: 0.1917 - acc: 0.9261 - val_loss: 0.2548 - val_acc: 0.9023
Epoch 225/500
Epoch 00224: val_loss did not improve
3s - loss: 0.1873 - acc: 0.9292 - val_loss: 0.2568 - val_acc: 0.9059
Epoch 226/500
Epoch 00225: val_loss did not improve
3s - loss: 0.1931 - acc: 0.9261 - val_loss: 0.2502 - val_acc: 0.9049
Epoch 227/500
Epoch 00226: val_loss did not improve
3s - loss: 0.1878 - acc: 0.9279 - val_loss: 0.2598 - val_acc: 0.9016
Epoch 228/500
Epoch 00227: val_loss did not improve
3s - loss: 0.1914 - acc: 0.9291 - val_loss: 0.2554 - val_acc: 0.9021
Epoch 229/500
Epoch 00228: val_loss did not improve
3s - loss: 0.1891 - acc: 0.9269 - val_loss: 0.2503 - val_acc: 0.9036
Epoch 230/500
Epoch 00229: val_loss did not improve
4s - loss: 0.1878 - acc: 0.9284 - val_loss: 0.2590 - val_acc: 0.9003
Epoch 231/500
Epoch 00230: val_loss did not improve
3s - loss: 0.1872 - acc: 0.9283 - val_loss: 0.2724 - val_acc: 0.8988
Epoch 232/500
Epoch 00231: val_loss did not improve
4s - loss: 0.1858 - acc: 0.9288 - val_loss: 0.2636 - val_acc: 0.9016
Epoch 233/500
Epoch 00232: val_loss did not improve
3s - loss: 0.1882 - acc: 0.9295 - val_loss: 0.2547 - val_acc: 0.9039
Epoch 234/500
Epoch 00233: val_loss did not improve
3s - loss: 0.1850 - acc: 0.9308 - val_loss: 0.2577 - val_acc: 0.8993
Epoch 235/500
Epoch 00234: val_loss did not improve
3s - loss: 0.1860 - acc: 0.9286 - val_loss: 0.2569 - val_acc: 0.9021
Epoch 236/500
Epoch 00235: val_loss did not improve
3s - loss: 0.1863 - acc: 0.9291 - val_loss: 0.2584 - val_acc: 0.9046
Epoch 237/500
Epoch 00236: val_loss did not improve
3s - loss: 0.1885 - acc: 0.9277 - val_loss: 0.2644 - val_acc: 0.8985
Epoch 238/500
Epoch 00237: val_loss did not improve
4s - loss: 0.1864 - acc: 0.9278 - val_loss: 0.2587 - val_acc: 0.9016
Epoch 239/500
Epoch 00238: val_loss did not improve
3s - loss: 0.1845 - acc: 0.9299 - val_loss: 0.2615 - val_acc: 0.9001
Epoch 240/500
Epoch 00239: val_loss did not improve
3s - loss: 0.1838 - acc: 0.9305 - val_loss: 0.2611 - val_acc: 0.9041
Epoch 241/500
Epoch 00240: val_loss did not improve
3s - loss: 0.1830 - acc: 0.9306 - val_loss: 0.2600 - val_acc: 0.9052
Epoch 242/500
Epoch 00241: val_loss did not improve
3s - loss: 0.1845 - acc: 0.9305 - val_loss: 0.2581 - val_acc: 0.9046
Epoch 243/500
Epoch 00242: val_loss did not improve
4s - loss: 0.1849 - acc: 0.9301 - val_loss: 0.2601 - val_acc: 0.9044
Epoch 244/500
Epoch 00243: val_loss did not improve
Epoch 00243: early stopping
3s - loss: 0.1849 - acc: 0.9295 - val_loss: 0.2576 - val_acc: 0.9029
training done!
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
cov1 (Convolution1D)             (None, 58, 80)        1360        convolution1d_input_2[0][0]      
____________________________________________________________________________________________________
leakyrelu_1 (LeakyReLU)          (None, 58, 80)        0           cov1[0][0]                       
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 58, 80)        0           leakyrelu_1[0][0]                
____________________________________________________________________________________________________
cov2 (Convolution1D)             (None, 57, 80)        12880       dropout_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_2 (LeakyReLU)          (None, 57, 80)        0           cov2[0][0]                       
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 57, 80)        0           leakyrelu_2[0][0]                
____________________________________________________________________________________________________
cov3 (Convolution1D)             (None, 54, 80)        25680       dropout_2[0][0]                  
____________________________________________________________________________________________________
leakyrelu_3 (LeakyReLU)          (None, 54, 80)        0           cov3[0][0]                       
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 54, 80)        0           leakyrelu_3[0][0]                
____________________________________________________________________________________________________
cov4 (Convolution1D)             (None, 51, 80)        25680       dropout_3[0][0]                  
____________________________________________________________________________________________________
leakyrelu_4 (LeakyReLU)          (None, 51, 80)        0           cov4[0][0]                       
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 51, 80)        0           leakyrelu_4[0][0]                
____________________________________________________________________________________________________
cov5 (Convolution1D)             (None, 48, 80)        25680       dropout_4[0][0]                  
____________________________________________________________________________________________________
leakyrelu_5 (LeakyReLU)          (None, 48, 80)        0           cov5[0][0]                       
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 48, 80)        0           leakyrelu_5[0][0]                
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 3840)          0           dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           384100      flatten_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_6 (LeakyReLU)          (None, 100)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 100)           0           leakyrelu_6[0][0]                
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             101         dropout_6[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    
====================================================================================================
Total params: 475481
____________________________________________________________________________________________________
**************vadiation results on validation dataset************
  32/3922 [..............................] - ETA: 0s  64/3922 [..............................] - ETA: 2s 160/3922 [>.............................] - ETA: 1s 256/3922 [>.............................] - ETA: 0s 352/3922 [=>............................] - ETA: 0s 448/3922 [==>...........................] - ETA: 0s 544/3922 [===>..........................] - ETA: 0s 640/3922 [===>..........................] - ETA: 0s 736/3922 [====>.........................] - ETA: 0s 832/3922 [=====>........................] - ETA: 0s 928/3922 [======>.......................] - ETA: 0s1024/3922 [======>.......................] - ETA: 0s1120/3922 [=======>......................] - ETA: 0s1216/3922 [========>.....................] - ETA: 0s1312/3922 [=========>....................] - ETA: 0s1408/3922 [=========>....................] - ETA: 0s1504/3922 [==========>...................] - ETA: 0s1600/3922 [===========>..................] - ETA: 0s1696/3922 [===========>..................] - ETA: 0s1792/3922 [============>.................] - ETA: 0s1888/3922 [=============>................] - ETA: 0s1984/3922 [==============>...............] - ETA: 0s2080/3922 [==============>...............] - ETA: 0s2176/3922 [===============>..............] - ETA: 0s2272/3922 [================>.............] - ETA: 0s2368/3922 [=================>............] - ETA: 0s2464/3922 [=================>............] - ETA: 0s2560/3922 [==================>...........] - ETA: 0s2656/3922 [===================>..........] - ETA: 0s2752/3922 [====================>.........] - ETA: 0s2848/3922 [====================>.........] - ETA: 0s2944/3922 [=====================>........] - ETA: 0s3040/3922 [======================>.......] - ETA: 0s3136/3922 [======================>.......] - ETA: 0s3232/3922 [=======================>......] - ETA: 0s3328/3922 [========================>.....] - ETA: 0s3456/3922 [=========================>....] - ETA: 0s3584/3922 [==========================>...] - ETA: 0s3712/3922 [===========================>..] - ETA: 0s3840/3922 [============================>.] - ETA: 0s[0.24940089115465497, 0.90438551747148466]
  32/3922 [..............................] - ETA: 0s 160/3922 [>.............................] - ETA: 0s 288/3922 [=>............................] - ETA: 0s 416/3922 [==>...........................] - ETA: 0s 544/3922 [===>..........................] - ETA: 0s 672/3922 [====>.........................] - ETA: 0s 800/3922 [=====>........................] - ETA: 0s 928/3922 [======>.......................] - ETA: 0s1056/3922 [=======>......................] - ETA: 0s1184/3922 [========>.....................] - ETA: 0s1312/3922 [=========>....................] - ETA: 0s1440/3922 [==========>...................] - ETA: 0s1568/3922 [==========>...................] - ETA: 0s1696/3922 [===========>..................] - ETA: 0s1824/3922 [============>.................] - ETA: 0s1952/3922 [=============>................] - ETA: 0s2080/3922 [==============>...............] - ETA: 0s2208/3922 [===============>..............] - ETA: 0s2336/3922 [================>.............] - ETA: 0s2464/3922 [=================>............] - ETA: 0s2592/3922 [==================>...........] - ETA: 0s2720/3922 [===================>..........] - ETA: 0s2848/3922 [====================>.........] - ETA: 0s2976/3922 [=====================>........] - ETA: 0s3104/3922 [======================>.......] - ETA: 0s3232/3922 [=======================>......] - ETA: 0s3360/3922 [========================>.....] - ETA: 0s3488/3922 [=========================>....] - ETA: 0s3616/3922 [==========================>...] - ETA: 0s3744/3922 [===========================>..] - ETA: 0s3872/3922 [============================>.] - ETA: 0s  32/3922 [..............................] - ETA: 0s 160/3922 [>.............................] - ETA: 0s 288/3922 [=>............................] - ETA: 0s 416/3922 [==>...........................] - ETA: 0s 544/3922 [===>..........................] - ETA: 0s 672/3922 [====>.........................] - ETA: 0s 800/3922 [=====>........................] - ETA: 0s 928/3922 [======>.......................] - ETA: 0s1056/3922 [=======>......................] - ETA: 0s1184/3922 [========>.....................] - ETA: 0s1312/3922 [=========>....................] - ETA: 0s1440/3922 [==========>...................] - ETA: 0s1568/3922 [==========>...................] - ETA: 0s1696/3922 [===========>..................] - ETA: 0s1824/3922 [============>.................] - ETA: 0s1952/3922 [=============>................] - ETA: 0s2080/3922 [==============>...............] - ETA: 0s2208/3922 [===============>..............] - ETA: 0s2336/3922 [================>.............] - ETA: 0s2464/3922 [=================>............] - ETA: 0s2592/3922 [==================>...........] - ETA: 0s2720/3922 [===================>..........] - ETA: 0s2848/3922 [====================>.........] - ETA: 0s2976/3922 [=====================>........] - ETA: 0s3104/3922 [======================>.......] - ETA: 0s3232/3922 [=======================>......] - ETA: 0s3360/3922 [========================>.....] - ETA: 0s3488/3922 [=========================>....] - ETA: 0s3616/3922 [==========================>...] - ETA: 0s3744/3922 [===========================>..] - ETA: 0s3872/3922 [============================>.] - ETA: 0s************************
auc: 0.961100979555
mcc: 0.809845886136
negative ---> precision:0.884578079534, recall:0.930137684855, f1score:0.906785980611, support:1961
positive ---> precision:0.926344086022, recall:0.878633350331, f1score:0.901858152316, support:1961
************************
**************prediction results on test dataset************
  32/3928 [..............................] - ETA: 0s 160/3928 [>.............................] - ETA: 0s 288/3928 [=>............................] - ETA: 0s 416/3928 [==>...........................] - ETA: 0s 544/3928 [===>..........................] - ETA: 0s 672/3928 [====>.........................] - ETA: 0s 800/3928 [=====>........................] - ETA: 0s 928/3928 [======>.......................] - ETA: 0s1056/3928 [=======>......................] - ETA: 0s1184/3928 [========>.....................] - ETA: 0s1312/3928 [=========>....................] - ETA: 0s1440/3928 [=========>....................] - ETA: 0s1568/3928 [==========>...................] - ETA: 0s1696/3928 [===========>..................] - ETA: 0s1824/3928 [============>.................] - ETA: 0s1952/3928 [=============>................] - ETA: 0s2080/3928 [==============>...............] - ETA: 0s2208/3928 [===============>..............] - ETA: 0s2336/3928 [================>.............] - ETA: 0s2464/3928 [=================>............] - ETA: 0s2592/3928 [==================>...........] - ETA: 0s2720/3928 [===================>..........] - ETA: 0s2848/3928 [====================>.........] - ETA: 0s2976/3928 [=====================>........] - ETA: 0s3104/3928 [======================>.......] - ETA: 0s3232/3928 [=======================>......] - ETA: 0s3360/3928 [========================>.....] - ETA: 0s3488/3928 [=========================>....] - ETA: 0s3616/3928 [==========================>...] - ETA: 0s3744/3928 [===========================>..] - ETA: 0s3872/3928 [============================>.] - ETA: 0s[0.26326315827258007, 0.90631364549978688]
  32/3928 [..............................] - ETA: 0s 160/3928 [>.............................] - ETA: 0s 288/3928 [=>............................] - ETA: 0s 416/3928 [==>...........................] - ETA: 0s 544/3928 [===>..........................] - ETA: 0s 672/3928 [====>.........................] - ETA: 0s 800/3928 [=====>........................] - ETA: 0s 928/3928 [======>.......................] - ETA: 0s1056/3928 [=======>......................] - ETA: 0s1184/3928 [========>.....................] - ETA: 0s1312/3928 [=========>....................] - ETA: 0s1440/3928 [=========>....................] - ETA: 0s1568/3928 [==========>...................] - ETA: 0s1696/3928 [===========>..................] - ETA: 0s1824/3928 [============>.................] - ETA: 0s1952/3928 [=============>................] - ETA: 0s2080/3928 [==============>...............] - ETA: 0s2208/3928 [===============>..............] - ETA: 0s2336/3928 [================>.............] - ETA: 0s2464/3928 [=================>............] - ETA: 0s2592/3928 [==================>...........] - ETA: 0s2720/3928 [===================>..........] - ETA: 0s2848/3928 [====================>.........] - ETA: 0s2976/3928 [=====================>........] - ETA: 0s3104/3928 [======================>.......] - ETA: 0s3232/3928 [=======================>......] - ETA: 0s3360/3928 [========================>.....] - ETA: 0s3488/3928 [=========================>....] - ETA: 0s3616/3928 [==========================>...] - ETA: 0s3744/3928 [===========================>..] - ETA: 0s3872/3928 [============================>.] - ETA: 0s  32/3928 [..............................] - ETA: 0s 160/3928 [>.............................] - ETA: 0s 288/3928 [=>............................] - ETA: 0s 416/3928 [==>...........................] - ETA: 0s 544/3928 [===>..........................] - ETA: 0s 672/3928 [====>.........................] - ETA: 0s 800/3928 [=====>........................] - ETA: 0s 928/3928 [======>.......................] - ETA: 0s1056/3928 [=======>......................] - ETA: 0s1184/3928 [========>.....................] - ETA: 0s1312/3928 [=========>....................] - ETA: 0s1440/3928 [=========>....................] - ETA: 0s1568/3928 [==========>...................] - ETA: 0s1696/3928 [===========>..................] - ETA: 0s1824/3928 [============>.................] - ETA: 0s1952/3928 [=============>................] - ETA: 0s2080/3928 [==============>...............] - ETA: 0s2208/3928 [===============>..............] - ETA: 0s2336/3928 [================>.............] - ETA: 0s2464/3928 [=================>............] - ETA: 0s2592/3928 [==================>...........] - ETA: 0s2720/3928 [===================>..........] - ETA: 0s2848/3928 [====================>.........] - ETA: 0s2976/3928 [=====================>........] - ETA: 0s3104/3928 [======================>.......] - ETA: 0s3232/3928 [=======================>......] - ETA: 0s3360/3928 [========================>.....] - ETA: 0s3488/3928 [=========================>....] - ETA: 0s3616/3928 [==========================>...] - ETA: 0s3744/3928 [===========================>..] - ETA: 0s3872/3928 [============================>.] - ETA: 0s************************
auc: 0.956042134179
mcc: 0.813407481337
negative ---> precision:0.889268292683, recall:0.928207739308, f1score:0.908320876931, support:1964
positive ---> precision:0.924920127796, recall:0.884419551935, f1score:0.904216553878, support:1964
************************
