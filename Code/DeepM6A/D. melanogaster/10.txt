Using Theano backend.
Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 10.0% of memory, cuDNN 5005)
loading data
10
(17039, 61, 4)
('train_label: count', (array([0, 1]), array([8520, 8519])))
('valid_label: count', (array([0, 1]), array([1064, 1065])))
('test_label: count', (array([0, 1]), array([1064, 1065])))
building model...............
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
cov1 (Convolution1D)             (None, 58, 80)        1360        convolution1d_input_1[0][0]      
____________________________________________________________________________________________________
leakyrelu_1 (LeakyReLU)          (None, 58, 80)        0           cov1[0][0]                       
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 58, 80)        0           leakyrelu_1[0][0]                
____________________________________________________________________________________________________
cov2 (Convolution1D)             (None, 57, 80)        12880       dropout_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_2 (LeakyReLU)          (None, 57, 80)        0           cov2[0][0]                       
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 57, 80)        0           leakyrelu_2[0][0]                
____________________________________________________________________________________________________
cov3 (Convolution1D)             (None, 54, 80)        25680       dropout_2[0][0]                  
____________________________________________________________________________________________________
leakyrelu_3 (LeakyReLU)          (None, 54, 80)        0           cov3[0][0]                       
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 54, 80)        0           leakyrelu_3[0][0]                
____________________________________________________________________________________________________
cov4 (Convolution1D)             (None, 51, 80)        25680       dropout_3[0][0]                  
____________________________________________________________________________________________________
leakyrelu_4 (LeakyReLU)          (None, 51, 80)        0           cov4[0][0]                       
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 51, 80)        0           leakyrelu_4[0][0]                
____________________________________________________________________________________________________
cov5 (Convolution1D)             (None, 48, 80)        25680       dropout_4[0][0]                  
____________________________________________________________________________________________________
leakyrelu_5 (LeakyReLU)          (None, 48, 80)        0           cov5[0][0]                       
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 48, 80)        0           leakyrelu_5[0][0]                
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 3840)          0           dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           384100      flatten_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_6 (LeakyReLU)          (None, 100)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 100)           0           leakyrelu_6[0][0]                
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             101         dropout_6[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    
====================================================================================================
Total params: 475481
____________________________________________________________________________________________________
compiling and fitting model...........
Train on 17039 samples, validate on 2129 samples
Epoch 1/500
Epoch 00000: val_loss improved from inf to 0.69322, saving model to ./bestmodel10.hdf5
2s - loss: 0.6933 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.5002
Epoch 2/500
Epoch 00001: val_loss improved from 0.69322 to 0.69314, saving model to ./bestmodel10.hdf5
2s - loss: 0.6932 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 3/500
Epoch 00002: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.5002
Epoch 4/500
Epoch 00003: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 5/500
Epoch 00004: val_loss improved from 0.69314 to 0.69314, saving model to ./bestmodel10.hdf5
2s - loss: 0.6931 - acc: 0.5020 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 6/500
Epoch 00005: val_loss did not improve
1s - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 7/500
Epoch 00006: val_loss did not improve
1s - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4998
Epoch 8/500
Epoch 00007: val_loss improved from 0.69314 to 0.69314, saving model to ./bestmodel10.hdf5
2s - loss: 0.6933 - acc: 0.4914 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 9/500
Epoch 00008: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4939 - val_loss: 0.6931 - val_acc: 0.5853
Epoch 10/500
Epoch 00009: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4964 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 11/500
Epoch 00010: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 12/500
Epoch 00011: val_loss did not improve
1s - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 13/500
Epoch 00012: val_loss improved from 0.69314 to 0.69313, saving model to ./bestmodel10.hdf5
2s - loss: 0.6931 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5007
Epoch 14/500
Epoch 00013: val_loss improved from 0.69313 to 0.69313, saving model to ./bestmodel10.hdf5
3s - loss: 0.6932 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 15/500
Epoch 00014: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 16/500
Epoch 00015: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 17/500
Epoch 00016: val_loss did not improve
1s - loss: 0.6931 - acc: 0.5030 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 18/500
Epoch 00017: val_loss improved from 0.69313 to 0.69313, saving model to ./bestmodel10.hdf5
3s - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 19/500
Epoch 00018: val_loss improved from 0.69313 to 0.69312, saving model to ./bestmodel10.hdf5
3s - loss: 0.6932 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 20/500
Epoch 00019: val_loss did not improve
1s - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4998
Epoch 21/500
Epoch 00020: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 22/500
Epoch 00021: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 23/500
Epoch 00022: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 24/500
Epoch 00023: val_loss improved from 0.69312 to 0.69312, saving model to ./bestmodel10.hdf5
2s - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 25/500
Epoch 00024: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4951 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 26/500
Epoch 00025: val_loss did not improve
1s - loss: 0.6933 - acc: 0.4957 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 27/500
Epoch 00026: val_loss did not improve
1s - loss: 0.6932 - acc: 0.5028 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 28/500
Epoch 00027: val_loss improved from 0.69312 to 0.69310, saving model to ./bestmodel10.hdf5
2s - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 29/500
Epoch 00028: val_loss did not improve
1s - loss: 0.6932 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5002
Epoch 30/500
Epoch 00029: val_loss improved from 0.69310 to 0.69308, saving model to ./bestmodel10.hdf5
2s - loss: 0.6931 - acc: 0.5040 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 31/500
Epoch 00030: val_loss improved from 0.69308 to 0.69307, saving model to ./bestmodel10.hdf5
3s - loss: 0.6930 - acc: 0.5071 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 32/500
Epoch 00031: val_loss improved from 0.69307 to 0.69306, saving model to ./bestmodel10.hdf5
3s - loss: 0.6931 - acc: 0.5003 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 33/500
Epoch 00032: val_loss improved from 0.69306 to 0.69305, saving model to ./bestmodel10.hdf5
2s - loss: 0.6931 - acc: 0.5036 - val_loss: 0.6931 - val_acc: 0.4998
Epoch 34/500
Epoch 00033: val_loss improved from 0.69305 to 0.69303, saving model to ./bestmodel10.hdf5
3s - loss: 0.6931 - acc: 0.5040 - val_loss: 0.6930 - val_acc: 0.5002
Epoch 35/500
Epoch 00034: val_loss improved from 0.69303 to 0.69301, saving model to ./bestmodel10.hdf5
2s - loss: 0.6931 - acc: 0.5034 - val_loss: 0.6930 - val_acc: 0.4998
Epoch 36/500
Epoch 00035: val_loss improved from 0.69301 to 0.69299, saving model to ./bestmodel10.hdf5
2s - loss: 0.6931 - acc: 0.5049 - val_loss: 0.6930 - val_acc: 0.4998
Epoch 37/500
Epoch 00036: val_loss improved from 0.69299 to 0.69297, saving model to ./bestmodel10.hdf5
2s - loss: 0.6931 - acc: 0.5055 - val_loss: 0.6930 - val_acc: 0.4998
Epoch 38/500
Epoch 00037: val_loss improved from 0.69297 to 0.69289, saving model to ./bestmodel10.hdf5
2s - loss: 0.6931 - acc: 0.5047 - val_loss: 0.6929 - val_acc: 0.4998
Epoch 39/500
Epoch 00038: val_loss did not improve
1s - loss: 0.6929 - acc: 0.5094 - val_loss: 0.6930 - val_acc: 0.5002
Epoch 40/500
Epoch 00039: val_loss improved from 0.69289 to 0.69270, saving model to ./bestmodel10.hdf5
4s - loss: 0.6928 - acc: 0.5102 - val_loss: 0.6927 - val_acc: 0.4998
Epoch 41/500
Epoch 00040: val_loss improved from 0.69270 to 0.69249, saving model to ./bestmodel10.hdf5
2s - loss: 0.6925 - acc: 0.5249 - val_loss: 0.6925 - val_acc: 0.5002
Epoch 42/500
Epoch 00041: val_loss improved from 0.69249 to 0.69199, saving model to ./bestmodel10.hdf5
3s - loss: 0.6922 - acc: 0.5363 - val_loss: 0.6920 - val_acc: 0.6905
Epoch 43/500
Epoch 00042: val_loss improved from 0.69199 to 0.69076, saving model to ./bestmodel10.hdf5
2s - loss: 0.6912 - acc: 0.5586 - val_loss: 0.6908 - val_acc: 0.6186
Epoch 44/500
Epoch 00043: val_loss improved from 0.69076 to 0.68554, saving model to ./bestmodel10.hdf5
3s - loss: 0.6885 - acc: 0.6070 - val_loss: 0.6855 - val_acc: 0.6844
Epoch 45/500
Epoch 00044: val_loss improved from 0.68554 to 0.63886, saving model to ./bestmodel10.hdf5
2s - loss: 0.6693 - acc: 0.6769 - val_loss: 0.6389 - val_acc: 0.6773
Epoch 46/500
Epoch 00045: val_loss improved from 0.63886 to 0.55419, saving model to ./bestmodel10.hdf5
4s - loss: 0.5582 - acc: 0.7361 - val_loss: 0.5542 - val_acc: 0.7163
Epoch 47/500
Epoch 00046: val_loss improved from 0.55419 to 0.52546, saving model to ./bestmodel10.hdf5
3s - loss: 0.5043 - acc: 0.7718 - val_loss: 0.5255 - val_acc: 0.7332
Epoch 48/500
Epoch 00047: val_loss improved from 0.52546 to 0.50618, saving model to ./bestmodel10.hdf5
2s - loss: 0.4788 - acc: 0.7877 - val_loss: 0.5062 - val_acc: 0.7590
Epoch 49/500
Epoch 00048: val_loss improved from 0.50618 to 0.50076, saving model to ./bestmodel10.hdf5
2s - loss: 0.4403 - acc: 0.8108 - val_loss: 0.5008 - val_acc: 0.7689
Epoch 50/500
Epoch 00049: val_loss improved from 0.50076 to 0.48873, saving model to ./bestmodel10.hdf5
2s - loss: 0.4195 - acc: 0.8253 - val_loss: 0.4887 - val_acc: 0.7764
Epoch 51/500
Epoch 00050: val_loss improved from 0.48873 to 0.47521, saving model to ./bestmodel10.hdf5
2s - loss: 0.4007 - acc: 0.8343 - val_loss: 0.4752 - val_acc: 0.7868
Epoch 52/500
Epoch 00051: val_loss did not improve
1s - loss: 0.3874 - acc: 0.8386 - val_loss: 0.4876 - val_acc: 0.7788
Epoch 53/500
Epoch 00052: val_loss improved from 0.47521 to 0.46904, saving model to ./bestmodel10.hdf5
3s - loss: 0.3744 - acc: 0.8472 - val_loss: 0.4690 - val_acc: 0.7961
Epoch 54/500
Epoch 00053: val_loss improved from 0.46904 to 0.46792, saving model to ./bestmodel10.hdf5
2s - loss: 0.3668 - acc: 0.8526 - val_loss: 0.4679 - val_acc: 0.7961
Epoch 55/500
Epoch 00054: val_loss improved from 0.46792 to 0.46635, saving model to ./bestmodel10.hdf5
2s - loss: 0.3600 - acc: 0.8565 - val_loss: 0.4663 - val_acc: 0.7943
Epoch 56/500
Epoch 00055: val_loss did not improve
1s - loss: 0.3488 - acc: 0.8582 - val_loss: 0.4783 - val_acc: 0.7919
Epoch 57/500
Epoch 00056: val_loss did not improve
1s - loss: 0.3492 - acc: 0.8609 - val_loss: 0.4739 - val_acc: 0.7919
Epoch 58/500
Epoch 00057: val_loss did not improve
1s - loss: 0.3407 - acc: 0.8649 - val_loss: 0.5017 - val_acc: 0.7886
Epoch 59/500
Epoch 00058: val_loss did not improve
1s - loss: 0.3369 - acc: 0.8657 - val_loss: 0.4700 - val_acc: 0.7971
Epoch 60/500
Epoch 00059: val_loss improved from 0.46635 to 0.44815, saving model to ./bestmodel10.hdf5
2s - loss: 0.3312 - acc: 0.8667 - val_loss: 0.4482 - val_acc: 0.8046
Epoch 61/500
Epoch 00060: val_loss did not improve
1s - loss: 0.3272 - acc: 0.8713 - val_loss: 0.4611 - val_acc: 0.8037
Epoch 62/500
Epoch 00061: val_loss improved from 0.44815 to 0.43357, saving model to ./bestmodel10.hdf5
2s - loss: 0.3234 - acc: 0.8729 - val_loss: 0.4336 - val_acc: 0.8084
Epoch 63/500
Epoch 00062: val_loss did not improve
1s - loss: 0.3210 - acc: 0.8735 - val_loss: 0.4493 - val_acc: 0.8051
Epoch 64/500
Epoch 00063: val_loss did not improve
1s - loss: 0.3187 - acc: 0.8743 - val_loss: 0.4588 - val_acc: 0.8046
Epoch 65/500
Epoch 00064: val_loss improved from 0.43357 to 0.42515, saving model to ./bestmodel10.hdf5
3s - loss: 0.3149 - acc: 0.8769 - val_loss: 0.4251 - val_acc: 0.8154
Epoch 66/500
Epoch 00065: val_loss did not improve
1s - loss: 0.3125 - acc: 0.8782 - val_loss: 0.4475 - val_acc: 0.8140
Epoch 67/500
Epoch 00066: val_loss did not improve
1s - loss: 0.3046 - acc: 0.8810 - val_loss: 0.4518 - val_acc: 0.8112
Epoch 68/500
Epoch 00067: val_loss did not improve
1s - loss: 0.3038 - acc: 0.8822 - val_loss: 0.4256 - val_acc: 0.8248
Epoch 69/500
Epoch 00068: val_loss improved from 0.42515 to 0.42212, saving model to ./bestmodel10.hdf5
3s - loss: 0.3055 - acc: 0.8831 - val_loss: 0.4221 - val_acc: 0.8281
Epoch 70/500
Epoch 00069: val_loss did not improve
1s - loss: 0.2987 - acc: 0.8839 - val_loss: 0.4328 - val_acc: 0.8225
Epoch 71/500
Epoch 00070: val_loss did not improve
1s - loss: 0.2968 - acc: 0.8831 - val_loss: 0.4331 - val_acc: 0.8262
Epoch 72/500
Epoch 00071: val_loss did not improve
1s - loss: 0.2962 - acc: 0.8830 - val_loss: 0.4304 - val_acc: 0.8196
Epoch 73/500
Epoch 00072: val_loss improved from 0.42212 to 0.41158, saving model to ./bestmodel10.hdf5
2s - loss: 0.2909 - acc: 0.8862 - val_loss: 0.4116 - val_acc: 0.8337
Epoch 74/500
Epoch 00073: val_loss did not improve
1s - loss: 0.2880 - acc: 0.8871 - val_loss: 0.4449 - val_acc: 0.8093
Epoch 75/500
Epoch 00074: val_loss did not improve
1s - loss: 0.2859 - acc: 0.8887 - val_loss: 0.4243 - val_acc: 0.8300
Epoch 76/500
Epoch 00075: val_loss did not improve
1s - loss: 0.2850 - acc: 0.8885 - val_loss: 0.4118 - val_acc: 0.8276
Epoch 77/500
Epoch 00076: val_loss improved from 0.41158 to 0.40884, saving model to ./bestmodel10.hdf5
2s - loss: 0.2795 - acc: 0.8914 - val_loss: 0.4088 - val_acc: 0.8333
Epoch 78/500
Epoch 00077: val_loss did not improve
1s - loss: 0.2810 - acc: 0.8908 - val_loss: 0.4327 - val_acc: 0.8271
Epoch 79/500
Epoch 00078: val_loss did not improve
1s - loss: 0.2802 - acc: 0.8921 - val_loss: 0.4211 - val_acc: 0.8295
Epoch 80/500
Epoch 00079: val_loss did not improve
1s - loss: 0.2784 - acc: 0.8914 - val_loss: 0.4145 - val_acc: 0.8248
Epoch 81/500
Epoch 00080: val_loss did not improve
1s - loss: 0.2752 - acc: 0.8944 - val_loss: 0.4233 - val_acc: 0.8248
Epoch 82/500
Epoch 00081: val_loss did not improve
1s - loss: 0.2737 - acc: 0.8947 - val_loss: 0.4251 - val_acc: 0.8262
Epoch 83/500
Epoch 00082: val_loss did not improve
1s - loss: 0.2713 - acc: 0.8941 - val_loss: 0.4546 - val_acc: 0.8234
Epoch 84/500
Epoch 00083: val_loss did not improve
1s - loss: 0.2701 - acc: 0.8968 - val_loss: 0.4462 - val_acc: 0.8248
Epoch 85/500
Epoch 00084: val_loss improved from 0.40884 to 0.40374, saving model to ./bestmodel10.hdf5
2s - loss: 0.2688 - acc: 0.8957 - val_loss: 0.4037 - val_acc: 0.8408
Epoch 86/500
Epoch 00085: val_loss did not improve
1s - loss: 0.2645 - acc: 0.8969 - val_loss: 0.4157 - val_acc: 0.8408
Epoch 87/500
Epoch 00086: val_loss did not improve
1s - loss: 0.2650 - acc: 0.8972 - val_loss: 0.4221 - val_acc: 0.8333
Epoch 88/500
Epoch 00087: val_loss did not improve
1s - loss: 0.2625 - acc: 0.9011 - val_loss: 0.4043 - val_acc: 0.8394
Epoch 89/500
Epoch 00088: val_loss did not improve
1s - loss: 0.2598 - acc: 0.8995 - val_loss: 0.4197 - val_acc: 0.8394
Epoch 90/500
Epoch 00089: val_loss improved from 0.40374 to 0.40338, saving model to ./bestmodel10.hdf5
3s - loss: 0.2585 - acc: 0.9012 - val_loss: 0.4034 - val_acc: 0.8394
Epoch 91/500
Epoch 00090: val_loss did not improve
1s - loss: 0.2543 - acc: 0.9017 - val_loss: 0.4199 - val_acc: 0.8412
Epoch 92/500
Epoch 00091: val_loss did not improve
1s - loss: 0.2589 - acc: 0.9004 - val_loss: 0.4085 - val_acc: 0.8380
Epoch 93/500
Epoch 00092: val_loss improved from 0.40338 to 0.39367, saving model to ./bestmodel10.hdf5
2s - loss: 0.2540 - acc: 0.9049 - val_loss: 0.3937 - val_acc: 0.8441
Epoch 94/500
Epoch 00093: val_loss did not improve
1s - loss: 0.2514 - acc: 0.9033 - val_loss: 0.3945 - val_acc: 0.8436
Epoch 95/500
Epoch 00094: val_loss did not improve
1s - loss: 0.2512 - acc: 0.9021 - val_loss: 0.4451 - val_acc: 0.8257
Epoch 96/500
Epoch 00095: val_loss did not improve
1s - loss: 0.2479 - acc: 0.9050 - val_loss: 0.4088 - val_acc: 0.8450
Epoch 97/500
Epoch 00096: val_loss did not improve
1s - loss: 0.2455 - acc: 0.9061 - val_loss: 0.4098 - val_acc: 0.8445
Epoch 98/500
Epoch 00097: val_loss did not improve
1s - loss: 0.2471 - acc: 0.9080 - val_loss: 0.4013 - val_acc: 0.8422
Epoch 99/500
Epoch 00098: val_loss did not improve
1s - loss: 0.2447 - acc: 0.9049 - val_loss: 0.3969 - val_acc: 0.8478
Epoch 100/500
Epoch 00099: val_loss improved from 0.39367 to 0.38642, saving model to ./bestmodel10.hdf5
3s - loss: 0.2419 - acc: 0.9087 - val_loss: 0.3864 - val_acc: 0.8478
Epoch 101/500
Epoch 00100: val_loss did not improve
1s - loss: 0.2380 - acc: 0.9097 - val_loss: 0.3951 - val_acc: 0.8455
Epoch 102/500
Epoch 00101: val_loss did not improve
1s - loss: 0.2386 - acc: 0.9096 - val_loss: 0.4190 - val_acc: 0.8314
Epoch 103/500
Epoch 00102: val_loss improved from 0.38642 to 0.37977, saving model to ./bestmodel10.hdf5
2s - loss: 0.2394 - acc: 0.9083 - val_loss: 0.3798 - val_acc: 0.8525
Epoch 104/500
Epoch 00103: val_loss did not improve
1s - loss: 0.2383 - acc: 0.9102 - val_loss: 0.3894 - val_acc: 0.8464
Epoch 105/500
Epoch 00104: val_loss did not improve
1s - loss: 0.2333 - acc: 0.9113 - val_loss: 0.3896 - val_acc: 0.8455
Epoch 106/500
Epoch 00105: val_loss did not improve
1s - loss: 0.2358 - acc: 0.9097 - val_loss: 0.3826 - val_acc: 0.8530
Epoch 107/500
Epoch 00106: val_loss improved from 0.37977 to 0.37974, saving model to ./bestmodel10.hdf5
2s - loss: 0.2331 - acc: 0.9112 - val_loss: 0.3797 - val_acc: 0.8488
Epoch 108/500
Epoch 00107: val_loss did not improve
1s - loss: 0.2304 - acc: 0.9148 - val_loss: 0.3931 - val_acc: 0.8506
Epoch 109/500
Epoch 00108: val_loss improved from 0.37974 to 0.36785, saving model to ./bestmodel10.hdf5
2s - loss: 0.2297 - acc: 0.9124 - val_loss: 0.3679 - val_acc: 0.8544
Epoch 110/500
Epoch 00109: val_loss did not improve
1s - loss: 0.2324 - acc: 0.9124 - val_loss: 0.3914 - val_acc: 0.8511
Epoch 111/500
Epoch 00110: val_loss did not improve
1s - loss: 0.2253 - acc: 0.9145 - val_loss: 0.3867 - val_acc: 0.8539
Epoch 112/500
Epoch 00111: val_loss did not improve
1s - loss: 0.2265 - acc: 0.9142 - val_loss: 0.3891 - val_acc: 0.8520
Epoch 113/500
Epoch 00112: val_loss did not improve
1s - loss: 0.2280 - acc: 0.9114 - val_loss: 0.3940 - val_acc: 0.8520
Epoch 114/500
Epoch 00113: val_loss did not improve
1s - loss: 0.2264 - acc: 0.9151 - val_loss: 0.4139 - val_acc: 0.8473
Epoch 115/500
Epoch 00114: val_loss improved from 0.36785 to 0.36046, saving model to ./bestmodel10.hdf5
3s - loss: 0.2263 - acc: 0.9153 - val_loss: 0.3605 - val_acc: 0.8535
Epoch 116/500
Epoch 00115: val_loss did not improve
1s - loss: 0.2234 - acc: 0.9163 - val_loss: 0.3897 - val_acc: 0.8520
Epoch 117/500
Epoch 00116: val_loss did not improve
1s - loss: 0.2222 - acc: 0.9157 - val_loss: 0.3830 - val_acc: 0.8577
Epoch 118/500
Epoch 00117: val_loss did not improve
1s - loss: 0.2220 - acc: 0.9168 - val_loss: 0.3757 - val_acc: 0.8525
Epoch 119/500
Epoch 00118: val_loss did not improve
1s - loss: 0.2198 - acc: 0.9181 - val_loss: 0.3895 - val_acc: 0.8497
Epoch 120/500
Epoch 00119: val_loss did not improve
1s - loss: 0.2202 - acc: 0.9178 - val_loss: 0.3910 - val_acc: 0.8459
Epoch 121/500
Epoch 00120: val_loss did not improve
1s - loss: 0.2182 - acc: 0.9188 - val_loss: 0.3761 - val_acc: 0.8549
Epoch 122/500
Epoch 00121: val_loss did not improve
1s - loss: 0.2187 - acc: 0.9141 - val_loss: 0.3901 - val_acc: 0.8525
Epoch 123/500
Epoch 00122: val_loss did not improve
1s - loss: 0.2154 - acc: 0.9197 - val_loss: 0.3937 - val_acc: 0.8539
Epoch 124/500
Epoch 00123: val_loss did not improve
1s - loss: 0.2163 - acc: 0.9192 - val_loss: 0.3765 - val_acc: 0.8549
Epoch 125/500
Epoch 00124: val_loss did not improve
1s - loss: 0.2169 - acc: 0.9202 - val_loss: 0.3908 - val_acc: 0.8572
Epoch 126/500
Epoch 00125: val_loss did not improve
1s - loss: 0.2163 - acc: 0.9217 - val_loss: 0.3956 - val_acc: 0.8516
Epoch 127/500
Epoch 00126: val_loss did not improve
1s - loss: 0.2150 - acc: 0.9184 - val_loss: 0.3689 - val_acc: 0.8553
Epoch 128/500
Epoch 00127: val_loss did not improve
1s - loss: 0.2135 - acc: 0.9220 - val_loss: 0.4043 - val_acc: 0.8492
Epoch 129/500
Epoch 00128: val_loss did not improve
1s - loss: 0.2133 - acc: 0.9208 - val_loss: 0.3802 - val_acc: 0.8553
Epoch 130/500
Epoch 00129: val_loss did not improve
1s - loss: 0.2124 - acc: 0.9202 - val_loss: 0.3795 - val_acc: 0.8577
Epoch 131/500
Epoch 00130: val_loss did not improve
1s - loss: 0.2100 - acc: 0.9209 - val_loss: 0.3987 - val_acc: 0.8572
Epoch 132/500
Epoch 00131: val_loss did not improve
1s - loss: 0.2111 - acc: 0.9208 - val_loss: 0.3721 - val_acc: 0.8577
Epoch 133/500
Epoch 00132: val_loss did not improve
1s - loss: 0.2073 - acc: 0.9221 - val_loss: 0.3699 - val_acc: 0.8586
Epoch 134/500
Epoch 00133: val_loss did not improve
1s - loss: 0.2108 - acc: 0.9221 - val_loss: 0.3902 - val_acc: 0.8581
Epoch 135/500
Epoch 00134: val_loss did not improve
1s - loss: 0.2095 - acc: 0.9228 - val_loss: 0.3736 - val_acc: 0.8567
Epoch 136/500
Epoch 00135: val_loss did not improve
1s - loss: 0.2057 - acc: 0.9222 - val_loss: 0.3947 - val_acc: 0.8525
Epoch 137/500
Epoch 00136: val_loss did not improve
1s - loss: 0.2045 - acc: 0.9244 - val_loss: 0.4003 - val_acc: 0.8483
Epoch 138/500
Epoch 00137: val_loss did not improve
1s - loss: 0.2100 - acc: 0.9214 - val_loss: 0.3780 - val_acc: 0.8581
Epoch 139/500
Epoch 00138: val_loss did not improve
1s - loss: 0.2031 - acc: 0.9240 - val_loss: 0.3769 - val_acc: 0.8596
Epoch 140/500
Epoch 00139: val_loss did not improve
1s - loss: 0.2045 - acc: 0.9244 - val_loss: 0.3790 - val_acc: 0.8596
Epoch 141/500
Epoch 00140: val_loss did not improve
1s - loss: 0.2061 - acc: 0.9242 - val_loss: 0.3800 - val_acc: 0.8539
Epoch 142/500
Epoch 00141: val_loss did not improve
1s - loss: 0.2060 - acc: 0.9240 - val_loss: 0.3888 - val_acc: 0.8553
Epoch 143/500
Epoch 00142: val_loss did not improve
1s - loss: 0.2003 - acc: 0.9246 - val_loss: 0.3675 - val_acc: 0.8610
Epoch 144/500
Epoch 00143: val_loss did not improve
1s - loss: 0.2018 - acc: 0.9245 - val_loss: 0.3866 - val_acc: 0.8553
Epoch 145/500
Epoch 00144: val_loss did not improve
1s - loss: 0.2020 - acc: 0.9246 - val_loss: 0.3842 - val_acc: 0.8549
Epoch 146/500
Epoch 00145: val_loss did not improve
1s - loss: 0.2010 - acc: 0.9233 - val_loss: 0.3772 - val_acc: 0.8586
Epoch 147/500
Epoch 00146: val_loss did not improve
1s - loss: 0.1977 - acc: 0.9270 - val_loss: 0.3748 - val_acc: 0.8581
Epoch 148/500
Epoch 00147: val_loss did not improve
1s - loss: 0.2023 - acc: 0.9235 - val_loss: 0.3680 - val_acc: 0.8600
Epoch 149/500
Epoch 00148: val_loss did not improve
1s - loss: 0.2000 - acc: 0.9245 - val_loss: 0.3982 - val_acc: 0.8530
Epoch 150/500
Epoch 00149: val_loss did not improve
1s - loss: 0.1987 - acc: 0.9255 - val_loss: 0.3631 - val_acc: 0.8563
Epoch 151/500
Epoch 00150: val_loss did not improve
1s - loss: 0.1963 - acc: 0.9253 - val_loss: 0.3952 - val_acc: 0.8544
Epoch 152/500
Epoch 00151: val_loss did not improve
1s - loss: 0.1992 - acc: 0.9268 - val_loss: 0.3654 - val_acc: 0.8586
Epoch 153/500
Epoch 00152: val_loss did not improve
1s - loss: 0.1991 - acc: 0.9255 - val_loss: 0.4138 - val_acc: 0.8511
Epoch 154/500
Epoch 00153: val_loss did not improve
1s - loss: 0.2021 - acc: 0.9249 - val_loss: 0.3917 - val_acc: 0.8591
Epoch 155/500
Epoch 00154: val_loss did not improve
1s - loss: 0.2003 - acc: 0.9259 - val_loss: 0.3724 - val_acc: 0.8577
Epoch 156/500
Epoch 00155: val_loss did not improve
1s - loss: 0.2003 - acc: 0.9242 - val_loss: 0.3676 - val_acc: 0.8600
Epoch 157/500
Epoch 00156: val_loss did not improve
1s - loss: 0.1939 - acc: 0.9284 - val_loss: 0.3977 - val_acc: 0.8567
Epoch 158/500
Epoch 00157: val_loss did not improve
1s - loss: 0.1977 - acc: 0.9265 - val_loss: 0.3689 - val_acc: 0.8600
Epoch 159/500
Epoch 00158: val_loss did not improve
1s - loss: 0.1955 - acc: 0.9271 - val_loss: 0.3882 - val_acc: 0.8563
Epoch 160/500
Epoch 00159: val_loss did not improve
1s - loss: 0.1977 - acc: 0.9248 - val_loss: 0.3615 - val_acc: 0.8572
Epoch 161/500
Epoch 00160: val_loss did not improve
1s - loss: 0.1934 - acc: 0.9269 - val_loss: 0.3823 - val_acc: 0.8614
Epoch 162/500
Epoch 00161: val_loss did not improve
1s - loss: 0.1946 - acc: 0.9286 - val_loss: 0.3986 - val_acc: 0.8525
Epoch 163/500
Epoch 00162: val_loss did not improve
1s - loss: 0.1978 - acc: 0.9262 - val_loss: 0.3730 - val_acc: 0.8567
Epoch 164/500
Epoch 00163: val_loss did not improve
1s - loss: 0.1943 - acc: 0.9275 - val_loss: 0.3645 - val_acc: 0.8619
Epoch 165/500
Epoch 00164: val_loss did not improve
1s - loss: 0.1931 - acc: 0.9278 - val_loss: 0.3803 - val_acc: 0.8581
Epoch 166/500
Epoch 00165: val_loss did not improve
Epoch 00165: early stopping
1s - loss: 0.1881 - acc: 0.9304 - val_loss: 0.3772 - val_acc: 0.8619
training done!
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
cov1 (Convolution1D)             (None, 58, 80)        1360        convolution1d_input_2[0][0]      
____________________________________________________________________________________________________
leakyrelu_1 (LeakyReLU)          (None, 58, 80)        0           cov1[0][0]                       
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 58, 80)        0           leakyrelu_1[0][0]                
____________________________________________________________________________________________________
cov2 (Convolution1D)             (None, 57, 80)        12880       dropout_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_2 (LeakyReLU)          (None, 57, 80)        0           cov2[0][0]                       
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 57, 80)        0           leakyrelu_2[0][0]                
____________________________________________________________________________________________________
cov3 (Convolution1D)             (None, 54, 80)        25680       dropout_2[0][0]                  
____________________________________________________________________________________________________
leakyrelu_3 (LeakyReLU)          (None, 54, 80)        0           cov3[0][0]                       
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 54, 80)        0           leakyrelu_3[0][0]                
____________________________________________________________________________________________________
cov4 (Convolution1D)             (None, 51, 80)        25680       dropout_3[0][0]                  
____________________________________________________________________________________________________
leakyrelu_4 (LeakyReLU)          (None, 51, 80)        0           cov4[0][0]                       
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 51, 80)        0           leakyrelu_4[0][0]                
____________________________________________________________________________________________________
cov5 (Convolution1D)             (None, 48, 80)        25680       dropout_4[0][0]                  
____________________________________________________________________________________________________
leakyrelu_5 (LeakyReLU)          (None, 48, 80)        0           cov5[0][0]                       
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 48, 80)        0           leakyrelu_5[0][0]                
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 3840)          0           dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           384100      flatten_1[0][0]                  
____________________________________________________________________________________________________
leakyrelu_6 (LeakyReLU)          (None, 100)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_6 (Dropout)              (None, 100)           0           leakyrelu_6[0][0]                
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             101         dropout_6[0][0]                  
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 1)             0           dense_2[0][0]                    
====================================================================================================
Total params: 475481
____________________________________________________________________________________________________
**************vadiation results on validation dataset************
  32/2129 [..............................] - ETA: 0s 160/2129 [=>............................] - ETA: 0s 288/2129 [===>..........................] - ETA: 0s 416/2129 [====>.........................] - ETA: 0s 512/2129 [======>.......................] - ETA: 0s 608/2129 [=======>......................] - ETA: 0s 704/2129 [========>.....................] - ETA: 0s 800/2129 [==========>...................] - ETA: 0s 896/2129 [===========>..................] - ETA: 0s 992/2129 [============>.................] - ETA: 0s1088/2129 [==============>...............] - ETA: 0s1184/2129 [===============>..............] - ETA: 0s1312/2129 [=================>............] - ETA: 0s1408/2129 [==================>...........] - ETA: 0s1504/2129 [====================>.........] - ETA: 0s1600/2129 [=====================>........] - ETA: 0s1696/2129 [======================>.......] - ETA: 0s1824/2129 [========================>.....] - ETA: 0s1952/2129 [==========================>...] - ETA: 0s2080/2129 [============================>.] - ETA: 0s[0.36045575615048353, 0.85345232511921743]
  32/2129 [..............................] - ETA: 0s 160/2129 [=>............................] - ETA: 0s 288/2129 [===>..........................] - ETA: 0s 416/2129 [====>.........................] - ETA: 0s 544/2129 [======>.......................] - ETA: 0s 672/2129 [========>.....................] - ETA: 0s 800/2129 [==========>...................] - ETA: 0s 928/2129 [============>.................] - ETA: 0s1056/2129 [=============>................] - ETA: 0s1184/2129 [===============>..............] - ETA: 0s1312/2129 [=================>............] - ETA: 0s1440/2129 [===================>..........] - ETA: 0s1568/2129 [=====================>........] - ETA: 0s1696/2129 [======================>.......] - ETA: 0s1824/2129 [========================>.....] - ETA: 0s1952/2129 [==========================>...] - ETA: 0s2080/2129 [============================>.] - ETA: 0s  32/2129 [..............................] - ETA: 0s 160/2129 [=>............................] - ETA: 0s 288/2129 [===>..........................] - ETA: 0s 416/2129 [====>.........................] - ETA: 0s 544/2129 [======>.......................] - ETA: 0s 672/2129 [========>.....................] - ETA: 0s 800/2129 [==========>...................] - ETA: 0s 928/2129 [============>.................] - ETA: 0s1056/2129 [=============>................] - ETA: 0s1184/2129 [===============>..............] - ETA: 0s1312/2129 [=================>............] - ETA: 0s1440/2129 [===================>..........] - ETA: 0s1568/2129 [=====================>........] - ETA: 0s1696/2129 [======================>.......] - ETA: 0s1824/2129 [========================>.....] - ETA: 0s1952/2129 [==========================>...] - ETA: 0s2080/2129 [============================>.] - ETA: 0s************************
auc: 0.92106498641
mcc: 0.711303057465
negative ---> precision:0.818104906937, recall:0.908834586466, f1score:0.861086375779, support:1064
positive ---> precision:0.897571277719, recall:0.798122065728, f1score:0.844930417495, support:1065
************************
**************prediction results on test dataset************
  32/2138 [..............................] - ETA: 0s 160/2138 [=>............................] - ETA: 0s 288/2138 [===>..........................] - ETA: 0s 416/2138 [====>.........................] - ETA: 0s 544/2138 [======>.......................] - ETA: 0s 672/2138 [========>.....................] - ETA: 0s 800/2138 [==========>...................] - ETA: 0s 928/2138 [============>.................] - ETA: 0s1056/2138 [=============>................] - ETA: 0s1184/2138 [===============>..............] - ETA: 0s1312/2138 [=================>............] - ETA: 0s1440/2138 [===================>..........] - ETA: 0s1568/2138 [=====================>........] - ETA: 0s1696/2138 [======================>.......] - ETA: 0s1824/2138 [========================>.....] - ETA: 0s1952/2138 [==========================>...] - ETA: 0s2080/2138 [============================>.] - ETA: 0s[0.29403724934700159, 0.87885874660356345]
  32/2138 [..............................] - ETA: 0s 192/2138 [=>............................] - ETA: 0s 352/2138 [===>..........................] - ETA: 0s 512/2138 [======>.......................] - ETA: 0s 672/2138 [========>.....................] - ETA: 0s 832/2138 [==========>...................] - ETA: 0s 992/2138 [============>.................] - ETA: 0s1152/2138 [===============>..............] - ETA: 0s1312/2138 [=================>............] - ETA: 0s1472/2138 [===================>..........] - ETA: 0s1632/2138 [=====================>........] - ETA: 0s1792/2138 [========================>.....] - ETA: 0s1952/2138 [==========================>...] - ETA: 0s2112/2138 [============================>.] - ETA: 0s  32/2138 [..............................] - ETA: 0s 192/2138 [=>............................] - ETA: 0s 352/2138 [===>..........................] - ETA: 0s 512/2138 [======>.......................] - ETA: 0s 672/2138 [========>.....................] - ETA: 0s 832/2138 [==========>...................] - ETA: 0s 992/2138 [============>.................] - ETA: 0s1152/2138 [===============>..............] - ETA: 0s1312/2138 [=================>............] - ETA: 0s1472/2138 [===================>..........] - ETA: 0s1632/2138 [=====================>........] - ETA: 0s1792/2138 [========================>.....] - ETA: 0s1952/2138 [==========================>...] - ETA: 0s2112/2138 [============================>.] - ETA: 0s************************
auc: 0.945497790002
mcc: 0.757959293534
negative ---> precision:0.869525547445, recall:0.891487371375, f1score:0.880369515012, support:1069
positive ---> precision:0.8886756238, recall:0.866230121609, f1score:0.87730933207, support:1069
************************
